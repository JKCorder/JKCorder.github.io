[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"text rough draft companion text graduate methods course, PSCI 6920, Political Analysis II, offered Western Michigan University. text open source may suggest edits obtain original RMarkdown files github.idea text combine insights econometrics, recent work political science, programming tools R. recent political science works spans American politics comparative politics, particular attention work introduces builds new approaches statistical modeling. technical approach doesn’t rely matrix algebra calculus, complexity modeling level Greene (2000).second motivation constructing chapters produce series tidyverse-inspired examples translate applications political science literature contemporary programming context. chapters rely data programming Harvard Dataverse. chapters rely update sample data programming methods econometrics texts. chapter rely older data, introduce new modeling programming approaches reproduce estimates.text series chapters, self-contained R Notebook. Notebook format makes easier work chapters replicate run chunk code. file dependencies (things need download) identified within code chunk). sure download run _common.R load required libraries set options.","code":""},{"path":"why-math-why-r.html","id":"why-math-why-r","chapter":"1 Why math? Why R?","heading":"1 Why math? Why R?","text":"","code":""},{"path":"why-math-why-r.html","id":"math-in-social-science","chapter":"1 Why math? Why R?","heading":"1.1 Math in social science","text":"Social scientists exploited advances applied mathematics 100 years. Much early quantitative work Political Science Sociology relied analysis census aggregate data collected federal, state local government (election returns, instance). Advances survey research 1940s triggered explosion work political behavior. International relations drew heavily game theory Cold War era. Computers permitted rapid aggregation data complex statistical applications. integration math, technology, social science long history. CBS News relied team Columbia University social scientists employ latest technology predict outcome 1952 election live TV based early election returns.broad sense, two distinct types ways mathematics underpins social sciences research: (1) mechanistic formal models (2) empirical models statistical models. ambitious (controversial) social science combines . lively sobering account origins use tools check : Simulmatics Corporation Invented Future (Lepore 2020). learned details 1952 election night coverage book.sort distinction formal models empirical models talk simulations course cover.","code":""},{"path":"why-math-why-r.html","id":"mechanistic-models-aka-formal-models","chapter":"1 Why math? Why R?","heading":"Mechanistic models (aka formal models)","text":"number formal models may familiar . coursework American politics, work voting Anthony Downs. course international relations, maybe prisoner’s dilemma. social choice theory, Condorcet’s paradox. models draw data - strictly theoretical. don’t measure estimate parameters models. might use mathematical computational strategies come solutions.","code":""},{"path":"why-math-why-r.html","id":"the-paradox-of-voting","chapter":"1 Why math? Why R?","heading":"The paradox of voting","text":"Anthony Downs pioneered idea party differential (B) motivates someone vote election (Downs 1957). difference two competing parties large, much better preferred party wins, incentive vote. benefit gain function much voter matters (p). vote probability impacting outcome, expected gain voting zero. Later work building insight added considerations costs associated voting (C) benefits voting - related sense civic duty (D). benefit voting (R) can summarized simple formula:\\[R = (B \\times p) - C + D\\]\nR positive number, vote. less zero, don’t. don’t estimate parameters model - values p, B, C D. nevertheless specific empirical implications can test. instance, one area lower costs associated voting, else equal, turnout higher. elections competitive, meaning vote higher probability impacting outcome, turnout higher. model useful thinking turnout, even start measure anything.","code":""},{"path":"why-math-why-r.html","id":"the-prisoners-dilemma","chapter":"1 Why math? Why R?","heading":"The prisoner’s dilemma","text":"prisoner’s dilemma simple one-step, one-shot game models interaction two prisoners option confess stay silent. game strategic since payoffs prisoner depend actions . game widely used demonstrate individually rational choices can produce outcomes collectively sub-optimal.Figure 1. prisoner’s dilemmaThe Nash equilibrium - solution - game prisoners confess - neither player incentive change choice even knew choices . collectively optimal solution prisoners stay silent (combined prison time sums two years). (Criminal organizations overcome dilemma euphemistically called side payments making confessions costly leave prison.)","code":""},{"path":"why-math-why-r.html","id":"empirical-models-aka-statistical-models","chapter":"1 Why math? Why R?","heading":"Empirical models (aka statistical models)","text":"Empirical models grounded data. gross simplifications reality, designed help us understand causality , view, useful determine number potentially relevant variables might help us understand observe. voting behavior US function race, gender, education levels? church attendance matter? relative weight political factors determining length time appearance confirmed cases COVID-19 implementation Shelter--Place orders US states? explains judge’s decision declare search unreasonable? much judge’s ideology matter compared (case-specific) factors?types questions posed way sub-field?Two specific examples might clarify models used. Note theorizing informal - using mathematical approach generate specific empirical features model. simply claiming think X influences Y.","code":""},{"path":"why-math-why-r.html","id":"what-helps-us-understand-the-choices-of-a-judge","chapter":"1 Why math? Why R?","heading":"What helps us understand the choices of a judge?","text":"example Songer, Segal, Cameron (1994). example might quite align description since invoke formal model develop statistical model, don’t present manipulate formal model explicit way. model treats judge’s decision permit use evidence contested police search – positive coefficients context mean judge likely keep evidence, negative coefficients mean factor made less likely.Figure 2. Screenshot Songer, Segal, Cameron (1994)can see table case-specific factors matter - search conducted home business, judge less likely permit evidence introduced. search incident arrest executed obtaining search warrant, judge likely permit evidence. impact column suggests judicial ideology exerts powerful independent effect outcome cases decided US Circuit Courts. conservative panel likely permit evidence introduced.","code":""},{"path":"why-math-why-r.html","id":"what-helps-us-predict-how-long-it-tooks-states-to-adopt-shelter-in-place-orders","chapter":"1 Why math? Why R?","heading":"What helps us predict how long it tooks states to adopt shelter-in-place orders?","text":"recent co-authored piece Policy Design Practice, used statistical test different features US states may led rapid slower adoption Shelter--Place orders (Corder, Mingus, Blinova 2020). don’t offer type formal treatment specifying utility function governor, simply employ statistical model see types state acted quickly. model, negative coefficients mean shorter (smaller) number days observation confirmed cases executive action.Figure 3. Screenshot Corder, Mingus, Blinova (2020)found political factors (specifically, Republican party control state senate Republican governor) translate long delays, differences urban rural states close zero differences high population low population states relatively small.","code":""},{"path":"why-math-why-r.html","id":"use-and-mis-use-of-statistical-models","chapter":"1 Why math? Why R?","heading":"Use and mis-use of statistical models","text":"use statistical models commonplace, anxiety use models - particularly learn manipulating information attitudes can lead actions moment. reservations stop technology entrepreneurs going road - collecting massive amounts behavioral data, using data influence choice buy live read. indications combination better models better data effective. Companies like Cambridge Analytica claimed able make types interventions 2016 US presidential election, precisely targeting messages advertising influence immediate decisions small subsets persuadable voters. (case, data collected via Facebook violation multiple privacy data-sharing regulations). contexts, want learn behavioral psychologists leveraging insights improve (least change) public policy, read nudges (Thaler Sunstein 2008)","code":""},{"path":"why-math-why-r.html","id":"simulations---drawing-on-mechanistic-and-empirical-components","chapter":"1 Why math? Why R?","heading":"Simulations - drawing on mechanistic and empirical components","text":"broad class modeling approaches draw mechanistic empirical models - using parameters estimated observation simulate behavior outcome interest. cases models somewhat atheoretical, leveraging insights observation. cases, limited amount data inform model insights mechanics. Macroeconomists use complex simulation (“FRB/US”) forecast path economy Federal Reserve sets interest rates. Actuaries Social Security Administration use demographic actuarial approaches simulate value Social Security trust fund time. approaches work? Two examples news: weather (every day) growth COVID-19 cases (2020-2).","code":""},{"path":"why-math-why-r.html","id":"weather-forecasts","chapter":"1 Why math? Why R?","heading":"Weather forecasts","text":"short description weather forecasters use models published Washington Post May, 2018. Notice two distinct components - mathematical models extensive data collection.Weather forecasters often discuss models use help predict weather. models? work? strengths weaknesses?foundation models mathematical equations based physics characterize air moves heat moisture exchanged atmosphere. Weather observations (pressure, wind, temperature moisture) obtained ground sensors weather satellites fed equations. observations brought models process known data assimilation.model, atmosphere divided three-dimensional grid grid point given assimilated data. called initial conditions. grid point, mathematical equations applied stepped forward time. outputs many time steps specify future weather grid points.Capital Weather Gang (2018)key thing recognize implication “stepped forward time” - set equations translate initital conditions time t expected conditions time t+1, expected conditions time t+1 translated expected conditions time t+2. observing actual things t anything strictly function underlying equations. can imagine equations movement moisture heat fairly precise, still know weather forecasts beyond couple days reliable.","code":""},{"path":"why-math-why-r.html","id":"modeling-covid-19","chapter":"1 Why math? Why R?","heading":"Modeling COVID 19","text":"Weather forecasting helps understand modeling process, strictly natural phenomena. seem understanding spread virus. virus properties related easy moves one person another quickly show symptoms. also social components - closely people live , people likely wear masks? limit contact others outside households? Epidemiologists use considerations model number expected cases COVID-19. example type projection CDC. bold path ensemble model - estimate based 22 different models generated variety academic government teams.Figure 4. Centers Disease Control Prevention projections January 2021 Covid casesYou can see wide variation move one month future - anywhere 1.2 million cases 2 million cases per week January 16, 2021. time, even lower range projection seemed almost unfathomable. number ultimately ended 1.7 million, exactly predicted.","code":""},{"path":"why-math-why-r.html","id":"social-security-administration-office-of-chief-actuary","chapter":"1 Why math? Why R?","heading":"Social Security Administration Office of Chief Actuary","text":"SSA produces annual estimate balance OASDI trust funds - money available make shortfall Social Security revenues (FICA tax collections) Social Security expenditures (payments elderly disabled recipients). estimates factor demographic economic assumptions mortality, fertility, immigration, economic growth, inflation, interest rates data might impact future claims future revenue. estimates summarized three different scenarios - media coverage typically focuses intermediate assumptions (II). “low-cost” scenario III “high-cost” scenario.Figure 5. Screenshot Social Security Board Trustees (2020)can see combined Trust Funds likely exhausted 2035.","code":""},{"path":"why-math-why-r.html","id":"why-dont-we-see-many-or-any-of-these-types-of-simulations-in-political-science-journals","chapter":"1 Why math? Why R?","heading":"Why don’t we see many (or any) of these types of simulations in Political Science journals?","text":"Imagine approach modeling human behavior analogous weather forecasting approach. ? one, underlying mechanical models just mature physics, may never . addition, data collection also less extensive subject error, may changing. today technology just doesn’t seem task iterating many periods see effects particular message exposure information action long-run, instance.Second, use simulations form sordid depressing history Political Science. Think ethical implications actually worked. understand system human social political behavior manipulate initial conditions, bring outcomes desire. kind thinking ambitions led disastrous applications technology 1960s - counterinsurgency efforts Vietnam, efforts prevent urban unrest American cities. work carried political scientists, funded federal government, best fraudulent worst harmful (, see Lepore (2020)). can come positive useful examples - modeling arms race, example, modeling parties may moderate message order attract allies. latter, check book Paper Stones (Przeworski Sprague 1988). entire discipline formed around efforts (Cliodynamics) much research seeps Political Science journals.Notice important difference statistical model longer-term simulations. statistical models helping us understand messages influence choices next period - next minute - next choice make - don’t feed larger multi-period simulation even anticipate really care long-run effects.","code":""},{"path":"why-math-why-r.html","id":"the-narrow-focus-of-this-course","chapter":"1 Why math? Why R?","heading":"1.2 The narrow focus of this course","text":"Simply put, focus course statistical model. won’t looking formal models simulations. start simple, linear model advance complex models widen range things can predict (binary choices, counts, durations, data observed time). place broader research enterprise can difficult understand. Think ways learn observation - statistical model just one approach among many useful number assumptions conditions satisfied. graphic places effort broader context research enterprise, differentiating class qualitative work theory-building work. approach statistical modeling centers theory-testing large (least small) samples, sometimes randomly generated. Note comparative case study, instance, might employ tools - theory-building theory-testing, quantitative qualitative, text numbers data - labeling something comparative case study doesn’t tell type approach taking.Figure 6. regression fit ?works cited figure representative probably run across courses.","code":""},{"path":"why-math-why-r.html","id":"why-use-r","chapter":"1 Why math? Why R?","heading":"1.3 Why use R?","text":"order build understanding statistical modeling, use statistical software R. ?R freeR freeR flexibleR flexibleR supported user community dedicated idea open sourceR supported user community dedicated idea open sourceR transportable – can easily take learn one project apply anotherR transportable – can easily take learn one project apply anotherR reproducible – keyR reproducible – key","code":""},{"path":"why-math-why-r.html","id":"how-r-works","chapter":"1 Why math? Why R?","heading":"1.4 How R works","text":"Commands R create use known objects. object single number, column numbers (vector), matrix, matrix identified data (data frame sometimes abbreviated df). Objects named created.Commands R use functions take input produce output. many functions packed base R installation, specialized functions need add. specialized function published packages can install PC. use function package, must first make package visible R library functionThe typical structure R command isYou can chain together several functions pipe, %>%, might see many lines commands linking together generate one new object.","code":"object <- function()"},{"path":"why-math-why-r.html","id":"three-examples","chapter":"1 Why math? Why R?","heading":"1.5 Three examples","text":"","code":""},{"path":"why-math-why-r.html","id":"comparing-groups.-turnout-in-the-2016-presidential-election","chapter":"1 Why math? Why R?","heading":"Comparing groups. Turnout in the 2016 presidential election","text":"Current Population Survey US Census product based monthly survey ~60,000 people living United States. primary use CPS measure monthly unemployment rate. 1960 Census began include supplement November recorded whether someone voted. Since survey large, can use generate reliable estimates turnout small groups.Table 1. Turnout, race, 2016 US presidential electionYou can see turnout among White voters much higher racial ethnic minorities, one component Donald Trump’s victory something change 2020.","code":"\n# Any lines that start with a # are just comments, notes to myself so I can recall why I made particular choices.  I use comments liberally.\n\n# Use the read_fwf function from the read_r library\n# Read the data from the CPS raw data file\n# I used the codebook to identify the variables and column numbers\n# We can take a look at the codebook in class.\n# This is really tedious work, and potentially introduces errors\ncps16<- read_fwf(\"data/nov16pub.dat\", fwf_cols(\n  hrinsta=c(57,58),\n  prtage=c(122,123),\n  pemaritl=c(125,126),\n  sex=c(129,130),\n  ptdtrace=c(139,140),\n  pehspnon=c(157,159),\n  prpertyp=c(161,162),\n  penatvty=c(163,165),\n  pemntvty=c(166,168),\n  pefntvty=c(169,171),\n  prcitshp=c(172,173),\n  prinusyr=c(176,177),\n  weight=c(613,622),\n  prnmchld=c(635,636),\n  vote=c(951,952)),\n  col_types = cols()\n)\n\n# Filter to include only citizens 18 and older\n# The filter function just keeps a subset of observations in the new object\n# Notice the use of a pipe %>%\n# This requires the dyplr library\ncps_valid<- cps16 %>% filter(hrinsta==1 & prtage>=18 & prpertyp==2 & prcitshp!=5)\n\n# Recode to treat people who refused or did not answer as not voting\n# This is the dplyr recode function  \ncps_valid$turnout<-recode(cps_valid$vote, '-9' = 0, '-3' = 0, '-2' = 0, '2' = 0, '1' = 1)\n\n# Create new variable race and make it a character (a categorical variable)\n# Notice that I am reading from and writing to the same data frame\n# murate is the function to create a new variable from an existing variable\ncps_valid<- cps_valid %>%\n  mutate(\n    race = case_when(\n      ptdtrace==1 & pehspnon!=1 ~ \"White, not Hispanic\",\n      ptdtrace==2 ~ \"African American\",\n      ptdtrace==4 ~ \"Asian\",\n      ptdtrace==1 & pehspnon==1 ~ \"Hispanic\",\n      TRUE ~  \"Other\"\n    )\n  )\n\n# Create an R object  that summarizes turnout by race\n# the .groups option, included here, suppresses a warning message that doesn't apply with a single grouping variable\n# group_by is a function\n# summarize is a function\nd<- cps_valid %>%\n  group_by(race) %>%\n  summarize(turnout = 100*weighted.mean(turnout, weight, na.rm = TRUE), count=n(), .groups = 'drop')\n\n# Produce a table with the kable function and include options to customize column names, add a title, limit to two digits after the decimal point.\nkable(d, digits = 2, caption = \"**Table 1.  Turnout, by race, 2016 US presidential election**\", col.names=c(\"Race\", \"Turnout\", \"Count\"))"},{"path":"why-math-why-r.html","id":"creating-figures.-ideological-polarization-in-the-u.s.-house-of-representatives","chapter":"1 Why math? Why R?","heading":"Creating figures. Ideological polarization in the U.S. House of Representatives","text":"may encountered Poole-Rosenthal scores (also known D-NOMINATE scores) reading US Congress. scores use roll-call votes measure ideological position members Congress relative members. can use scores track ideological polarization Congress. figure summarizes mean Republican Democratic member House Representatives 2-year session.Figure 7. Polarization US House RepresentativesYou can see medians diverged time - Republicans become much conservative Democrats somewhat liberal differences Southern Northern Democrats diminish. data R code voteview website (Lewis et al. 2022)","code":"\n# This code is from  https://voteview.com/articles/party_polarization\n\n# Note that I am reading the data from a website - you can run this chunk to replicate the figure\nnom_dat <- read_csv(\"https://voteview.com/static/data/out/members/HSall_members.csv\", show_col_types=FALSE)\nsouth <- c(40:49,51,53)\n\n# Added this to suppress a warning message\noptions(dplyr.summarise.inform = FALSE)\n\npolar_dat <- nom_dat %>% \n    filter(congress>45 & chamber != \"President\") %>%\n    mutate(year = 2*(congress-1) + 1789) %>%\n    group_by(chamber,congress,year) %>% \n    summarise(\n      party.mean.diff.d1 = mean(nominate_dim1[party_code==200],na.rm=T) - \n                           mean(nominate_dim1[party_code==100],na.rm=T),\n      prop.moderate.d1 = mean(abs(nominate_dim1)<0.25,na.rm=T),\n      prop.moderate.dem.d1 = mean(abs(nominate_dim1[party_code==100])<0.25,na.rm=T),\n      prop.moderate.rep.d1 = mean(abs(nominate_dim1[party_code==200])<0.25,na.rm=T),\n      overlap = (sum(nominate_dim1[party_code==200] <\n                       max(nominate_dim1[party_code==100],na.rm=T),na.rm=T)  +\n                 sum(nominate_dim1[party_code==100] >\n                       min(nominate_dim1[party_code==200],na.rm=T),na.rm=T))/\n                 (sum(!is.na(nominate_dim1[party_code==100]))+\n                  sum(!is.na(nominate_dim1[party_code==200]))),\n      chamber.mean.d1 = mean(nominate_dim1,na.rm=T),\n      chamber.mean.d2 = mean(nominate_dim2,na.rm=T),\n      dem.mean.d1 = mean(nominate_dim1[party_code==100],na.rm=T),\n      dem.mean.d2 = mean(nominate_dim2[party_code==100],na.rm=T),\n      rep.mean.d1 = mean(nominate_dim1[party_code==200],na.rm=T),\n      rep.mean.d2 = mean(nominate_dim2[party_code==200],na.rm=T),\n      north.rep.mean.d1 = mean(nominate_dim1[party_code==200 & \n                                             !(state_icpsr %in% south)],na.rm=T),    \n      north.rep.mean.d2 = mean(nominate_dim2[party_code==200 & \n                                             !(state_icpsr %in% south)],na.rm=T),    \n      south.rep.mean.d1 = mean(nominate_dim1[party_code==200 & \n                                              (state_icpsr %in% south)],na.rm=T),    \n      south.rep.mean.d2 = mean(nominate_dim2[party_code==200 & \n                                             (state_icpsr %in% south)],na.rm=T),    \n      north.dem.mean.d1 = mean(nominate_dim1[party_code==100 & \n                                              !(state_icpsr %in% south)],na.rm=T),    \n      north.dem.mean.d2 = mean(nominate_dim2[party_code==100 & \n                                              !(state_icpsr %in% south)],na.rm=T),    \n      south.dem.mean.d1 = mean(nominate_dim1[party_code==100 & \n                                              (state_icpsr %in% south)],na.rm=T),    \n      south.dem.mean.d2 = mean(nominate_dim2[party_code==100 & \n                                              (state_icpsr %in% south)],na.rm=T),    \n    ) \n\npolar_dat_long <- polar_dat %>% gather(score,value,-chamber,-year,-congress)\nlabels <- c(\"dem.mean.d1\"=\"Democrat\",\n            \"rep.mean.d1\"=\"Republican\",\n            \"north.dem.mean.d1\"=\"Northern Democrats\",\n            \"south.dem.mean.d1\"=\"Southern Democrats\")\n\n# This is based on the voteview RMD but modified so that it does not use ggrepel and is not a function\n\npdatl <- polar_dat_long %>% \n              filter(chamber==\"House\",\n                       score %in% c(\"dem.mean.d1\",\"rep.mean.d1\",\n                                    \"north.dem.mean.d1\",\"south.dem.mean.d1\")) %>%\n                mutate(Party=labels[score]) %>%\n                ungroup()\n  \nggplot(data=pdatl,\n               aes(x=year,y=value,group=Party,col=Party)) +\n               scale_x_continuous(expand = c(0.15, 0), \n                                 breaks=seq(1880, max(pdatl$year), by=8)) +\n              geom_line(size=1) + geom_point(size=1.2) + \n              xlab(\"Year\") + ylab(\"Liberal-Conservative\") + \n              theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n              scale_color_manual(values=c(\"Republican\"=\"#c70828\",\n                                          \"Democrat\"=\"#1372ad\",\n                                          \"Northern Democrats\"=\"#6194F4\",\n                                          \"Southern Democrats\"=\"#81c4e4\"))"},{"path":"why-math-why-r.html","id":"estimating-a-model.-anova-with-one-variable","chapter":"1 Why math? Why R?","heading":"Estimating a model. ANOVA with one variable","text":"just example using lm function (short linear model). linear probability model learn alternative (better) modeling strategies situation week 4 (logistic regression). just wanted see can use lm perform ANOVA (regression categorical variable).R recognizes race categorical variable automatically creates dummy variable baseline category first factor compares group every group.(#tab:cps_model)Table 2. Race turnount, 2016, ANOVAWe learn White voters turnout bit higher rates African Americans, two groups substantially higher turnout racial ethnic minorities. Note predicted values correspond (match) levels reported Table 1.future classes work stargazer package produce regression tables publication-ready.","code":"\n# First I am using the factor function to make sure that White voters (the largest racial group) are the baseline category.\ncps_valid$race2<-factor(cps_valid$race, levels=c(\"White, not Hispanic\", \"African American\", \"Hispanic\", \"Asian\", \"Other\"))\n# In this case I just produce the table and don't retain the model info\n# lm estimates the model\n# tidy formats the output so that we can use kable\n# kable produces the table\ntemp<-tidy(lm(turnout~race2, weight=weight, data=cps_valid))\nkable(temp, digits=3, caption = \"**Table 2. Race and turnount, 2016, ANOVA**\")"},{"path":"why-math-why-r.html","id":"reproducing-results---why-is-that-important","chapter":"1 Why math? Why R?","heading":"1.6 Reproducing results - why is that important?","text":"One features R powerful ability share research replication. one needs proprietary expensive piece software access specialized resources reproduce work. Moreover, anyone working knowledge R can inspect programming see exactly transformed manipulated data, exactly modeling choices made, evaluated robustness choices. researchers can experiment different approaches improve work time. talk replication later term, good idea cultivate habit meticulous recording work. use comments Rmarkdown files .","code":""},{"path":"why-math-why-r.html","id":"why-cant-i-use-excel","chapter":"1 Why math? Why R?","heading":"Why can’t I use EXCEL?","text":"EXCEL useful data entry. two major drawbacks EXCEL research. First share data EXCEL, many people without access software can’t review work. Microsoft changes EXCEL time, data may become unreadable people newer versions. like avoid software dependence. R markdown files simply text files, anyone can look . replication archives encourage submit store data form - text file anyone can read. learn works learn Harvard Dataverse future classes.Another drawback EXCEL potential errors. can’t see cell value calculated without inspecting cell, tedious corrupted cell typo lead error. also potential mismatching records merge data different sources. cut--paste errors. really can’t much statistical analysis EXCEL, end reading data statistical software anyway.Think sounds far-fetched? recent public error pair influential economists. One short account Reinhart-Rogoff error – Excel economics describes error implications. happened?Reinhart Rogoff’s work showed average real economic growth slows (0.1% decline) country’s debt rises 90% gross domestic product (GDP) – 90% figure employed repeatedly political arguments high-profile austerity measures….Excel spreadsheet, Reinhart Rogoff selected entire row averaging growth figures: omitted data Australia, Austria, Belgium, Canada Denmark. words, accidentally included 15 20 countries analysis key calculation. error corrected, “0.1% decline” data became 2.2% average increase economic growth.Borwein Bailey (2013)fine use EXCEL record information, snapshot raw data preserved text file, manipulations, transformation, modeling done way transparent reproducible - R works purpose.","code":""},{"path":"why-math-why-r.html","id":"next-week","chapter":"1 Why math? Why R?","heading":"1.7 Next week","text":"One common misconception statistical models really useful applications political behavior, particularly analysis survey data. doesn’t describe way models applied Political Science , particular, ignores recent proliferation statistical experimental work public policy evaluation. spend week talking learning data, particular focus learning government programs perform. assignment due class loosely structured, designed get think potential work, practical ethical challenges, reasons evidence-based policy-making emerged powerful tool, particularly last decade.","code":""},{"path":"regression-review-and-assumptions.html","id":"regression-review-and-assumptions","chapter":"2 Regression: review and assumptions","heading":"2 Regression: review and assumptions","text":"","code":""},{"path":"regression-review-and-assumptions.html","id":"why-we-care","chapter":"2 Regression: review and assumptions","heading":"2.1 Why we care","text":"estimate parameters statistical model couple reasons. First, want know influence variable outcome interested . call point estimate parameter, \\(\\beta_i\\). Second, want know can draw inference population randomly sampled . order clear certainty inference, need understand sampling distribution \\(\\beta_i\\) well point estimate.","code":""},{"path":"regression-review-and-assumptions.html","id":"bias-and-efficiency","chapter":"2 Regression: review and assumptions","heading":"Bias and efficiency","text":"repeated samples certain assumptions satisfied, OLS estimator \\(\\beta_i\\) unbiased efficient.Unbiased means average \\(\\beta_i\\) across repeated samples \\(\\beta_i\\).Unbiased means average \\(\\beta_i\\) across repeated samples \\(\\beta_i\\).Efficient means OLS estimator lowest variance \\(\\beta_i\\).Efficient means OLS estimator lowest variance \\(\\beta_i\\).sampling distribution \\(\\beta_i\\) mean \\(\\beta_i\\) variance \\(\\sigma^2\\). standard error \\(\\beta_i\\) \\(\\sqrt{\\sigma^2}\\)figure shows two hypothetical sampling distributions - unbiased, one efficient .","code":"\nggplot(data.frame(x = c(-5, 5)), aes(x)) +\n  stat_function(fun = dnorm, color=\"blue\", args = list(mean = 0, sd = .5)) +\n  stat_function(fun = dnorm, color=\"red\",  args = list(mean = 0, sd = 1.5)) +\n  labs(x=expression(beta), y=\" \", caption=\"Figure 1. Efficient and inefficient estimators\" ) +   \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.1))) + theme(axis.text.x=element_blank(), axis.text.y=element_blank())"},{"path":"regression-review-and-assumptions.html","id":"type-ii-errors-with-ols","chapter":"2 Regression: review and assumptions","heading":"Type II errors with OLS","text":"Recall use t-tests evaluate whether estimated coefficient statistically significant.t-test ratio estimated coefficient standard error coefficient:\\[t =\\beta_i / \\sqrt{\\sigma^2}\\]use arbitrary standard p<.05 designate result significant reach typically reach threshold absolute value t greater 2 ( \\(\\left\\lvert{t}\\right\\rvert>2.00\\)).standard error \\(\\beta_i\\) high values t-test low drive value t-test unlikely observe statistically significant coefficients. Type II errorFor interesting discussion origins t-test check Guinnessometrics: Economic Foundation “Student’s t (Ziliak 2008)","code":""},{"path":"regression-review-and-assumptions.html","id":"when-should-we-expect-high-standard-errors-for-beta_i","chapter":"2 Regression: review and assumptions","heading":"2.2 When should we expect high standard errors for \\(\\beta_i\\)?","text":"Consider simple two variable model:\\[y=\\beta_0+\\beta_1x+u\\]formula slope :\\[\\beta_1=cov_{x,y}/var_{x}\\]calculate intercept:\\[\\beta_0 = (\\beta_1*\\mu_x)-\\mu_{y}\\]variance \\(\\beta_1\\) variance disturbance term divided variance x:\\[\\sigma_\\beta^2=\\sigma^2_u \\  /  var_{x}\\]approximate estimate variance disturbance term errors observed sample :\\[\\sigma^2_u =\\frac{\\sum(e_i)^2}{(n-2)}\\]sleight hand . approximation works certain assumptions satisfied. .know formula variance x :\\[\\sum(x_i-\\mu_x)^2\\]formula variance \\(\\beta_1\\) :\\[ \\sigma^2_{\\beta_1}=\\frac{\\sum(e_i)^2}{(n-2)\\times\\sum (x_i-\\mu_x)^2}\\]","code":""},{"path":"regression-review-and-assumptions.html","id":"research-design-implications","chapter":"2 Regression: review and assumptions","heading":"Research design implications","text":"expect see high variance \\(\\beta_1\\)n smallwhen n smallwhen range X small (decrease variance X)range X small (decrease variance X)error terms large (model imprecise).error terms large (model imprecise).type research help reduce variance: large n, wide range x. complete well-specified model","code":""},{"path":"regression-review-and-assumptions.html","id":"assumptions-behind-ols","chapter":"2 Regression: review and assumptions","heading":"2.3 Assumptions behind OLS","text":"Five key assumptions must satisfied order OLS estimator unbiased efficient (BLUE). assumption observed data generated three five assumptions error disturbance term (\\(u\\)). Depending text reference consult, assumptions may grouped numbered different ways.model linear parameters correctly specifiedThe model linear parameters correctly specifiedThe expected value disturbance term (\\(u\\)) zeroThe expected value disturbance term (\\(u\\)) zeroThe disturbance term spherical ..d. (independent identically distributed)disturbance term spherical ..d. (independent identically distributed)variance \\(u\\) constant across observations (homoskedastic)variance \\(u\\) constant across observations (homoskedastic)correlation two disturbance terms\n(autocorrelation serial correlation)correlation two disturbance terms\n(autocorrelation serial correlation)X uncorrelated disturbance term (\\(u\\)) X fixed (stronger form assumption curious since never satisfied!). generate multiple samples Xs errors vary. challenge observational data. evaluate assumption directly learn panel dataX uncorrelated disturbance term (\\(u\\)) X fixed (stronger form assumption curious since never satisfied!). generate multiple samples Xs errors vary. challenge observational data. evaluate assumption directly learn panel dataThe number observations exceed number variables (model identified) exact linear correlation two regressors (collinearity)number observations exceed number variables (model identified) exact linear correlation two regressors (collinearity)","code":""},{"path":"regression-review-and-assumptions.html","id":"using-information-in-the-error-term-to-diagnose-violations-of-ols-assumptions","chapter":"2 Regression: review and assumptions","heading":"2.4 Using information in the error term to diagnose violations of OLS assumptions","text":"calculate parameters linear model can obtain estimate error term associated observation.Given values X estimates \\(\\beta\\) can calculate predicted value Y (typically designated \\(\\hat{y}\\))\\[\\hat{y} = \\beta_0 + \\beta_1x_1+\\beta_2x_2\\]Since\\[y = \\beta_0 + \\beta_1x_1+\\beta_2x_2+e\\]error term (\\(e\\)) simply \\(y-\\hat{y}\\).estimate model using lm function R, residuals stored object along coefficients test statistics.can use new variable, \\(e\\), learn model violates assumptions either heteroskedasticity autocorrelation (sometimes labeled econometrics texts problems “nonspherical disturbances”).example draws sandwich package vignette (Zeileis 2004). dependent variable per capita expenditures public schools US state (District Columbia). independent variable predictor per capita income. expect positive link, wealthier states spending public education.","code":""},{"path":"regression-review-and-assumptions.html","id":"an-example-us-public-school-expenditures-and-state-income","chapter":"2 Regression: review and assumptions","heading":"An example: US public school expenditures and state income","text":"","code":"# The data function indicates that we want to have access to this example dataset included in the sandwich package. The sandwick package is loaded in the common.r file\ndata(\"PublicSchools\")\n\n# We uses pipes and the filter function from the dplyr package to filter out any observations with missing data for expenditures or per capita income.  You can see in the data window that we dropped one observation (51 obs goes to 50 obs).  \nps <- PublicSchools%>% filter(!is.na(Expenditure), !is.na(Income))\n\n# This line just changes Income from dollars to thousands of dollars\n# So 1,000 becomes 1\n# This makes it easier to read and interpret the output\nps$Income <- ps$Income * 0.001\n\n# We use the lm function from base R to estimate the relationship between per capita income and school expenditures at the state level.  The results are stored in an object called fit.  We are going to fit a line rather than the quadratic described in the vignette\nfit <- lm(Expenditure ~ Income, data = ps)\n\n# The summary command prints the output\nsummary(fit)\n\nCall:\nlm(formula = Expenditure ~ Income, data = ps)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-112.390  -42.146   -6.162   30.630  224.210 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -151.26      64.12  -2.359   0.0224 *  \nIncome         68.94       8.35   8.256 9.05e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 61.41 on 48 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.5782 \nF-statistic: 68.16 on 1 and 48 DF,  p-value: 9.055e-11"},{"path":"regression-review-and-assumptions.html","id":"a-better-table","chapter":"2 Regression: review and assumptions","heading":"A better table","text":"Since summary function doesn’t produce nicely formatted html table, employ stargazer stargazer package present results. added options produce type table like.\nTable 1. Public school expenditures function per capita income, OLS\n\ncan see table link per capita income spending public education positive, expected.residuals looks like regression? can use simple histogram get quick look errors conform key assumptions.typical econometrics text motivates discussion problems heteroskedasticity autocorrelation describing “variance-covariance matrix disturbance vector.”disturbance, simply referring nx1 vector contains unobserved error term (\\(u_i\\)). vector like :\\[\\begin{bmatrix}\nu_{1}\\\\\nu_{2}\\\\\n...\\\\\nu_{n}\n\\end{bmatrix}\\]can approximate vector looking sample error terms. variance-covariance matrix just known matrix algebra cross product vector \\(e^te\\). think n x n matrix includes - diagonal expected value variance error term (\\(e_i^2\\)) expected value covariance across error terms (\\(e_ie_j\\)). matrix form :\\[\\begin{bmatrix}\nu_{1}^2 & u_{2}u_{1}& u_{3}u_{1} & ...  & u_{n}u_{1} \\\\\nu_{1}u_{2} & u_{2}^2 & u_{3}u_{2} & ... & u_{n}u_{2}\\\\\nu_{1}u_{3} & u_{2}u_{3} & u_{3}^2 & ... & u_{n}u_{3}\\\\\n... & ... & ... & ... & ... \\\\\nu_{1}u_{n} & u_{2}u_{n} & u_{3}u_{n} & ... & u_{n}^2  \n\\end{bmatrix}\\]Homoskedasticity observed elements diagonal - expected value square disturbance term - identical across values x. series terms main diagonal summed positive number, indicate serial correlation, positive covariance error term observation preceded . serial correlation present error terms don’t covary.use assumptions simplify approximation variance slope coefficient. assumptions satisfied, calculation approximation standard error \\(\\beta_1\\) accurate.","code":"\nstargazer(fit,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Public school expenditures per capita\"),\n          title=\"Table 1.  Public school expenditures as a function of per capita income, OLS\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          covariate.labels=c(\"Per capita income, in thousands\"),\n          star.cutoffs = c(0.10,0.05))\nggplot(data=ps, aes(fit$residuals)) +\ngeom_histogram(binwidth = 20, color = \"black\", fill = \"blue\") +\ntheme(panel.background = element_rect(fill = \"white\"), axis.line.x=element_line(), axis.line.y=element_line()) + scale_y_continuous(breaks=c(0,3,6,9)) + theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.1)))+\nlabs(x =\"Residual\" , y=NULL, caption=\"Figure 2.  Residuals from the public schools model\")"},{"path":"regression-review-and-assumptions.html","id":"what-happens-if-the-assumption-of-homoskedasticity-is-violated","chapter":"2 Regression: review and assumptions","heading":"2.5 What happens if the assumption of homoskedasticity is violated?","text":"Heteroskedasticity present variance error term constant across observations.","code":""},{"path":"regression-review-and-assumptions.html","id":"when-might-heteroskedasticity-occur","chapter":"2 Regression: review and assumptions","heading":"When might heteroskedasticity occur?","text":"state-level country-level observations: accuracy data may function level development. expect highly developed economies record report accurate data economic performancestate-level country-level observations: accuracy data may function level development. expect highly developed economies record report accurate data economic performanceindividual-level observations: types people may give accurate (less accurate) response surveys. Highly educated respondents likely misrepresent voting behavior (knowing expected turn vote)individual-level observations: types people may give accurate (less accurate) response surveys. Highly educated respondents likely misrepresent voting behavior (knowing expected turn vote)data collected time: quality data collected reported improves time- less measurement error - model errors lower today past.data collected time: quality data collected reported improves time- less measurement error - model errors lower today past.true functional form model linear. (Many texts, consistent King Roberts piece linked , note likely culprit observe heteroskedastic errors. Maybe fix underlying cause rather adjust standard errors?)true functional form model linear. (Many texts, consistent King Roberts piece linked , note likely culprit observe heteroskedastic errors. Maybe fix underlying cause rather adjust standard errors?)","code":""},{"path":"regression-review-and-assumptions.html","id":"consequences-of-heteroskedasticity","chapter":"2 Regression: review and assumptions","heading":"Consequences of heteroskedasticity","text":"OLS estimators inefficient.OLS estimators inefficient.Methods used estimate variance biased, t-tests confidence intervals corrupted.Methods used estimate variance biased, t-tests confidence intervals corrupted.estimate coefficient unbiased.estimate coefficient unbiased.","code":""},{"path":"regression-review-and-assumptions.html","id":"diagnostic-tests","chapter":"2 Regression: review and assumptions","heading":"2.6 Diagnostic tests","text":"","code":""},{"path":"regression-review-and-assumptions.html","id":"breusch-pagan-test-statistic-bp","chapter":"2 Regression: review and assumptions","heading":"Breusch Pagan test statistic (BP)","text":"basic test heteroskedasticity Breusch Pagan test statistic (BP). test statistic positive function explained sum squares regression squared OLS residuals independent variables. variance error term function level independent variables test statistic statistically significant. Table 3 reports output bptest function, evaluating heteroskedasticity model estimated.(#tab:model_demobp)Table 3. Breusch-Pagan testTable 3. Heteroskedasticity diagnostics.Since null hypothesis rejected (p~0.00), error term case heteroskedastic.. can perform test manually - regressing X squared error can confirm test statistic plotting squared error versus independent variable. plot confirms existence relationship squared error predictor Income. makes sense poor states many resources spend education, rich states discretion allocate resources spend others. appears one outlier drives still see upward sloping regression line.","code":"\ntemp<-tidy(bptest(fit))\n\nkable(temp[1:2], caption=\"Table 3. Breusch-Pagan test\", digits=3)\nfit$ressq<-fit$residuals^2\nggplot(data=ps, aes(y=fit$ressq, x=Income))+\n  geom_smooth(formula = y ~ x, method = \"lm\")+\n  geom_point() +\n  labs(y =\"Squared residual\" , x=\"Per capita income, in thousands\" , caption=\"Figure 3.  Squared error as a function of per capita income\") +   \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.1)))+\n  labs(x =\"Residual\" , y=NULL) "},{"path":"regression-review-and-assumptions.html","id":"solutions","chapter":"2 Regression: review and assumptions","heading":"2.7 Solutions","text":"find error term heteroskedastic? two types solutions commonly used. large samples, calculate robust standard errors. small samples, rely bootstrapped estimate error. R straightforward implement sandwich package. strategies, use different approaches calculate “variance-covariance” matrix model coefficients. can use stargazer compare results.","code":""},{"path":"regression-review-and-assumptions.html","id":"robust-standard-errors-large-samples","chapter":"2 Regression: review and assumptions","heading":"Robust standard errors (large samples)","text":"details vcov functions sandwich package can find helpful vignette link:ftp://journal.r-project.org/pub/R/web/packages/sandwich/vignettes/sandwich.pdfThe robust heteroskedasiticity-consistent (HC) standard errors rely different formula assume error independent X. many options one described references vignette. formula selected replicates robust standard errors STATA.","code":""},{"path":"regression-review-and-assumptions.html","id":"the-bootstrap-small-samples","chapter":"2 Regression: review and assumptions","heading":"The bootstrap (small samples)","text":"Bootstrapped standard errors calculated repeating regression using public school data creating unique random sample time. software samples replacement number observations dataset (observations may repeated maybe couple times, observations dropped entirely). Every sample unique. regression estimated 100 samples (specified R=100 ), standard deviation coefficient calculated across 100 estimates.","code":""},{"path":"regression-review-and-assumptions.html","id":"how-to-use-this-information","chapter":"2 Regression: review and assumptions","heading":"How to use this information","text":"small dataset, doesn’t take long calculate standard errors approach, calculate compare? going publish use results, probably just take conservative (highest) standard error.implement strategies , using estimating variance-covariance matrix model parameters. example model two parameters (\\(\\beta_0\\) \\(\\beta_1\\)). matrix takes form:\\[\\begin{bmatrix}\n\\beta_{0}^2 & \\beta_{0}\\beta_{1}\\\\\n\\beta_{1}\\beta_{0} & \\beta_{1}^2\\\\\n\\end{bmatrix}\\]focus elements main diagonal (\\(\\beta_0^2\\), \\(\\beta_1^2\\)), elements (covariance parameters) important later semester.chunk calculates bootstrapped standard errors.table includes point estimates standard errors calculated three different ways. point estimates , last set options stargazer function key: NULL shows original standard errors, column 2 robust standard errors column 3 bootstrapped standard errors.\nTable 4. Three different calculations standard error\nRecall rule thumb statistical significance - coefficient twice large standard error, result likely significant. case, \\(\\beta_1\\) 68.94 standard error 18.96 , clearly significant since \\(\\beta_1\\) 3 times standard error.case, bootstrapped standard errors robust standard errors close. Since sample small, use bootstrapped standard errors also report primary result statistically significant whether use generic standard errors, robust standard errors bootstrapped standard errors","code":"# The sandwich package contains the vcovHC functions and vcovBS\n\n# The key here is the type: const is regular standard errors, HC0 gives you what are known as White's adjust standard errors, HC3 replicates robust standard errors in STATA.  All of this is documented in the sandwich package documentation. \n\n#cov1 is the variance covariance matrix calculated with an adjustment that compensates for the presence of heteroskedasticity\ncov1         <- vcovHC(fit, type = \"HC3\")\n\n# this shows you what the matrix produced above looks like\n# You don't need this step to produce the table below\nprint(cov1)\n            (Intercept)     Income\n(Intercept)   19217.444 -2624.6180\nIncome        -2624.618   359.5008\n\n# this step takes the square root of the terms on the main diagonal.\n# So we have the standard error of both parameters: intercept and slope\nrobust_se    <- sqrt(diag(cov1))\n\n# this just shows the standard errors as we will import them into the table\n# You don't need this step to produce the table below.\nprint(robust_se)\n(Intercept)      Income \n  138.62700    18.96051 \n# Calculate standard errors via the bootstrap\ncov2  <- vcovBS(fit, cluster=NULL, use = \"pairwise.complete.obs\", type=\"xy\", R=100 )\nbootstrap_se <-sqrt(diag(cov2))\n\nstargazer(fit, fit, fit, type = \"html\", digits=2, style=\"apsr\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          star.cutoffs = c(0.10,0.05),\n          #column.names = c(\"Standard\", \"Robust\", \"Bootstrapped\"),\n          dep.var.labels=c(\"Public school expenditures per capita\"),\n          covariate.labels=c(\"Per capita income, in thousands\"),\n          omit.stat=c(\"ser\",\"f\"),\n          title=\"Table 4.  Three different calculations of the standard error\",\n          se = list(NULL, robust_se, bootstrap_se))"},{"path":"regression-review-and-assumptions.html","id":"notes-about-the-readings","chapter":"2 Regression: review and assumptions","heading":"2.8 Notes about the readings","text":"","code":""},{"path":"regression-review-and-assumptions.html","id":"a-different-perspective-on-adjusted-standard-errors","chapter":"2 Regression: review and assumptions","heading":"A different perspective on adjusted standard errors","text":"King Roberts (2015) highlight fact heteroskedasticity symptom problems - might productive fix underlying problem, rather adjusted standard errors:fact, robust classical standard errors differ need seen bright red flags signal compelling evidence uncorrected model misspecification. highlight statistical analyses begging replicated, respecified, reanalyzed, conclusions may need serious revision.","code":""},{"path":"regression-review-and-assumptions.html","id":"the-sandwich-package","chapter":"2 Regression: review and assumptions","heading":"The sandwich package","text":"Alternative approaches estimating standard errors parameters described detail sandwich package vignette. authors explain using type=CONST reproduces classical standard errors …others produce different kinds HC (heteroskedasticity-consistent) estimators. estimator HC0 suggested econometrics literature White(1980) justified asymptotic arguments. estimators HC1, HC2 HC3 suggested MacKinnon White (1985) improve performance small samples. extensive study small sample behaviour carried Long Ervin (2000) arrive conclusion HC3 provides best performance small samples gives less weight influential observations. Recently, Cribari-Neto(2004) suggested estimator HC4 improve small sample performance, especially presence influential observations.text cites sandwich package vignette: “Econometric Computing HC HAC Covariance Matrix Estimators” (Zeileis 2004)","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"3 Logistic regression","heading":"3 Logistic regression","text":"","code":""},{"path":"logistic-regression.html","id":"the-linear-probability-model","chapter":"3 Logistic regression","heading":"3.1 The linear probability model","text":"number applications political science involve dependent variables binary: political behavior (vote / don’t vote), judicial politics (affirm / reverse), study Congress (yes / roll call vote). OLS fails used model binary outcome. Specifically, one core assumptions classical linear regression model violated. error term normal mean zero variance \\(\\sigma^2\\).motivate search alternative estimator, consider simple model turnout function education. expectation highly-educated people likely vote - costs lower (easy navigate registration information hurdles) norms participation engagement yield strong payoff terms fulfilling civic duty. Exactly strong effect? , , strong relative factors like income, race gender?","code":""},{"path":"logistic-regression.html","id":"an-example-turnout-as-a-function-of-education-1992","chapter":"3 Logistic regression","heading":"An example: turnout as a function of education, 1992","text":"rely simple linear probability model test impact education. estimate coefficients OLS using lm function. table printed stargazer. example using ANES data 1992 .\nTable 1. Turnout function education, linear probability model\n\ncan see turnout higher people highly educated - slope coefficient positive.Specifically, someone grade school education (1) much less likely report voted someone advanced degree (7). can use constant slope calculate predicted probability groups.grade school education, calculation :\\[p=0.438 + (0.079*1) = 0.517\\]advanced degrees, calculation :\\[p=0.438 + (0.079*7) = 0.991\\]use linear model?","code":"\n# filter to select 1992\ntemp <- anes %>% filter(VCF0004==1992)\n# estimate a linear model of turnout as function of income\n# na.action=na.exclude instructs R\n# to ignore any observations with missing values\nmodel1<-lm(turnout~education, data=temp, na.action=na.exclude)\n# view the model info\nstargazer(model1, style=\"apsr\", type=\"html\",\n          covariate.labels = c(\"Education\", \"Constant\"),\n          dep.var.labels = c(\"Turnout\"),\n          title=\"**Table 1.  Turnout as a function of education, linear probability model**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          star.cutoffs = c(0.10,0.05), digits=3)"},{"path":"logistic-regression.html","id":"challenges-or-limitations","chapter":"3 Logistic regression","heading":"Challenges or limitations","text":"One problem can happen type model expected probability 100%, makes sense. second problem revealed simple inspection residuals. residuals distributed two clusters - mean +0.25 -0.75 (differences predicted values observed outcomes either 1 0). clearly normally distributed.residuals summarized Figure 1. used tidy function broom package pass errors created residuals ggplot produce figure.","code":"\n# Collect the residuals in a dataframe\nerrors<-tidy(residuals(model1))\n\nggplot(data=errors, aes(x=x))+\n  geom_histogram(bins=35, col=\"black\", fill=\"blue\") + \n  labs(y=\"Frequency\", x=\"Residual\") + \n labs(caption=\"Figure 1. Residuals from a linear probability model\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))"},{"path":"logistic-regression.html","id":"logistic-regression-or-logit","chapter":"3 Logistic regression","heading":"3.2 Logistic regression or logit","text":"order overcome problem, use logistic regression, rather OLS, dependent variable takes two values.Using dummy variable dependent variable requires think response (observed “vote” “abstain,” “Republican” “Democrat,” “Incumbent” ’Challenger”) probability. observe outcome, know individual voted probability 1.00 abstained probability 1.00.day election, describe probability someone vote abstain probability: 80% chance respondent vote election tomorrow. Logistic regression estimates unobserved probability someone votes vote, based observed data voting nonvoting group people. group together 100 people 80 percent chance voting, observe 80 voters.","code":""},{"path":"logistic-regression.html","id":"using-a-table-to-understand-the-link-between-education-and-turnout","chapter":"3 Logistic regression","heading":"Using a table to understand the link between education and turnout","text":"Persisting relationship education turnout, produced three tables summarize bivariate distribution education turnout using 1992 ANES. first table just reports proportions zero one education category. second table reports actual observed numbers zeroes ones category. Table 3 neatly summarizes probability turnout category.Table 2. Turnout, education, 1992 US presidential electionYou can see actual probability turnout sample 55% grade-school education 96% highest level education. linear probability model, , slightly -estimated probability turnout lowest level education -estimated probability turnout highest level education.","code":"# Produce and print the simple table of turnout by education level\nround(prop.table(table(temp$education, temp$turnout), margin=1),2)\n   \n       0    1\n  1 0.45 0.55\n  2 0.52 0.48\n  3 0.29 0.71\n  5 0.16 0.84\n  6 0.09 0.91\n  7 0.04 0.96\n\n# And here are the numbers for each group - we will use these below\ntable(temp$education, temp$turnout)\n   \n      0   1\n  1  75  91\n  2 133 123\n  3 211 527\n  5  85 434\n  6  31 323\n  7   7 157\n\n# Produce the table using group_by\n# to get the probability and the number\nd<- temp %>%  filter(!is.na(turnout), !is.na(education)) %>%\n  group_by(education) %>%\n  summarize(turnout = 100*mean(turnout, na.rm = TRUE),\n            count=n(),\n            .groups = 'drop')\n# Produce a table with the kable function and\n# include options to customize column names,\n# limit to 1 digit after the decimal point,\n# include a caption so the table is labeled\nkable(d,\n      digits = 1, \n      caption = \"**Table 2. Turnout, by education, 1992 US presidential election**\",\n      col.names=c(\"Education\", \"Turnout\", \"Count\")) "},{"path":"logistic-regression.html","id":"using-logistic-regression-to-understand-turnout","chapter":"3 Logistic regression","heading":"Using logistic regression to understand turnout","text":"unpacking logistic regression works, can take look example. use glm function implement logistic regression. Using command interpreting coefficients similar OLS regression. coefficients tell whether relationship positive negative. p-value indicates statistical significance. R, goodness fit test statistic reported AIC Akaike’s Information Criteria. discuss test .using glm estimate logit model, function helps us estimate much broader range models learn later term.\nTable 3. Turnout function education, logit\n\noutput looks familiar - table includes coefficients, standard errors, goodness--fit statistics. can see , expected, key coefficient positive - highly educated people likely turn .(Note use survey weights example. can use survey weights glm use assignments since later versions ANES include weights)","code":"\n\n# estimate the logit model\n# We know this a logistic regression since:  family=binomial(link = \"logit\")\nmodel2<-glm(turnout~education, data=temp, family=binomial(link = \"logit\"), na.action=na.exclude)\nstargazer(model2, style=\"apsr\", type=\"html\",\n          covariate.labels = c(\"Education\", \"Constant\"),\n          dep.var.labels = c(\"Turnout\"),\n          title=\"**Table 3.  Turnout as a function of education, logit**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          star.cutoffs = c(0.10,0.05), digits=3)"},{"path":"logistic-regression.html","id":"logistic-regression-the-details","chapter":"3 Logistic regression","heading":"3.3 Logistic regression: the details","text":"Logistic regression differs OLS several ways.model nonlinear: coefficients scale logitthe parameters estimated via maximum likelihoodthe goodness--fit test statistics based likelihoodeffects one predictor conditional others","code":""},{"path":"logistic-regression.html","id":"the-model-is-nonlinear","chapter":"3 Logistic regression","heading":"The model is nonlinear","text":"First, logit estimator nonlinear. scale estimation (coefficients reported table) probabilities scale logit.logit transforms unobserved probabilities (bounded 0 1) unbounded scale centered 0.0. formula transformation \\[logit(p)=\\ln(p/(1-p))\\]\\(p\\) probability, 0 1.\\(p\\)=0.5 logit=0.0If \\(p\\)=.05 logit -3.00If \\(p\\)=0.95. logit 3.00Coefficients reported table indicate much one unit increase X affects logit Y. , , one unit change education produces positive increase 0.48 logit Y.Translating change logit change probability easy. formula transformation :\\[prob(Y)=\\frac{1}{(1+exp^{(-(logit)})}\\]figure maps values logistic [0,1] probability interval","code":"\n# Generate x - a random uniform number from -3.5 to 3.5\n# I do this by taking 200 draws on the 0 to 7 and subtracting each from 3.5\nx<-3.5-runif(200,0,7)\n# Generate the probability y associated with logit x\ny<-1/(1+exp(-x))\n# PUt x and y in a data frame\ndata<-data.frame(x,y)\n# generate the figure\nggplot(data = data, aes(x = x, y = y)) +\n  geom_segment(aes(x=0,xend=0,y=0,yend=1), color=\"red\") +\n  geom_hline(yintercept=c(0.0, 1.0)) +\n  geom_line() + labs(x =\"Value of the logit\" , y=\"Probability\") +\n  coord_cartesian(ylim = c(0,1)) + theme(axis.line=element_blank(), axis.ticks=element_blank()) +\n  scale_x_continuous(breaks=c(-3.0,-1.5, 0.0, 1.5, 3.0))+\nlabs(caption=\"Figure 2. The shape of the logistic distribution\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))"},{"path":"logistic-regression.html","id":"the-coefficients-are-on-the-scale-of-the-logit","chapter":"3 Logistic regression","heading":"The coefficients are on the scale of the logit","text":"logit computed coefficients linear model \\(\\beta_0+\\beta_1X_1\\). determine much one unit change education affects probability voting, compute\\(\\beta_0+\\beta_1X_1\\) education level “1” compare logit (associated probability) education levels.","code":""},{"path":"logistic-regression.html","id":"the-effect-of-education-on-turnout","chapter":"3 Logistic regression","heading":"The effect of education on turnout","text":"used three approaches compute summarize predicted values regression. Using predict function, calculated logit observed value education. Using formula , converted logit probability. Finally, Using predict function designating type=“response” calculated predicted probability education level.Table 4. Predicted valuesThe predicted logit someone grade school education :\\[logit=-0.144= -0.623+(1*0.479)\\]translates \\(p\\) .464 probability 46.4%. Note still underestimates probability voting sample grade school education. mainly due something may noticed tables : people category 2 actually turnout higher rate people category 1, even simple nonlinear model capture .Figure 4 summarizes link education turnout implied model - moving education scale associated positive gains turnout, largest gains category 1 2 small gains category 6 7. say marginal effect education diminishes education increases (may heard phrase “diminishing marginal returns”).","code":"\n# Calculate predicted quantities\n\n# calculate the predicted logit\ntemp$fitted<-predict(model2)\n\n# convert to probability\ntemp$prob<-1/(1+exp(-temp$fitted))\n\n# Or I could grab the probability with the predict function\ntemp$response<-predict(model2, type=\"response\")\n\n\n# Since I only have one variable, in the model I can just create a simple table\ntable3<- temp %>% group_by(education) %>%\n    filter(!is.na(education)) %>%\n    summarize(\n    fitted   = mean(fitted, na.rm=TRUE),\n    probability     = mean(prob, na.rm=TRUE),\n    response = mean(response, na.rm=TRUE))\n\nkable(table3, digits=3, caption=\"**Table 4. Predicted values**\")\n#This plot just generates a figure from the probabilities in Table 3\nggplot(data = table3, aes(x = education, y = response)) +\n  geom_line() + labs(x =\"Education level\" , y=\"Probability of turnout\") + \n  scale_x_continuous(breaks=c(1,2,3,4,5,6,7))  +\nlabs(caption=\"Figure 3. Predicted turnout, as a function of education\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))"},{"path":"logistic-regression.html","id":"parameters-are-estimated-via-maximum-likelihood","chapter":"3 Logistic regression","heading":"3.3.1 Parameters are estimated via maximum likelihood","text":"second major difference logistic regression OLS parameters estimated via known maximum likelihood estimator. rely likelihood function - likelihood data (“0” “1”) observed given proposed parameters model? proposed parameters often designated \\(\\Theta\\). likelihood function maximized - parameters model generate highest likelihood observing ANES sample outcomes? calculation complex, intuition straightforward - want choose parameters model make likely observe outcomes sample given choice model.calculate likelihood? Consider observation 40% chance voting (\\(p\\)=0.40): observe “1” calculate likelihood observation 0.40 (\\(p\\)). observe “0” calculate likelihood observations 0.60 (1-\\(p\\)). every observation contributes likelihood.Given predicted probability unique observation observed 0 1, can multiply likelihoods together get number. want maximize number - use parameter estimates return highest possible value.\\[p(y\\mid\\Theta)=\\prod_{=1}^Np(y_i\\mid\\Theta)\\]Technically, accomplished taking log likelihood function maximizing log. log? log product likelihoods equal sum logs likelihood, calculation much easier feasible log likelihoods. log 1 zero - log small probability -5, sum logs large negative number. closer zero better.\\[\\ln p(y\\mid\\Theta)=\\sum_{=1}^Nlnp(y_i\\mid\\Theta)\\]walk details, stick numbers education. Looking table , 166 people grade-school educations - predicted probability turnout level education 0.464. observe 75 “0” 91 “1.” group’s contribution likelihood :\\[log((1.0-0.464)^{75} + log(0.464)^{91}\\]numbers add -116.6. calculate pair numbers 7 education categories end log-likelihood -1108.6. another combination parameters (intercept, slope) produce better (closer zero) log-likelihood.solution obtained via calculus - can determine equation likelihood function, take first derivative respect parameters (\\(\\beta_0\\) \\(\\beta_1\\)), set zero (maximum peak curve) solve \\(\\beta_0\\) \\(\\beta_1\\). discuss estimation strategy detail later chapters.","code":""},{"path":"logistic-regression.html","id":"the-goodness-of-fit-test-statistics-are-based-on-the-likelihood","chapter":"3 Logistic regression","heading":"The goodness-of-fit test statistics are based on the likelihood","text":"third difference OLS evaluate goodness--fit. evaluate model fit, can compare likelihood simple naive model treats every observation probability turnout (constant) likelihood 3-variable model, adding income age simply model turnout function education. statistical test compare likelihood LR test. example . estimate constant-model, education model compare log-likelihood 3-variable model.Table 5. Likelihood ratio (LR) testYou can see log-likelihood gets closer zero -1124 -1012 -920 - suggesting improved fit. improvement log-likehood statistically significant? LR indicating yes, significant \\(p<0.001\\).formula LR test :\\[LR = -2 ln\\left(\\frac{{L}(m_0)}{{L}(m_3))}\\right) = 2(logL(m_3)-logL(m_0))\\]\nlog likelihood constant model -1123,88 (\\(L(m_0)\\))log likelihood full model -920.49. (\\(L(m_3)\\))\\[ 2*(-920.49 - -1123.88) =  406.78 \\]","code":"\n# I am recreating the 1992 dataset to sweep out any observations with any missing data before I re-estimate the models so that I have the same number of observations in each regression.\n\ntemp <- anes %>% filter(VCF0004==1992, !is.na(turnout), !is.na(education), !is.na(income), !is.na(age), !is.na(turnout))\n\n# first estimate the new model with education, age and income as predictors\nmodel3<-glm(turnout~education+age+income, data=temp, na.action=na.exclude, family=binomial(link=\"logit\"))\n\n# estimate the constant only model\nmodel0<-glm(turnout~1, data=temp, family=binomial(link = \"logit\"), na.action=na.exclude)\n\nlogLik(model0)\n'log Lik.' -1123.883 (df=1)\n\n# estimate education only with the trimmed data\nmodel2a<-glm(turnout~education, data=temp, family=binomial(link = \"logit\"), na.action=na.exclude)\n\nlogLik(model2a)\n'log Lik.' -1012.031 (df=2)\n\n# Check the likelihood for the multivariate model\n\nlogLik(model3)\n'log Lik.' -920.4862 (df=4)\noptions(knitr.kable.NA = '')\n#use the lrtest function from the lmtest package to generate the statistics\ngfstats<-tidy(lrtest(model0, model3))\ngfstats$p.value[2]=sprintf(\"%.03f\", gfstats$p.value[2])\nkable(gfstats, digits = c(1, 2, 1, 2, 3), col.names=c(\"df\", \"Log-likelihood\", \"df\", \"chi-square\", \"p\"), caption=\"**Table 5. Likelihood ratio (LR) test**\")"},{"path":"logistic-regression.html","id":"what-is-pseudo-r-squared","chapter":"3 Logistic regression","heading":"What is pseudo R-squared?","text":"don’t think pseudo R-squared particularly useful summary model fit, pervasive literature. pseudo R-squared function model likelihood, \\(L(m)\\)\\[pseudo{\\text -}R^2 = 1-((L(m_0)/L(m_3))\\]\\(L(m_0)\\) constant log-likelihood \\(L(m_3)\\) full model log likelihood. variant likelihood ratio test.Note: constant represents constant model logit probability turnout. probability overall turnout:\\[p=1/(1+exp(-1.1444)) = 0.758 \\]log likelihood constant model -1123,88 (\\(L(m_0)\\))log likelihood full model -920.49. (\\(L(m_3)\\))formula pseudo R-squared :\\[ 1-(L(m_3)/L(m_0)=1-(-920.49/-1123.88)=0.180 \\]might statistic undesirable absolute goodness fit summary? way pseudo R-squared can approach 1.0 \\(L_m*\\) approaches zero. mean predicted value 1.0 (extremely high logit) respondents vote predicted value 0.0 (extremely low logit) respondents vote. unlikely observed practice, pseudo r-squared likely fairly low. statistic remains, like regular R-squared, useful comparison across models.","code":""},{"path":"logistic-regression.html","id":"other-measures-of-model-fit","chapter":"3 Logistic regression","heading":"Other measures of model fit","text":"may see two measures model fit reported maximum likelihood framework. Bayesian Information Criteria (BIC) Akaike’s Information Criteria (“” Information Criteria). AIC BIC simple functions log likelihood function evaluated particular set parameter estimates.\\[AIC = −N * L(m_*)+2k\\]\\[BIC = −2 *L(m_*)+k*\\log N\\]N number observations, k number model parameters, \\(L_(m_*)\\) log likelihood.model lowest IC best fit. set literature documents circumstances information criterion appropriate see used interchangeably political science work typically criteria point model specification best fit.","code":""},{"path":"logistic-regression.html","id":"effects-of-one-predictor-are-conditional-on-all-others","chapter":"3 Logistic regression","heading":"Effects of one predictor are conditional on all others","text":"Another feature logistic regression requires care - also useful - marginal effect variable depends value variables.consider works, estimate model use estimates model 3 predictors - age, education income. Using multiple predictors couple advantages. can isolate effect variable (talk logic - controlled comparison - next week). logit, can also pick interactions way - words, saw marginal effect education diminished higher levels education. marginal effect education also diminish people older high income? talk interaction effects Chapter 4.estimates 3-variable model reported Table 6. variables matters expected way: older, affluent highly-educated respondents likely report voted. compare AIC model tables, can see model fits data better education alone.\nTable 6. Turnout function education, age income\n\nmargins package useful interpret regression. ‘margins’ function calculates, default, Average Marginal Effect variable model - average marginal effects observed combinations age, education, income observed sample data. marginal effect impact one-unit change X probability Y. table ,Table 7. Average marginal effects (AME)\nmargins package also lets look conditional marginal effects. Conditional marginal effects just means marginal effects particular combinations X variables. functions evaluate marginal effect education conditional education (similar figure ), income followed age. figure, one variable varied along X-axis others held constant mean. default, cplot produces figure. suppress figure keep output pass numbers ggplot.marginal effect education clearly diminishes highly educated, higher income older. low-income, young, low level education, moving one category education increase probability turnout 0.10 (ten percent).","code":"\n\nstargazer(model3, style=\"apsr\", type=\"html\",\n          covariate.labels = c(\"Education\", \"Age\",\"Income\", \"Constant\"),\n          dep.var.labels = c(\"Turnout\"),\n          title=\"**Table 6.  Turnout as a function of education, age and income**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          star.cutoffs = c(0.10,0.05), digits=3)\n# In this chunk, we use the margins command to learn the marginal effects of each predictor\nm<-margins(model3)\n# This summary simply reports the Average Marginal Effect\n# Marginal just means the effect of a one unit change in X\nkable(summary(m), caption=\"**Table 7. Average marginal effects (AME)**\")\n\nfigure_3<-cplot(model3, x=\"education\", dx=\"education\", what=\"effect\", draw=FALSE)\nggplot(figure_3, aes(x = xvals)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"gray70\") +\n  geom_line(aes(y = yvals)) +\n  xlab(\"Education in 7 categories\") +\n  ylab(\"Marginal Effect of Education\") +\n  scale_x_continuous(breaks=c(1,2,3,4,5)) +\n    labs(caption=\"Figure 4. Marginal effect of education, conditional on education\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))\n \nfigure_4<-cplot(model3, x=\"income\", dx=\"education\", what=\"effect\", se.type=\"shade\", draw=FALSE)\n# And pass those numbers to ggplot for a better looking figure\nggplot(figure_4, aes(x = xvals)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"gray70\") +\n  geom_line(aes(y = yvals)) +\n  xlab(\"Income in 5 categories\") +\n  ylab(\"Marginal Effect of Education\") +\n  scale_x_continuous(breaks=c(1,2,3,4,5)) +\n  labs(caption=\"Figure 5. Marginal effect of education, conditional on income\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))\n\n\nfigure_5<-cplot(model3, x=\"age\", dx=\"education\", what=\"effect\", se.type=\"shade\", draw = FALSE)\n\nggplot(figure_5, aes(x = xvals)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"gray70\") +\n  geom_line(aes(y = yvals)) +\n  xlab(\"Age, in years\") +\n  ylab(\"Marginal Effect of Education\") +\n  scale_x_continuous(breaks=c(20,30,40,50,60,70)) +\nlabs(caption=\"Figure 6. Marginal effect of education, conditional on age\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))"},{"path":"logistic-regression.html","id":"reading-and-questions","chapter":"3 Logistic regression","heading":"3.4 Reading and questions","text":"","code":""},{"path":"logistic-regression.html","id":"reading-foxlawless2004","chapter":"3 Logistic regression","heading":"3.4.1 Reading Fox and Lawless (2004)","text":"article posted Elearning interesting good application logistic regression. authors surveyed number people might think appropriate run political office (highly educated professionals). objective figure women -represented political office. -represented pipeline professions? consider running office? consider , take step running? run, win?. focus article middle process - considering running office.read article, notice Table 1 reveals women - controlling factors - less likely run political office. know logistic regression scale - can imagine effect -0.73 quite large.punchline Table 4 - based logistic regression, controlling factors Table 3, men consider qualified office twice likely consider running office (60% men compared 30% women). top fact men nearly twice likely consider “” qualified (26% compared 13%)!Notice exactly - response hold variables mean, vary gender self-perceived qualifications?","code":""},{"path":"logistic-regression.html","id":"what-is-an-interaction-term-and-can-you-use-one-in-this-context","chapter":"3 Logistic regression","heading":"3.4.2 What is an interaction term and can you use one in this context?","text":"controversy need usefulness interaction terms logit model. See (Berry, DeMeritt, Esarey 2010), “Testing Interaction Binary Logit Probit Models: Product Term Essential.” upshot work (1) don’t need product interaction term test interaction effects (2) can see interaction effects use margins commands summarize predicted probabilities – instance report probability behavior White males, White females, African American females African American males learn effect gender racial groups effect race men women. without introducing interaction terms, can learn marginal effects conditional variables model.","code":""},{"path":"using-interaction-terms.html","id":"using-interaction-terms","chapter":"4 Using interaction terms","heading":"4 Using interaction terms","text":"simplify introduction interaction terms, rely model includes one ordinal one dummy variable. know , many cases, may interested variables qualitative. call types variables dummy variables. specifically dummy variable dichotomous typically represents presence absence quality attributes. convention practice code dummy variables 1=presence attribute 0=.","code":""},{"path":"using-interaction-terms.html","id":"political-participation-as-a-function-of-gender","chapter":"4 Using interaction terms","heading":"4.1 Political participation as a function of gender","text":"start simple example: level campaign activity function gender. ANES includes several questions ask type campaign activity involved : trying persuade, displaying preferences, going meetings, giving money, political work. recoded campaign activity measure scale 0 (persuading, attending meeting rallies, wearing buttons stickers, giving money campaign, working candidate) 5 (things). data extracted cumulative ANES election year example 1960. test effect stable time swapping different election year RMD file.","code":""},{"path":"using-interaction-terms.html","id":"comparing-ols-and-a-t-test","chapter":"4 Using interaction terms","heading":"Comparing OLS and a t-test","text":"","code":""},{"path":"using-interaction-terms.html","id":"political-activity-of-women-and-men","chapter":"4 Using interaction terms","heading":"Political activity of women and men","text":"take look simple table first, distribution activity men women. Table 1 summarizes distribution men women: rows table percentage category. sample data tell us common response men women activity, men likely engage type activity. count go high five, activity observed 1960 three.Table 1. Political Activity gender","code":"\n# This table draws on multiple functions:\n# round, prop.table, table, and kable\nkable(round(100*prop.table(table(anes$female_c, anes$activity), margin=1), digits=1), caption=\"**Table 1.  Political Activity and gender**\")"},{"path":"using-interaction-terms.html","id":"a-t-test","chapter":"4 Using interaction terms","heading":"A t-test","text":"first step determine effect gender using t-test evaluate difference mean men women.Table 2. Difference mean activity, men compared women","code":"\n# t.test is a base R function\n# tidy is a function from the broom package. tidy converts test output to a data frame so we can use ggplot and kable to display and plot the results.\n\ntable2<-t.test(activity ~ female, data = anes, var.equal = TRUE) %>%\n tidy() %>%\n select(estimate1, estimate2, statistic, p.value)\n kable(table2, caption=\"**Table 2.  Difference in mean activity, men compared to women**\", digit=3, col.names = c(\"Men\", \"Women\", \"t-test\", \"p-value\"))"},{"path":"using-interaction-terms.html","id":"ols","chapter":"4 Using interaction terms","heading":"OLS","text":"second identical strategy estimate parameters simple model dummy variable indicating female male predictor campaign activity. linear model, estimated via OLS. equivalent ANOVA since variables use predictors categories. coefficients reported Table 3.\nTable 3. Modeling campaign activity function gender\n\nSince coefficient female statistically significant, can conclude gender related level political activity. specifically, can say women less politically active men difference mean activity -0.131. Review R Markdown file notice text dynamic: called result model 1 previous sentence.case dummy variables, constant can substantively useful. know, constant value Y independent variable (X) equal zero. past examples (using dummy variables), independent variables often substantive meaning set zero, constant substantive meaning. dummy variables, hand, independent variable substantive meaning, case, X o zero means respondent male. ,Mean activity level men:\\[E(Y_i | X_i = 0)=\\beta_0+\\beta_1*0\\]\\[ E(Y_i | X_i = 0) = \\beta_0  \\]\\[E(Y_i | X_i = 0) = 0.557\\]Mean activity level women:\\[E(Y_i | D_i = 1)=\\beta_0+\\beta_1*1\\]\\[E(Y_i | D_i = 1) = \\beta_0 +\\beta_1\\]\\[E(Y_i | D_i = 0) = 0.557+ -0.131 = 0.426\\]constant gives us average mean value dependent variable (, level political activity) men, slope (\\(\\beta_1\\)) measures much mean value dependent value women differs men, Notice numbers regression t-test identical - test.","code":"\n# estimate activity as a function of gender\nmodel1<-lm(activity~female, data=anes, na.action=na.exclude)\n\n# view the model info\nstargazer(model1,style=\"apsr\", type = 'html', digits=3,\n          title=\"**Table 3. Modeling campaign activity as a function of gender**\", notes= \"p<.01*** ; p<.05**\",  notes.append = FALSE, dep.var.labels = c(\"Political Activity Scale\"), covariate.labels = c(\"Female\", \"Constant\"), omit.stat=c(\"ser\",\"f\"))"},{"path":"using-interaction-terms.html","id":"controlling-for-education","chapter":"4 Using interaction terms","heading":"4.2 Controlling for education","text":"impact education? saw turnout examples education good predictor whether votes. effect education similar men women? effect gender similar respondents similar levels education?","code":""},{"path":"using-interaction-terms.html","id":"the-logic-of-controllled-comparison","chapter":"4 Using interaction terms","heading":"The logic of controllled comparison","text":"turning focus class, let’s talk logic controlled comparison, specifically leveraging example.intuition controlled comparison primarily interested effect gender (X) activity (Y), know variables (Z) related X Y. case idea might correlation education gender, correlation education activity - case, estimate effect gender, , distorted exclude education. form omitted variable bias.","code":""},{"path":"using-interaction-terms.html","id":"a-simple-example","chapter":"4 Using interaction terms","heading":"4.2.1 A simple example","text":"keep things simple, start considering impact education. control gender, marginal effect education remain consistent, men women?simplest way approach controlled comparison look marginal effects - way X impacts Y - level control variable (Z).Since two categories gender, control gender estimating effect education separately men women – logic answer question: marginal effect education sample compare men women? estimates Table 4 explore possibility.\nTable 4. Modeling campaign activity function education, gender\n\ntable suggests, , yes, see positive effect education men positive effect education women. Education influence level campaign activity effect similar.","code":"\nwomen <- anes %>% filter(female_c==\"Women\")\nmen   <- anes %>% filter(female_c==\"Men\")\nmodel2a<-lm(activity~education, data=women, na.action=na.exclude)\nmodel2b<-lm(activity~education, data=men, na.action=na.exclude)\n# view the model info\nstargazer(model2a, model2b, style=\"apsr\", type = 'html', digits=3, title=\"**Table 4. Modeling campaign activity as a function of education, by gender**\", notes= \"p<.01*** ; p<.05**\",  notes.append = FALSE, dep.var.labels = c(\"Political Activity Scale\"), covariate.labels = c(\"Education\", \"Constant\"), omit.stat=c(\"ser\",\"f\"), column.labels=c(\"Women\", \"Men\"))"},{"path":"using-interaction-terms.html","id":"exactly-how-does-the-effect-of-education-differ-for-men-and-women","chapter":"4 Using interaction terms","heading":"Exactly how does the effect of education differ for men and women?","text":"Figure 1 summarizes estimates model Table 4. red line link education activity women. blue line link education activity men. can see differences predicted campaign activity smaller, 1960, highly-educated men women, larger group lowest levels education. Think means substantively - low level education decreases activity lot women, less men.","code":"\nggplot(women, aes(x = education, y = activity)) +\n  geom_smooth(formula = y ~ x, method=lm, color= \"red\", se=FALSE) +\n  geom_smooth(formula = y ~ x, data=men, method=lm, color= \"blue\", se=FALSE) +\n  scale_x_continuous(name=\"Education\", limits=c(1, 7), breaks=c(1,2,3,4,5,6,7)) +\n  labs( y=\"Predicted activity\", caption=\"Figure 1. Campaign activity as a function of education, for men (blue) and women (red)\")"},{"path":"using-interaction-terms.html","id":"a-controlled-comparison","chapter":"4 Using interaction terms","heading":"4.2.2 A controlled comparison","text":"easier way executve type comparison simply include variables one multivariate model.introduce control variable, one three things can happen:control variable primary variable significant. (effects additive).control variable primary variable significant. (effects additive).primary variable shifts significant . (original result spurious.)primary variable shifts significant . (original result spurious.)primary variable shifts significant significant. (original model confounding.)primary variable shifts significant significant. (original model confounding.)Important note: correlation X Z absolutely change model results. correlation primary control variables enters calculation \\(\\beta_1\\), zero correlation means difference coefficient uncorrelated control added.","code":""},{"path":"using-interaction-terms.html","id":"a-multivariate-model-of-campaign-activity","chapter":"4 Using interaction terms","heading":"4.3 A multivariate model of campaign activity","text":"model estimates political activity function education combines education female. Remember data 1960 - need sensitive means terms role gender education gender education related.Column 1 table reports impact gender alone, replicating result . Column 2 reports impact education alone. Column 3 includes variables, model estimating effect education controlling gender , , gender controlling effect education.Since working across models, helpful differentiate couple phrases:full effect: effect X Y controlsfull effect: effect X Y controlspartial effect: effect X Y presence controls scale \\(X\\beta\\). also labeled marginal effect, think specific chang X generates change Y holding variables constant mean (substantively meaningful level)partial effect: effect X Y presence controls scale \\(X\\beta\\). also labeled marginal effect, think specific chang X generates change Y holding variables constant mean (substantively meaningful level)conditional marginal effects: marginal effects X may conditional value Z (, nonlinear models, value X)conditional marginal effects: marginal effects X may conditional value Z (, nonlinear models, value X)\nTable 5. Modeling campaign activity function gender education\n\nexample , effects gender persist control education effects education persist control gender - effects education gender additive. operate independently - women less active men, people high education active people low levels education.Keep mind: way constructed model assumes education similar impact male female levels activity. Said another way, assuming (estimates conform assumption) mean level activity men different mean level activity women, also assuming (implicitly) rate change activity based educational attainment men women. unit homogeneity assumption - may also see described treatment homogeneitySince know electorate 50 percent women 50 percent men, makes sense average typical marginal effect education, estimated reported Table 6, 0.08. coefficient education, controlling gender (0.08), average effect men (0.05) women (0.11) Table 5.general, can think coefficient reported table weighted average marginal effect X across categories combinations variables.","code":"\n# estimate activity as a function of gender and education\nmodel2<-lm(activity~education, data=anes, na.action=na.exclude)\nmodel3<-lm(activity~female+education, data=anes, na.action=na.exclude)\n\nstargazer(model1, model2, model3, style=\"apsr\", type = 'html', title=\"**Table 5. Modeling campaign activity as a function of gender and education**\",  notes= \"p<.01*** ; p<.05**\", notes.append = FALSE, dep.var.labels = c(\"Political Activity Scale\"), digits=3, covariate.labels = c(\"Female\", \"Education\"), omit.stat=c(\"ser\",\"f\"))"},{"path":"using-interaction-terms.html","id":"using-interaction-terms-1","chapter":"4 Using interaction terms","heading":"4.4 Using interaction terms","text":"noticed something curious results Table 4 Figure 1- effect education positive men women, seems larger women. can test difference statistically significant relying known interaction term. introduction term, simply product education gender (X times Z) permits us relax unit homogeneity assumption. (X Z multiplied together generate interaction terms individual variables referred constitutive variables.)Introduction interaction term typically explicitly theoretically motivated, rather triggered type empirical test.\nTable 6. Modeling campaign activity function gender, education interaction term\n\ninteraction term statistically significant, confirms suspicion effects eduction different men women. exactly much?","code":"\n\n# If you are using the lm function, then you don't need to actually create the interaction term by multiplying, you simply indicate that you want an interaction and R automatically includes both variables and their product in the model.\n\n# estimate model with interaction term\nmodel5<-lm(activity~female*education, data=anes, na.action=na.exclude)\n\n# view the model info\nstargazer(model1, model2, model5, style=\"apsr\", type = 'html', digits=3,\n          title=\"**Table 6. Modeling campaign activity as a function of gender, education and an interaction term**\",\n          notes= \"p<.01*** ; p<.05**\",  notes.append = FALSE,\n          dep.var.labels = c(\"Political Activity Scale\"), \n          covariate.labels = c(\"Female\", \"Education\", \"Interaction term\"),\n          omit.stat=c(\"ser\",\"f\"), column.sep.width=c(\"120pt\"))"},{"path":"using-interaction-terms.html","id":"using-model-coefficients-to-clarify-the-size-of-effects.","chapter":"4 Using interaction terms","heading":"4.4.1 Using model coefficients to clarify the size of effects.","text":"basic model one dummy, one ordinal variable interaction term :\\[Y=\\beta_0+\\beta_1D_1+\\beta_2X_1+\\beta_3(D_1X_1)\\]can use coefficients sample calculate expected mean activity level men:\\[E(Y_i | X_i = 0) = \\beta_0 + \\beta_1*(0) + \\beta_2*(education) + \\beta_3*(0)*(education)\\]\\[E(Y_i | X_i = 0) = 0.408 + 0.0543*(education)\\]expected mean activity level women:\\[ E(Y_i | X_i = 1) = \\beta_0 +\\beta_1*(1) + \\beta_2*(education) + \\beta_3*(1)*(education) \\]\\[ E(Y_i | X_i = 1) = (\\beta_0 +\\beta_1) + (\\beta_2+\\beta_3)*(education) \\]\\[ E(Y_i | X_i = 1) =0.112 + 0.108*education \\]Compare results interaction term model results estimated model men women separately - exactly .use interaction term dummy variable equivalent testing effect X Z running two separate regressions.conclude marginal effect education activity conditional gender. conditional marginal effect men \\(\\beta_2\\) conditional marginal effect women \\(\\beta_2+\\beta_3\\).","code":""},{"path":"using-interaction-terms.html","id":"summarizing-marginal-effects-with-figures","chapter":"4 Using interaction terms","heading":"4.4.2 Summarizing marginal effects with figures","text":"relatively easy calculate conditional marginal effects margins package, consistent prescriptions outlined Brambor, Clark, Golder (2006), discussed detail .use cplot function generate data figures, `ggplot’ produce figures. red dashed line shows marginal effect average level education. solid dashed line zero marginal effect.first figure summarizes conditional marginal effect gender, education. second figure summarizes conditional marginal effect education, gender. Notice figures different Figure 1: y-axis marginal effect, predicted outcome.first figure reveals gender matters women lower average level education. higher levels education, effect zero. (red dotted line indicates average level educationr - , average, diffrence men women -0.13, matching output models (Table 3)).second figure suggests marginal effect education somewhat higher women men, gaps activity men women diminish education increases. confidence intervals suggest really can’t make claim effect much larger women. can confident learn Figure 5.","code":"\n# Use cplot to generate the data and refine the figure with ggplot\ncplotdata1 <- cplot(model5, x = \"education\", dx = \"female\", what=\"effect\", n=7, draw=FALSE)\n# Figure 2\nggplot(cplotdata1, aes(x = xvals, y = yvals)) +\n  geom_line(lwd = 1.5) +\n  geom_line(aes(y = upper)) +\n  geom_line(aes(y = lower)) +\n  labs(x=\"Education\", y=\"Marginal effect of gender\", caption=\"Figure 2. Marginal effect of gender, by level of education\")+\n  geom_hline(yintercept=0.00, lty=2) +\n  geom_vline(xintercept=mean(anes$education, na.rm=TRUE), color=\"red\", lty=2)+\n  scale_x_continuous(name=\"Education\", limits=c(1, 7), breaks=c(1,2,3,4,5,6,7)) \n# Use cplot to generate the data and refine the figure with ggplot\ncplotdata2 <- cplot(model5, x = \"female\", dx = \"education\", what=\"effect\", n=2, draw=FALSE)\nggplot(cplotdata2, aes(x = xvals, y = yvals)) +\n  geom_point() +\n  geom_errorbar(data=cplotdata2, mapping=aes(x=xvals, ymin=upper, ymax=lower), width=0.1, size=1, color=\"blue\") +\n  theme(axis.title.x=element_blank(), axis.ticks.x=element_blank()) +\n  labs(x=NULL, y=\"Marginal effect of education\", caption=\"Figure 3. Marginal effect of education, by gender\") +\n  scale_x_continuous(breaks=c(0,1), limits=c(-0.5,1.5), labels=c(\"Men\", \"Women\"))"},{"path":"using-interaction-terms.html","id":"readings-and-notes","chapter":"4 Using interaction terms","heading":"4.5 Readings and notes","text":"Brambor, Clark, Golder (2006), widely cited piece interaction terms, gives basic useful guidance handle interactions statistical models.","code":""},{"path":"using-interaction-terms.html","id":"include-interaction-terms","chapter":"4 Using interaction terms","heading":"Include interaction terms","text":"compelling reasons think marginal effects vary across observations - due individual-level differences, institutional social contexts- include terms. Otherwise risk model specification error. possible downside adding interaction terms interacted variables variables, complex inefficient model.","code":""},{"path":"using-interaction-terms.html","id":"include-all-constitutive-terms-and-dont-worry-about-collinearity","chapter":"4 Using interaction terms","heading":"Include all constitutive terms (and don’t worry about collinearity)","text":"Review: collinearity? can imagine introduce variable X three dummy variables, proceed test conditional marginal effects three interaction terms, might strong correlation X interaction terms (X*Di). form collinearity reduce efficiency, increasing likelihood Type II error. temptation economize dropping variables - either just leaving interaction terms excluding variables significant. (shorcut good idea case). Brambor et al demonstrate results biased interaction terms included one constitutive terms dropped. combine imperative include interaction terms include constitutive terns, anticipate inefficiency may price pay accurate specification. (special case classic trade-– variables means inefficiency variables risks omitted variable bias).","code":""},{"path":"using-interaction-terms.html","id":"do-not-interpret-constitutive-terms-as-unconditional-marginal-effects","chapter":"4 Using interaction terms","heading":"Do not interpret constitutive terms as unconditional marginal effects","text":"use interaction terms, coefficient X longer unconditional marginal effect X Y. must know unconditional marginal effect, just plot marginal effect check number typical average value X ().","code":""},{"path":"using-interaction-terms.html","id":"calculate-substantively-meaningful-marginal-effects-and-measures-of-uncertainty","chapter":"4 Using interaction terms","heading":"Calculate substantively meaningful marginal effects and measures of uncertainty","text":"Brambor et al highlight fact can’t simply think standard errors additive - can careful clarify exactly differences - marginal effects outcomes - statistically significant. Figure 2 communicates information need - unconditional marginal effect, conditional marginal effects, measures uncertainty effect.","code":""},{"path":"working-with-time-series.html","id":"working-with-time-series","chapter":"5 Working with time series","heading":"5 Working with time series","text":"","code":""},{"path":"working-with-time-series.html","id":"modeling-presidential-approval","chapter":"5 Working with time series","heading":"5.1 Modeling presidential approval","text":"assignment due two weeks model predict approval US presidents function economic conditions war. dependent variable presidential approval, measured using Gallup poll. 2010 article Public Opinion Quarterly, Benny Geys concludes presidential approval influenced US spending foreign conflicts (Geys 2010). relied data 1948 2008, capturing experience 11 US Presidents four long-term foreign conflicts (Korea, Vietnam, Iraq Afghanistan 9/11). data organized quarter, starting 1948 Q2 ending 2008 Q3. Gallup Poll, administered dozens times every year (years, several times month), often includes question “approve disapprove way [president’s name] handling job president?” approval rating simply percentage population answers “approve.” data summarized can see long-run average probably around 50%. Work political scientists linked level approval rating state economy highlighted fact approval ratings immediately following election typically higher average (“honeymoon” period). work linked increasing casualties war declining presidential approval Geys starts piece claim consider fiscal well human costs.quarterly level presidential approval summarized figure .Data organized date known time series data form introduce particular challenges arcane names like autocorrelation nonstationarity. particular compelling advantage using data organized date - can add new variables - anything can imagine - expect relevant.cover challenges turn:means stationary series, important, transform nonstationary series stationary series.means stationary series, important, transform nonstationary series stationary series.means serial correlation, important know, fix problem.means serial correlation, important know, fix problem.use presidential approval data introduce tests stationarity, estimate simple model see serial correlation looks like, next week talking alternative ways fix problem.","code":"\n\n# Use the haven package read_dta function to read the STATA dataset\npoq<- read_dta(\"data/poq_guys.dta\")\n\n# Use the lubridate make_date function to create a date\n# Notice the trick to convert the quarter to the first month of the quarter - be careful with merging data to make sure it is matched with the right period\npoq <- poq %>% \n  mutate(\n    date = make_date(year, (3*quarter-2)))\n\n# Create a figure\nggplot(poq, aes(y=approval, x=date)) +\n  ggtitle(\"Figure 1. Presidential approval, 1948-2008\") +\n  geom_line(color=\"#095872\", size=1) +\n  theme(plot.title.position = \"plot\", plot.title = element_text(face=\"bold\")) +\n  labs(x=\"Date\", y=\"Percent approving\")"},{"path":"working-with-time-series.html","id":"nonstationary-data-can-lead-to-spurious-findings","chapter":"5 Working with time series","heading":"5.2 Nonstationary data can lead to spurious findings","text":"first step working time series data test series stationary. Statisticians working time series data uncovered serious problem standard econometric techniques applied time series: estimation parameters OLS model produces statistically significant results time series contain trend otherwise random. revelation dates back mid-1920s, piece titled “sometimes get nonsense-correlations Time-Series?” (Yule 1926). finding led considerable work determine properties time series must possess econometric techniques used. basic conclusion time series used econometric applications must stationary. (discuss one exception – next week – two nonstationary time series cointegrated.).","code":""},{"path":"working-with-time-series.html","id":"determining-the-properties-of-a-series","chapter":"5 Working with time series","heading":"Determining the properties of a series","text":"can think time series composed four different elements: random error, trend, drift, memory (much value today depends value yesterday.) put together, equation looks like :\\[Y_t=\\rho Y_{t-1}+\\mu+\\beta t +\\epsilon_t\\]memory captured \\(\\rho\\). \\(\\rho=0\\), memory. value \\(\\rho>1\\) \\(\\rho<-1\\), series explosive.drift represented \\(\\mu\\).trend represented \\(\\beta t\\). \\(\\beta=0\\), trend.parameters zero, left white noise random error (\\(\\epsilon_t\\)).special case \\(\\rho\\) =1. time series described random walk variable interest (Yt) function past values plus random error:\\[Y_t=Y_{t-1}+\\epsilon_t\\]Notice \\(\\rho=1\\), \\(\\beta=0\\), \\(\\mu=0\\)Yt may accompanied constant, \\(\\mu\\), means best guess Yt+1 Yt+\\(\\mu\\). designated random walk drift.Rather depending upon Yt-1 , Yt may simply function deterministic trend. case, Yt function \\(\\beta*t\\) \\(\\epsilon_t\\). designated trend-stationary process.general, series stationary \\(-1<\\rho<1\\) formally test . (\\(\\rho\\)=1, random walk, series stationary.","code":""},{"path":"working-with-time-series.html","id":"some-examples","chapter":"5 Working with time series","heading":"Some examples","text":"give sense difference stationary series one stationary, create several series random numbers, one entirely random, three \\(\\rho=0.7\\) three unit root \\(\\rho=1\\), one explosive.first three series appear stationary - constant mean constant variance across range series. fourth series - ar(1) trend - appears constant variance mean increases time. nextlast three series stationary. fourth higher mean middle time series. next two (explosive series) mean increases time. Notice ar(1) drift stationary random walke drift . really matters autoregressive parameter lower 1.0.","code":"\n\n# Adapted from Principles of  Econometrics with R\n# https://bookdown.org/ccolonescu/RPoE4/\n\nN <- 500\na <- 0.5\nl <- 0.01\nrho <- 0.7\n\nset.seed(246810)\nv <- ts(rnorm(N,0,1))\n\n# Random noise\ny4 <- ts(rep(0,N))\nfor (t in 2:N){\n  y4[t]<- v[t]\n}\nplot(y4,type='l', ylab=\"\", main=\"Random or white noise\")\nabline(h=0)\n\n# AR(1)\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- rho*y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"AR(1)\")\nabline(h=0)\n\n# AR(1) with drift\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- a+rho*y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"AR(1) with drift\")\nabline(h=0)\n\n# AR(1) with drift and trend\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- a+l*time(y)[t]+rho*y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"AR(1) with trend and drift\")\nabline(h=0)\n\n# Random walk\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"Random walk\")\nabline(h=0)\n\n# Random Walk with drift\ny3 <- ts(rep(0,N))\nfor (t in 2:N){\n  y3[t]<- a+y3[t-1]+v[t]\n}\nplot(y3,type='l', ylab=\"\", main=\"Random walk with drift\")\nabline(h=0)\n\n# Random walk with trend and drift\"\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- a+l*time(y)[t]+y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"Random walk with trend and drift\")\nabline(h=0)\n\n\n# explosive\ny <- ts(rep(0,N))\nfor (t in 2:N){\n  y[t]<- 1.075*y[t-1]+v[t]\n}\nplot(y,type='l', ylab=\"\", main=\"Explosive\")\nabline(h=0)"},{"path":"working-with-time-series.html","id":"a-statistical-test","chapter":"5 Working with time series","heading":"5.2.1 A statistical test","text":"characteristics time series – mean variance - constant time? mean variance constant time, series stationary. mean variance change, series nonstationary.can get idea problems nonstationarity just looking plot. series appear stable mean trend? variance series appear constant time?can use simple statistical test estimate parameters times series using, identify trend drift, asses value \\(\\rho\\). don’t rely intuition based figures, estimate properties sample data.can know series stationary? Remember key thing want know value \\(\\rho\\) reject idea value parameter 1.0.order test revisit equation :\\[Y_t=\\rho Y_{t-1}+\\mu+\\beta t +\\epsilon_t\\]Subtract Yt-1 side equation :\\[Y_t-Y_{t-1}=(\\rho-1) Y_{t-1}+\\mu+\\beta t +\\epsilon_t\\]can use OLS estimate parameters equation. output presidential approval variable.output known Dickey-Fuller test.Since counter (\\(t\\)) statistically significant, trend. since (\\(\\rho-1\\)) different zero, series stationary. constant zero, ar(1) drift. can reject hypothesis series random walk.coefficient lagged value series zero, \\((\\rho - 1)=0\\) series stationary (series contains called unit root). example , (\\(\\rho - 1) < 0\\) (coefficient -0.139 coefficient statistically significant). means \\(\\rho<1\\). typically see stationary series.\\(\\beta>0\\) series contains trend. example \\(\\beta=0\\).\\(\\beta=0\\) (significant) \\((\\rho - 1)\\) zero (significant), series stationary.approval series stationary.can use test make sure variables order model stationary.key thing avoid regressing nonstationary \\(Y\\) nonstationary \\(X\\).","code":"\n# Create some new variables and identify the variables retained for the assignment.\n\n# WAR VARIABLES\n# Create a duration counter including each conflict\npoq$weary<-poq$weary_kor+poq$weary_Viet+poq$weary_Afgh\n# Create a single casualty variable\npoq$casualty<-poq$vietnam+poq$iraqcas+poq$koreacas\n# Create a cumulative casualty variable\npoq$cumulative<-poq$koreacasCUM+poq$VietcasCUM+poq$iraqcasCUM\n# Create a dummy variable for war\npoq$wardummy<-poq$Iraqdum+poq$Vietdum+poq$Koreadum\n# The defense spending variable is DoDspend\n\n# ECONOMIC VARIABLES\n# Inflation is  Infl_new\n# Unemployment is unempl\n# Economic growth is growth\n# Michigan Index of consumer sentiment is mics\n\n# CONTROLS\n# Honeymoon is honeymoon\n# Divided government is dg\n# Election year is elyear\n# Watergate is water\n\n# Create the data for the paper and use this data for the rest of the notes.\n\npoq<-poq %>% select(date, approval, unempl, Infl_new, growth, mics, weary, casualty, wardummy, cumulative, honeymoon, water, elyear, dg, DoDspend) %>% filter(date<\"2009-01-01\")\n# create lag and difference\npoq$l.approval<-lag(poq$approval, 1, na.pad=TRUE)\npoq$d.approval<-poq$approval-poq$l.approval\n# Add a counter\n\npoq$count<-c(1:nrow(poq))\n\ndf_example<-lm(d.approval~l.approval+count, data=poq)\n# I am using summary() rather than stargazer since I want to show the t-values\nsummary(df_example) \n\nCall:\nlm(formula = d.approval ~ l.approval + count, data = poq)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.724  -3.869  -0.609   2.625  33.667 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.291512   2.051907   4.041 7.19e-05 ***\nl.approval  -0.139016   0.033121  -4.197 3.82e-05 ***\ncount       -0.006526   0.006131  -1.064    0.288    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.592 on 238 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.07053,   Adjusted R-squared:  0.06272 \nF-statistic:  9.03 on 2 and 238 DF,  p-value: 0.0001659"},{"path":"working-with-time-series.html","id":"why-we-are-concerned-about-spurious-regression","chapter":"5 Working with time series","heading":"5.2.1.1 Why we are concerned about spurious regression","text":"can create random time series, \\(Y_1\\), random time series, \\(X\\). also create third series, \\(Y_2\\), linear function \\(X\\).able use OLS determine \\(Y_i\\) unrelated (random) related. table reports coefficients regressions evaluate links.\nNotice statistically significant link \\(Y_1\\) X, even though totally random unrelated. spurious result risk reporting variables stationary.","code":"\n# Adapted from Principles of Econometrics with R\n\nset.seed(246810)\nv1 <- ts(rnorm(N,0,1))\nv2 <- ts(rnorm(N,0,1))\nv3<-  ts(rnorm(N,0,1))\n\n# Create a series with a unit root for X and Y1\n\nY1 <- ts(rep(0,N))\nfor (t in 2:N){\n  Y1[t]<- Y1[t-1]+v1[t]\n}\n\nX <- ts(rep(0,N))\nfor (t in 2:N){\n  X[t]<- X[t-1]+v2[t]\n}\n\n# Create Y2 as a linear function of X plus an error\n\nY2 <- 2+5*X+v3\n\ndata<-cbind(Y1,Y2,X)\nm1<-lm(Y1~X, data=data)\nm2<-lm(Y2~X, data=data)\nstargazer(m1, m2, type = 'text',  model.numbers=FALSE, omit.stat=c(\"ser\",\"f\"))\n\n=========================================\n                 Dependent variable:     \n             ----------------------------\n                   Y1            Y2      \n-----------------------------------------\nX               0.142***      5.000***   \n                (0.038)        (0.006)   \n                                         \nConstant       -3.966***      1.965***   \n                (0.419)        (0.063)   \n                                         \n-----------------------------------------\nObservations      500            500     \nR2               0.027          0.999    \nAdjusted R2      0.025          0.999    \n=========================================\nNote:         *p<0.1; **p<0.05; ***p<0.01"},{"path":"working-with-time-series.html","id":"using-the-urca-package-and-the-ur.test-function","chapter":"5 Working with time series","heading":"5.2.2 Using the urca package and the ur.test function","text":"can pursue simple test stationarity using ur.test function urca package. remain focused test unit root, might use regression test presence trend. One wrinkle: standard p-value used assess statistical significance, critical value reported end output use evaluate whether series significant (regression result may show p<0.05 need check critical value)Note key statistic \\(tau3\\) t-value associated first lag sereis (\\(z.lag1\\)) : significant means stationary.output just shows results test simulated series - random variable (y4) figures unit root (y3) set figures.can see test stationary series rejects null- series identified stationary.test unit root created reject null - series stationary.Use tests determine time series stationary well-established practice.","code":"summary(ur.df(y=poq$approval, lags=0, type='trend'))\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.724  -3.869  -0.609   2.625  33.667 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.284986   2.049168   4.043 7.13e-05 ***\nz.lag.1     -0.139016   0.033121  -4.197 3.82e-05 ***\ntt          -0.006526   0.006131  -1.064    0.288    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.592 on 238 degrees of freedom\nMultiple R-squared:  0.07053,   Adjusted R-squared:  0.06272 \nF-statistic:  9.03 on 2 and 238 DF,  p-value: 0.0001659\n\n\nValue of test-statistic is: -4.1972 6.0219 9.0305 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47# This command instructs the function to omit missing values and include one lag of Y, as we did above.\nsummary(ur.df(y4,  type='trend', lags=0))  \n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.75720 -0.71175 -0.00893  0.70039  3.04527 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.304e-03  9.104e-02   0.014    0.989    \nz.lag.1     -1.050e+00  4.507e-02 -23.306   <2e-16 ***\ntt          -8.131e-05  3.155e-04  -0.258    0.797    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.015 on 496 degrees of freedom\nMultiple R-squared:  0.5227,    Adjusted R-squared:  0.5208 \nF-statistic: 271.6 on 2 and 496 DF,  p-value: < 2.2e-16\n\n\nValue of test-statistic is: -23.3058 181.0659 271.5939 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.98 -3.42 -3.13\nphi2  6.15  4.71  4.05\nphi3  8.34  6.30  5.36\nsummary(ur.df(y3, type='trend', lags=0))\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.65403 -0.71415  0.03071  0.70711  3.07356 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.471046   0.093106   5.059 5.94e-07 ***\nz.lag.1     -0.009994   0.006514  -1.534    0.126    \ntt           0.004904   0.003263   1.503    0.134    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.014 on 496 degrees of freedom\nMultiple R-squared:  0.004852,  Adjusted R-squared:  0.0008397 \nF-statistic: 1.209 on 2 and 496 DF,  p-value: 0.2993\n\n\nValue of test-statistic is: -1.5343 38.322 1.2093 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.98 -3.42 -3.13\nphi2  6.15  4.71  4.05\nphi3  8.34  6.30  5.36"},{"path":"working-with-time-series.html","id":"variables-may-require-some-elaborate-transformations.","chapter":"5 Working with time series","heading":"5.2.3 Variables may require some elaborate transformations.","text":"happens variable stationary? time series must transformed way – subtracting trend creating first difference, using alternative functional form.several variables choose specify model presidential approval assignment. variables stationary, others . need figure types transformations might appropriate variables implications transformations.like use three variables running example: casualties, index consumer sentiment (subjective perception economy , honeymoon period. expect presidential approval decrease casualties, increase people believe economy well, higher first quarter term.need test variables stationarity, just like tested \\(Y\\) variable.tests reveal couple things - casualties honeymoon variable stationary, index consumer sentiment, first test, . series stationary.","code":"summary(ur.df(poq$honeymoon,  type='trend', lags=0))  \n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8044 -0.1848 -0.1561 -0.1338  3.8678 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.1993973  0.1092503   1.825   0.0692 .  \nz.lag.1     -0.3877001  0.0512282  -7.568 8.27e-13 ***\ntt          -0.0003184  0.0007626  -0.417   0.6767    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.823 on 238 degrees of freedom\nMultiple R-squared:  0.194, Adjusted R-squared:  0.1872 \nF-statistic: 28.64 on 2 and 238 DF,  p-value: 7.133e-12\n\n\nValue of test-statistic is: -7.5681 19.0962 28.6444 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\nsummary(ur.df(poq$casualty,  type='trend', lags=0))  \n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1879.7  -137.8   -52.9     7.8  6918.1 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 215.43037   86.18242    2.50   0.0131 *  \nz.lag.1      -0.20696    0.03927   -5.27 3.05e-07 ***\ntt           -1.08346    0.58260   -1.86   0.0642 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 594.5 on 238 degrees of freedom\nMultiple R-squared:  0.1046,    Adjusted R-squared:  0.09705 \nF-statistic:  13.9 on 2 and 238 DF,  p-value: 1.957e-06\n\n\nValue of test-statistic is: -5.2702 9.265 13.8975 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47\n# I had to filter to get rid of the quarters without mics\ntest<- poq %>% filter(date>\"1952-10-01\")\nsummary(ur.df(test$mics,  type='trend', lags=0))\n\n############################################### \n# Augmented Dickey-Fuller Test Unit Root Test # \n############################################### \n\nTest regression trend \n\n\nCall:\nlm(formula = z.diff ~ z.lag.1 + 1 + tt)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6974  -3.2255   0.1581   3.0782  15.1889 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  8.026702   2.677881   2.997  0.00304 **\nz.lag.1     -0.089116   0.029427  -3.028  0.00275 **\ntt          -0.002522   0.005122  -0.492  0.62287   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.89 on 219 degrees of freedom\nMultiple R-squared:  0.04123,   Adjusted R-squared:  0.03247 \nF-statistic: 4.708 on 2 and 219 DF,  p-value: 0.009953\n\n\nValue of test-statistic is: -3.0284 3.1809 4.7083 \n\nCritical values for test statistics: \n      1pct  5pct 10pct\ntau3 -3.99 -3.43 -3.13\nphi2  6.22  4.75  4.07\nphi3  8.43  6.49  5.47"},{"path":"working-with-time-series.html","id":"we-have-options","chapter":"5 Working with time series","heading":"We have options","text":"Detrend. test stationarity two parts – one test trend.\ntrend variable significant, detrend – remove linear trend. Identify linear trend (regression \\(Y\\) t). Create predicted value. Subtract predicted value original. Note MICS variable linear trend, probably won’t work instance. demonstrate next week.Detrend. test stationarity two parts – one test trend.\ntrend variable significant, detrend – remove linear trend. Identify linear trend (regression \\(Y\\) t). Create predicted value. Subtract predicted value original. Note MICS variable linear trend, probably won’t work instance. demonstrate next week.First–difference. many cases, series can transformed nonstationary stationary taking first difference. typical approach manage nonstationarity. Rather using \\(X_t\\) independent variable, independent variable becomes \\(X_t-X_{t-1}\\). series stationary without transformation designated (0), integrated order 0. series stationary first differences designated (1), integrated order 1.First–difference. many cases, series can transformed nonstationary stationary taking first difference. typical approach manage nonstationarity. Rather using \\(X_t\\) independent variable, independent variable becomes \\(X_t-X_{t-1}\\). series stationary without transformation designated (0), integrated order 0. series stationary first differences designated (1), integrated order 1.transformations. take log use type transformation.transformations. take log use type transformation.Carefully consider substantive implications transformations. Returning presidential approval example, consider implications positive link presidential approval economic conditions.","code":""},{"path":"working-with-time-series.html","id":"how-to-use-mics","chapter":"5 Working with time series","heading":"How to use MICS?","text":"Alternative 1 (raw series). level presidential approval function level MICS. consumer sentiment high, approval high. run regression since know approval (0) mics (1) – model “unbalanced.”Alternative 1 (raw series). level presidential approval function level MICS. consumer sentiment high, approval high. run regression since know approval (0) mics (1) – model “unbalanced.”Alternative 2 (de-trended series). Presidential approval function departure MICS trend. MICS increasing remains increasing, level presidential approval change. rate increase MICS falls (observation trend), presidential approval falls. take approach since know MICS trend.Alternative 2 (de-trended series). Presidential approval function departure MICS trend. MICS increasing remains increasing, level presidential approval change. rate increase MICS falls (observation trend), presidential approval falls. take approach since know MICS trend.Alternative 3 (first difference). Levels presidential approval function changes MICS. MICS falls, presidential approval falls. MICS falls high high, presidential approval falls. change consumer sentiment (remains low remains high), approval expected remain mean. Notice subtle changes theory incorporate transform variable.Alternative 3 (first difference). Levels presidential approval function changes MICS. MICS falls, presidential approval falls. MICS falls high high, presidential approval falls. change consumer sentiment (remains low remains high), approval expected remain mean. Notice subtle changes theory incorporate transform variable.Alternative 4 (percent change). Presidential approval responds just change MICS current level, big change current level, percentage. change 100 120 (+20%) meaningful change 110 120 (+9.1%)Alternative 4 (percent change). Presidential approval responds just change MICS current level, big change current level, percentage. change 100 120 (+20%) meaningful change 110 120 (+9.1%)Alternative 5 (alternative functional form - log). cases, may reasons transform series. uncommon large numbers transformed log logarithm. POQ piece uses data US casualties war, includes log numbers. intuition level casualties matter, even difference last period, percentage change matters - small changes low levels important small changes high levels. can picked log common see series couple high numbers many low numbers transformed using log.Alternative 5 (alternative functional form - log). cases, may reasons transform series. uncommon large numbers transformed log logarithm. POQ piece uses data US casualties war, includes log numbers. intuition level casualties matter, even difference last period, percentage change matters - small changes low levels important small changes high levels. can picked log common see series couple high numbers many low numbers transformed using log.Notice technical prescription may may coincide think happens substantively.","code":""},{"path":"working-with-time-series.html","id":"what-the-transformed-series-look-like","chapter":"5 Working with time series","heading":"5.2.4 What the transformed series look like","text":"can compare plots MICS data using transformations. (Note need check new, transformed series stationary .) just show transformations work.","code":"\n# Create the first differnce\npoq$d.mics<-poq$mics-lag(poq$mics)\n\n#Create  transformed series\n\n#We have to add 1 to make sure zeroes are converted to log=0\npoq$log.mics<-log(poq$mics+1)\n\n# Calculate the percent change for mics\n# First calculate the lag\npoq$l.mics=lag(poq$mics)\npoq$pct.mics=100*(poq$mics-poq$l.mics)/(poq$l.mics+1)  \npoq$d.mics<-poq$mics-poq$l.mics\n\n# compare the plots\n\nggplot(poq, aes(y=mics, x=date)) +  ggtitle(\"Index of Consumer Sentiment\") + geom_line(color=\"blue\", size=1.25) + labs(x=\"Date\", y=\"MICS\")\n\nggplot(poq, aes(y=log.mics, x=date)) +  ggtitle(\"Log, Index of Consumer Sentiment\") + geom_line(color=\"blue\", size=1.25) + labs(x=\"Date\", y=\"Log, MICS\")\n\nggplot(poq, aes(y=pct.mics, x=date)) +  ggtitle(\"Percent change, Index of Consumer Sentiment\") +  geom_line(color=\"blue\", size=1.25) + labs(x=\"Date\", y=\"Percent change from previous quarter\")\n\nggplot(poq, aes(y=d.mics, x=date)) +  ggtitle(\"First difference, Index of Consumer Sentiment\") +\n  geom_line(color=\"blue\", size=1.25) + labs(x=\"Date\", y=\"First difference\")"},{"path":"working-with-time-series.html","id":"residuals-may-be-correlated","chapter":"5 Working with time series","heading":"5.3 Residuals may be correlated","text":"","code":""},{"path":"working-with-time-series.html","id":"what-is-autocorrelation-what-problems-does-autocorrelation-introduce","chapter":"5 Working with time series","heading":"5.3.1 What is autocorrelation? What problems does autocorrelation introduce?","text":"date discussed two common violations assumptions classical linear regression model: one formal assumption (heteroskedasticity) another implicit assumption (unit homogeneity). approached heteroskedasticity nuisance – can select estimators standard errors robust violations? week take similar approach deal problem autocorrelation – also labeled serial correlation special case time series data. Next week adopt different strategy dealing autocorrelation – treating autocorrelation additional information (good thing) data – similar way learned something new added information included interaction term.Autocorrelation label observed correlation error terms across observations model. classical linear regression model assumes errors term uncorrelated – random – across observations. typically observe autocorrelation two applications – observations organized time series (serial correlation) data geographic units share borders (spatial correlation). cases “neighboring” observations – close proximity space time – related. implications OLS?Parameter estimates generated via OLS still unbiased, estimated variances OLS parameters biased. OLS presence serial correlation tends underestimate true variances standard errors, inflate t values, potentially leading erroneous conclusion coefficients statistically different 0. formula used compute error variance (\\(\\sigma^2\\)) biased estimator – usually underestimates actual variance error, , estimated \\(R^2\\) reliable estimate true \\(R^2\\)","code":""},{"path":"working-with-time-series.html","id":"detecting-autocorrelation","chapter":"5 Working with time series","heading":"5.3.2 Detecting Autocorrelation","text":", heteroskedasticity, actually observe error (\\(u\\)) variance\norder determine autocorrelation exists, rely estimate u, \\(e\\).","code":""},{"path":"working-with-time-series.html","id":"starting-with-ols","chapter":"5 Working with time series","heading":"5.3.3 Starting with OLS","text":"Start, always, model, likely model involving time series data. case dependent variable presidential approval predictors Michigan Index Consumer Sentiment indicator honeymoon period (4 first quarter presidential term, 3,2,1 0 first quarter second year). index consumer sentiment (MICS) subjective measure people perceive state econonmy. idea economy good, presidential approval higher. also dummy variable honeymoon period - first quarter presidential term. expect approval higher earlier term, rather later term. use first differnce MICS.\\[Presidential.approval_t = \\beta_0 + \\beta_1(d.mics_t)+ \\beta_2(honeymoon) + u_t\\]\nTable 1. Presidential approval function economy, war honeymoon, OLS\n","code":"\npoq$d.mics=poq$mics-lag(poq$mics)\n\nmodel1<-lm(approval~d.mics+casualty+honeymoon, data=poq)\nstargazer(model1,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 1. Presidential approval as a function of economy, war and the honeymoon, OLS**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          covariate.labels=c(\"MICS, first difference\", \"Casualities\", \"Honeymoon\"), omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))"},{"path":"working-with-time-series.html","id":"using-the-residuals-to-diagnose-problems","chapter":"5 Working with time series","heading":"5.3.4 Using the residuals to diagnose problems","text":", like heteroskedasticity, first thing actually look residuals, determine whether appear follow sort pattern. first figure just plots residuals time look fairly random. second figure shows residuals clearly related - error today positive function error previous quarter.","code":"\nplot(lag(model1$residuals),model1$residuals)"},{"path":"working-with-time-series.html","id":"a-statistical-test-for-autocorrelation","chapter":"5 Working with time series","heading":"5.4 A statistical test for autocorrelation","text":"figure instructive, specific guidance severity problem. instead rely simple measure serial correlation, durbin-watson, defined :\\[d = \\dfrac{\\sum_{= 2}^N (e_i - e_{-1})^2}{\\sum_{= 1}^N e_i^2}\\]\nwords, sum every residual minus lag, squared, divided sum residuals squared. (, accurately, covariance error lagged error divided variance error term). statistic easy compute, even easier given just every statistical package kick Durbin-Watson statistic automatically ask . reporting regression results time series absolutely standard report Durbin-Watson \\(d\\) test autocorrelationSeveral conditions must hold, however, order use Durbin-Watson \\(d\\)First: regression equation must include constant intercept term. reason model include term (sometimes theoretical/statistical reasons suppress intercept), Durbin-Watson appropriateSecond: assume disturbances, \\(u_i\\), generated following underlying mechanism:\\[u_t = \\rho u_{t-1} + v_t  \\textrm{ } \\textrm{  } –1 < \\rho < 1\\]\nform equation familiar. \\(\\rho\\), rho, called coefficient autocorrelation, describes relationship error term past values – assumption error model function relationship last value purely random term, \\(v_t\\).important assumes specific error-generating mechanism known first-order autoregressive, AR(1). mechanism implies relationship errors error immediate lagThis assumption always hold – situations might expect error term second-, third-, fourth- (-) order regressive (quarterly data?)value durbin-watson test statistic approximately (2*(1-\\(\\rho\\))). \\(\\rho\\) = parameter linear model first order autocorrelation aboveFinally, model include lagged values dependent variable one explanatory variables. cover dynamic models next week","code":""},{"path":"working-with-time-series.html","id":"assuming-these-conditions-are-fulfilled-how-do-we-use-the-d-w-d","chapter":"5 Working with time series","heading":"5.4.1 Assuming these conditions are fulfilled, how do we use the D-W d?","text":"d closer 0 means positive autocorrelation, d closer 4 means negative autocorrelationIn order determine close 0 4 close enough determine model either positive negative autocorrelation, upper lower critical values d, depend number observations (N) number explanatory variables (k).null hypothesis : autocorrelation. ouput , test ambiguous. clearly must reject null accept alternative hypothesis - autocorrelation.","code":"dwtest(model1)\n\n    Durbin-Watson test\n\ndata:  model1\nDW = 0.24122, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0"},{"path":"working-with-time-series.html","id":"a-more-contemporary-test-breusch-godfrey-bg","chapter":"5 Working with time series","heading":"5.4.2 A more contemporary test: Breusch-Godfrey (BG)","text":"time series work proliferated advanced, number tests emerged alternatives Durbin-Watson. general consensus use known Breusch-Godfrey test. test two advantages Durbin-Watson. First, test robust including lagged values dependent variable, step take next week. Second, test permits us look serial correlation beyond first lag. error strictly product random error:\\[e_t=v_t\\],\\(v_t\\) mean zero variance \\(\\sigma^2\\), autocorrelation function composed \\(\\rho\\)=0 k>0.described white noise process.serial correlation – lag 1 lags – model, error term appear white noise. formal tests implemented many statistical packages. Breusch Godfrey (BG) test statistic function R^2 regression error t errors lags 1 k. zero close zero errors uncorrelated. Specifically:\\[BG =(T-k)*R^2\\]T number time periods, k number lags tested, R2 goodness fit test statistic regression error lags. bgtest function reports test statistically significant. test significant, residuals correlated. , calculate test statistic manually 1 lag call test lag 4. first number just show test statistic comes . second set output use assignment.","code":"model4<-lm(model1$residual~lag(model1$residual))\nsummary(model4)$r.squared*(222-1)\n[1] 170.4665\nkable(tidy(bgtest(model1, order=1, type = c(\"Chisq\"))))bgtest(model1, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  model1\nLM test = 175.62, df = 4, p-value < 2.2e-16"},{"path":"working-with-time-series.html","id":"next-week-1","chapter":"5 Working with time series","heading":"5.5 Next week","text":"confident variables stationary, can choose modeling strategy helps mitigate problems serial correlation. two common approaches","code":""},{"path":"working-with-time-series.html","id":"arima-models","chapter":"5 Working with time series","heading":"5.5.1 ARIMA models","text":"autoregressive integrated moving average (ARIMA) modeling strategy permits us remedy problem serial correlation treating error term dynamic process. dynamics - influence past current observations - introduced via error term. assumes effects X Y contemporaneous. way (technically substantively) modeling framework past values X cause current values Y past values Y influence current values Y. suggests ARIMA strategy may special case general strategy handling dynamics may appropriate.","code":""},{"path":"working-with-time-series.html","id":"dynamic-models","chapter":"5 Working with time series","heading":"5.5.2 Dynamic models","text":"Dynamic models simply models include lagged (prior) values X Y. Instead treating error dynamic, model persistence dynamics directly, including lagged values X Y? Read Kennedy, Chapter 19 next week. Much going class next week derived Keele Kelly (2006), might review online.","code":""},{"path":"dynamic-models-1.html","id":"dynamic-models-1","chapter":"6 Dynamic Models","heading":"6 Dynamic Models","text":"","code":""},{"path":"dynamic-models-1.html","id":"modeling-presidential-approval-with-dynamics","chapter":"6 Dynamic Models","heading":"6.1 Modeling presidential approval with dynamics","text":"end class last week, knew couple things modeling presidential approval:presidential approval stationary, measure consumer sentiment . use first difference want include economic sentiment model approvalpresidential approval stationary, measure consumer sentiment . use first difference want include economic sentiment model approvalif use OLS estimate relative importance casualties war, economic sentiment, honeymoon period, residuals correlated , violating one key assumptions behind OLS.use OLS estimate relative importance casualties war, economic sentiment, honeymoon period, residuals correlated , violating one key assumptions behind OLS.address problem serial correlation, use two categories solutions:incorporate dynamics error term known ARIMA modelwe incorporate dynamics error term known ARIMA modelwe introduce lagged values dependent independent variables known dynamic models.introduce lagged values dependent independent variables known dynamic models.going use two basic approaches estimate model parameters - functions dyn package arima function part base R.","code":""},{"path":"dynamic-models-1.html","id":"the-arima-model---the-special-case","chapter":"6 Dynamic Models","heading":"6.2 The ARIMA model - the special case","text":"autoregressive integrated moving average (ARIMA) modeling strategy permits us remedy problem serial correlation error term. possible estimate effect X Y period \\(t\\). One way think modeling strategy dynamics - influence past current observations - introduced via error term. assumption implicit model estimated last week: effects X Y contemporaneous. way (technically substantively) modeling framework past values X cause current values Y past values Y influence current values Y. suggests ARIMA strategy may special case general strategy handling dynamics may appropriate.model summarized treats presidential approval function first difference current economic conditions (Michigan Index Consumer Sentiment. MICS), total casualties war quarter, counter indicate honeymoon period (4 first quarter first year falling 0 first quarter second year). Since know MICS variable stationary, use first difference MICS.remedy relies knowledge nature interdependence disturbance terms (\\(u\\)). Recall, basic model :\\[Approval_t = \\beta_0 + \\beta_1(diff(MICS_t))+ \\beta_2(Honeymoon_t) +\\beta_3(Casualty_t)+ u_{t}\\]order use Durbin-Watson test assumed error generated AR(1) mechanism, :\\[ –1<\\rho< 1 \\]\\[ u_t = \\rho u_{t-1} + v_t\\]goal somehow transform model transformed model error term serially independent (autocorrelation)can transform model way, OLS estimates transformed model BLUE, assuming OLS assumptions fulfilled. simply combine equations .\\[Approval_t = \\beta_0 + \\beta_1(diff(MICS_t))+ \\beta_2(Honeymoon_t) +\\beta_3(Casualty_t)+ \\rho u_{t-1} + v_t\\]transformation can generalized higher-order schemes, AR(2), etc., worry . Estimation model simple R using arima function.R implements nonlinear strategy simultaneously estimating \\(\\rho\\) parameters model (\\(\\beta_i\\)). Note approach contemporary superior alternative two techniques might see older journal articles – Prais-Winsten regression Cochran-Orchutt estimation. use OLS get problem. two-step approaches - first estimating \\(\\rho\\) estimating parameters given estimate. tests typically report durbin-watson test statistic evaluate reduction serial correlation. reading literature published , say, 2002, might see approach. substantive statistical findings nearly identical ARIMA approach .\nTable 1. Presidential approval function economy, war honeymoon, ARIMA\ncouple things note output.First, notice estimate \\(\\rho\\), ar(1) parameter: 0.88. indicates lot persistence. Closer 1.0 means process - updating approval new information - takes time - approval sticky.Second, reproduce basic OLS , notice honeymoon variable significant (+3) economic sentiment variable significant, casualties.Finally, notice B-G test insignificant, serial correlation lag 4.","code":"\n\n# Use the haven package read_dta function to read the STATA dataset\npoq<- read_dta(\"data/poq_guys.dta\")\n\n# Use the lubridate make_date function to create a date\n# Notice the trick to convert the quarter to the first month of the quarter\n# be careful with merging data to make sure it is matched with the right period\npoq <- poq %>% \n  mutate(date = make_date(year, (3*quarter-2)))\n\n# WAR VARIABLES\n# Create a duration counter including each conflict\npoq$weary<-poq$weary_kor+poq$weary_Viet+poq$weary_Afgh\n# Create a single casualty variable\npoq$casualty<-poq$vietnam+poq$iraqcas+poq$koreacas\n# Create a cumulative casualty variable\npoq$cumulative<-poq$koreacasCUM+poq$VietcasCUM+poq$iraqcasCUM\n# Create a dummy variable for war\npoq$wardummy<-poq$Iraqdum+poq$Vietdum+poq$Koreadum\n# Other costs of war\n# The defense spending variable is DoDspend\n\n# ECONOMIC VARIABLES\n# Inflation is  Infl_new\n# Unemployment is unempl\n# Economic growth is growth\n# Michigan Index of consumer sentiment is mics\n\n# CONTROLS\n# Honeymoon is honeymoon\n# Divided government is dg\n# Election year is elyear\n# Watergate is water\n# While I use the dyn package below to estimate\n# dynamic models, the arima function requires\n# that I create the first difference variable\n# and then bind all of the X variables together.\n\n# Create the first difference variable dmics\n# this syntax places the first calculated difference in the second row\n# if you forget this, you will get an error message\npoq$dmics[2:nrow(poq)]<-diff(poq$mics)\n\n\nxvar<-cbind(poq$dmics, poq$honeymoon, poq$casualty)\n\n\n# estimate the arima model (requires nlme package)\narima<-arima(poq$approval, xreg=xvar,  order=c(1,0,0))\n\n# produce the table\nstargazer(arima,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          covariate.labels=c(\"ar1\", \"Constant\", \"MICS, first difference\",  \"Honeymoon\", \"Casualities\"),\n          title=\"**Table 1. Presidential approval as a function of economy, war and the honeymoon, ARIMA**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))# test for serial correlation up to lag 4\nbgtest(arima$residual~1, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  arima$residual ~ 1\nLM test = 6.0617, df = 4, p-value = 0.1946"},{"path":"dynamic-models-1.html","id":"dynamic-models-using-the-dyn-package-dl-pa-adl-ecm---a-general-approach","chapter":"6 Dynamic Models","heading":"6.3 Dynamic models using the dyn package (dl, pa, adl, ecm) - a general approach","text":"order use dyn package functions identify variable time series. Notice specify model just indicate diff(mics) - first difference MICS, rather using new variable created .start simple OLS test serial correlation using Breusch-Godrey test, final result last chapter.\nTable 2. Presidential approval function economy, war honeymoon, OLS\npresidential approval depend economic conditions previous quarter, rather current quarter? Isn’t possible sentiments may immediately updated objective conditions change? , case, sentiments might immediately affect approval President? assume ?test presence longer-term effects, introduce lagged values independent dependent variables. Models incorporate lagged variables dynamic models - since effects change X time t effects later values Y, t+1 t+2.","code":"\n# Since I only use this library for this chapter, I call it here rather than in _common.R\nlibrary(dyn)\n# identify each variable as a time series\n# this steps give us the ability to use differences and\n# lags in the dny models below and to bind the columns for arima\nmics<-ts(poq$mics)\napproval<-ts(poq$approval)\nhoneymoon<-ts(poq$honeymoon)\ncasualty<-ts(poq$casualty)\nDoDspend<-ts(poq$DoDspend)\n\n# estimate the static model with no correction for serial correlation\n# approval as a function of the first difference in consumer sentiment, casualties, and the honeymoon period.\n# This is where we left off last week.\nstatic<-dyn$lm(approval~diff(mics)+casualty+honeymoon)\n# view the model info\nstargazer(static,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 2. Presidential approval as a function of economy, war and the honeymoon, OLS**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          covariate.labels=c(\"MICS, first difference\", \"Casualities\", \"Honeymoon\"), omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))# generate the residuals and test for serial correlation\nbgtest(static, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  static\nLM test = 175.62, df = 4, p-value < 2.2e-16"},{"path":"dynamic-models-1.html","id":"distributed-lag-models","chapter":"6 Dynamic Models","heading":"6.3.1 Distributed lag models","text":"multiple lagged values independent variable included, model distributed lag model. effect change X distributed future time periods.Example: suppose true model outcome \\[y=1.0+0.6*x_{t-1}+0.4*x_{t-2}+0.2*x_{t-3}\\]X stable 1 many periods, Y equal \\[1+0.6+0.4+0.2=2.2\\]X rose 2 period t, Y equal :2.2 period t (effect)\n2.8 period t+1\n3.2 period t+2\n3.4 period t+3 (beyond)difference \\(x=1\\) \\(x=2\\), one unit change X, . long-run. SUM coefficients lagged value \\(x\\):\n\\[(0.6+0.4+0.2=1.2)\\]might lagged effects occur? psychology, intertia, technology, institutions. Think required someone update attitude belief. ever reasonable think happens instantly?table reports estimates autoregressive distributed lag model includes 4 lags first difference MICS.\nTable 3. Presidential approval function economy, war honeymoon, ADL(4)\ntreat coefficient \\(x_t\\) short-run impact.sum coefficients x lags long-run impact (LRI) total multiplier. example , short-run impact economic sentiment \\(0.15\\) long-run multiplier \\(0.15+0.27+0.19+0.14+0.12 = 0.87\\)\\[LRI=\\beta_{X_1}+\\beta_{X_1,t-1}+\\beta_{X_1,t-2}+...\\beta_{X_1,t-n}\\]means 10 unit increases MICS translate - four quarters - 8.7 point increases presidential approval.Note one practical problem approach. values \\(x_t\\) highly correlated time - index lags highly correlated . Remember effect collinearity increase standard errors. Since using first differences economic sentiment model, problem, can problem many applications, including use raw series casualties.don’t correction serial correlation white noise test indicates serial correlation still problem even introduced lagged values X.","code":"\n# estimate the distributed lag model (with four lags of the first difference of mics)\nadl<-dyn$lm(approval~diff(mics)+\n           +stats::lag(diff(mics), -1)\n           +stats::lag(diff(mics), -2)\n           +stats::lag(diff(mics), -3)\n           +stats::lag(diff(mics), -4)\n           +casualty\n           +stats::lag(casualty, -1)\n           +stats::lag(casualty, -2)\n           +stats::lag(casualty, -3)\n           +stats::lag(casualty, -4)\n           +honeymoon)\n\n# view the model info\nstargazer(static,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 3. Presidential approval as a function of economy, war and the honeymoon, ADL(4)**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))# generate the residuals and test for serial correlation\nbgtest(adl, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  adl\nLM test = 170.19, df = 4, p-value < 2.2e-16"},{"path":"dynamic-models-1.html","id":"partial-adjustment-models-lagged-value-of-approval","chapter":"6 Dynamic Models","heading":"6.3.2 Partial adjustment models (lagged value of approval)","text":"lagged value dependent variable included, model partial adjustment model. model specification designed overcome potential problem collinearity distributed lag model. Using model specification, OLS estimator performs well large samples absence serial correlation - data residuals must stationary.(Note: see next week - strategy used panel model testing link settled borders polity2 score Owsiak (2013).approval data, basic model estimated via OLS:\\[approval_t=\\beta_0+\\beta_1diff(mics)_t+\\beta_2casualty_t+\\beta_3honeymoon_t+ \\beta_4approval_{t-1}\\]\nTable 4. Presidential approval function economy, war honeymoon, Partial Adjustment\ncouple features output noticeable - now introducing persistence outcome directly model. 84% (0.84) last quarter’s approval rating carries . effect consumer sentiment large use strategy.coefficient X can interpreted short-run impact. long run impact calculated \\(\\beta_1 / (1-\\beta_4)\\).\\[LRI=\\beta_{X1} / (1.0-\\beta_{Y,t-1})\\]results suggest short-run impact 0.33 long-run impact 0.33/(1-0.840)=2.061.10 point jump Michigan Index Consumer Sentiment translate 20.6 point increase presidential approval, things equal, long run.Note slight difference two modeling approaches – ADL. long run effect six times larger short-run effect. example 4, long run effect seven times larger. due fact long run effect plays larger number lags. .33 t, .33.88 t+1, ,33.84*.84 t+2….","code":"\npa<-dyn$lm(approval~diff(mics)\n             +casualty\n             +honeymoon\n             +stats::lag(approval, -1))\n\n# view the model info\n#stargazer (pa, style=\"qje\", type=\"text\" , omit=\"factor\" , dep.var.labels = c(\"Presidential approval\", \"\\n\"), digits=2, title=c(\"Partial adjustment model\"), omit.stat=c(\"ser\",\"f\"),column.sep.width=c(\"12pt\"))\n# produce the table\nstargazer(pa,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          covariate.labels=c(\"MICS, first difference\", \"Casualities\", \"Honeymoon\", \"Approval, first lag\", \"Constant\"),\n          title=\"**Table 4. Presidential approval as a function of economy, war and the honeymoon, Partial Adjustment**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))# generate the residuals and test for serial correlation\nbgtest(pa, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  pa\nLM test = 7.4025, df = 4, p-value = 0.1161"},{"path":"dynamic-models-1.html","id":"autoregressive-distributed-lag-lagged-value-of-approval-and-first-lag-of-difference-in-mics","chapter":"6 Dynamic Models","heading":"6.3.3 Autoregressive distributed lag (lagged value of approval and first lag of difference in MICS)","text":"autoregressive distributed lag model, lagged values independent variable dependent variable included. common form model ADL (1,1) model:\\[Y_t=\\beta_0+\\beta_1X1_t+\\beta_2X1_{t-1} +\\beta_3X2_t+\\beta_4X2_{t-1}+\\beta_5Y_{t-1}\\]\nNote general model . \\(\\beta_2=0\\) \\(\\beta_4=0\\) partial adjustment model. \\(\\beta_5=0\\). distributed lag model.long-run, effect one unit change X1 calculated:\\[LRI=(\\beta_{X1,t}+\\beta_{X1,t-1)}/(1-\\beta_{Y,t-1})\\]\nTable 6. Presidential approval function economy, war honeymoon, PADL\ncase, test serial correlation indicates longer serial correlation residuals.coefficient X can interpreted short-run impact. long run impact calculated \\[LRI_{mics}=(\\beta_{d.mics}+\\beta_{lag(d.mics)}) / \\beta_{Y_{t-1}} \\]results suggest short-run impact 0.32 total long-run impact \\((0.32+0.17)/(1.0-0.86)=3.1\\). effect first period.","code":"\n# estimate the distributed lag model (with four lags of the first difference of mics)\npadl<-dyn$lm(approval~diff(mics)+\n           +stats::lag(diff(mics), -1)\n           +casualty\n           +stats::lag(casualty, -1)\n           +honeymoon\n           +stats::lag(approval, -1))\n\n# produce the table\nstargazer(padl,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 6. Presidential approval as a function of economy, war and the honeymoon, PADL**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))# generate the residuals and test for serial correlation\nbgtest(padl, order=4, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 4\n\ndata:  padl\nLM test = 5.7219, df = 4, p-value = 0.2209"},{"path":"dynamic-models-1.html","id":"error-correction-models","chapter":"6 Dynamic Models","heading":"6.3.4 Error Correction Models","text":"variant model becoming widely used political science. recent work (mainly Suzanne DeBoef [Linn]) focuses long run relationship two times series - series may fact share trend (nonstationary)- proposes error correction model (ECM). logic error correction model long run relationship two series takes many periods changes X Y incorporated. set models, use nonstationary raw MICS series.Initial versions ECM used two-step process. Simple OLS applied relationship X Y. observe relationship statistically significant residuals unit root - known cointegrating regression. include lag residual regression difference Y lag X. , since know first stage may biased inefficient X Y contain unit roots, estimate parameters models jointly. algebra transformation outlined De Boef Keele (2008). general form model known single equation error correction model becomes:\\[Y_t-Y_{t-1}=\\beta_0 + \\beta_1Y_{t-1}+\\beta_2 X_{t-1}+ \\beta_3(X_t-X_{t-1})+e_t\\]case, \\(\\beta_2\\) short-run effect long run effect :\\[LRI=(\\beta_3) / -(\\beta_1)\\]approach considerable advantage can applied stationary nonstationary (unit root) processes. overcome serial correlation problem directly modeling dynamics model, error term. overcome nonstationary problem getting stationary error term worked model. also compelling behavioral story - current period differences Y function short-term impact X extent Y equilibrium respect X previous period. nuanced empirical story explained incorporated model - involving mixture short-term long-term effects. Technically model uncomplicated.\nTable 7. Presidential approval function economy, war honeymoon, ECM\ndata: ecm\nLM test = 7.1651, df = 4, p-value = 0.1274\nThes short run effect 0.13, long run effect estimated ECM :\\[(0.37)/(-(-0.17))=2.17\\]10 point increase MICS generate 21.7 point increases approval.ECM DL(1) models similar except – ECM advantage – left hand side dependent variable first difference. see similarities, models side--side, . simplify table, just use MICS defense spending predictors. Notice level MICS, difference \\(X\\) variable.\nTable 8. Presidential approval function economy, war honeymoon, SSDPADL\ndata: ecmx\nLM test = 6.9034, df = 4, p-value = 0.1411data: adl1x\nLM test = 6.9034, df = 4, p-value = 0.1411In table , long-run effect MICS ADL model (0.347+-0.212)/(1-.0796) identical long-run effect ECM model, 0.134/(-(-0.204)).","code":"\n\necm<-dyn$lm(diff(approval)~\n            +stats::lag(approval, -1)\n            +stats::lag(mics, -1)\n            +diff(mics)\n            +stats::lag(casualty,-1)\n            +diff(casualty)\n            +stats::lag(honeymoon,-1)\n            +diff(honeymoon))\n\n\n# view the model info\n# produce the table\nstargazer(ecm,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 7. Presidential approval as a function of economy, war and the honeymoon, ECM**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05), column.sep.width=c(\"12pt\"))\n\n\n# generate the residuals and test for serial correlation\nbgtest(ecm, order=4, type = c(\"Chisq\"))Breusch-Godfrey test for serial correlation of order up to 4\n\necmx<-dyn$lm(diff(approval)~\n            stats::lag(approval, -1)\n            +stats::lag(mics, -1)\n            +diff(mics)\n\n            )\n\nadl1x<-dyn$lm(approval~\n           stats::lag(approval, -1)\n           +mics\n           +stats::lag(mics, -1)\n\n            )\n\n\n# view the model info\n# produce the table\nstargazer(ecmx, adl1x,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 8. Presidential approval as a function of economy, war and the honeymoon, SSDPADL**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05))\n\n# generate the residuals and test for serial correlation\nbgtest(ecmx, order=4, type = c(\"Chisq\"))Breusch-Godfrey test for serial correlation of order up to 4\nbgtest(adl1x, order=4, type = c(\"Chisq\"))Breusch-Godfrey test for serial correlation of order up to 4"},{"path":"dynamic-models-1.html","id":"a-couple-of-notes-about-other-models","chapter":"6 Dynamic Models","heading":"6.4 A couple of notes about other models","text":"","code":""},{"path":"dynamic-models-1.html","id":"a-state-specific-dynamic-model","chapter":"6 Dynamic Models","heading":"6.4.1 A state-specific dynamic model","text":"can also deploy interaction terms.\nTable 9. Presidential approval function economy, war honeymoon, SSDM\ndata: ssd\nLM test = 7.2099, df = 4, p-value = 0.1252","code":"\nssd<-dyn$lm(approval~diff(mics)+\n            +stats::lag(approval, -1)\n            +diff(mics):stats::lag(approval, -1)\n            +casualty\n            +honeymoon )\n\n# view the model info\n# produce the table\nstargazer(padl,  type=\"html\",\n          model.names=FALSE, model.numbers=FALSE, style=\"apsr\", digits=2,\n          dep.var.labels=c(\"Presidential approval\"),\n          title=\"**Table 9. Presidential approval as a function of economy, war and the honeymoon, SSDM**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append = FALSE,\n          omit.stat=c(\"ser\",\"f\"),\n          star.cutoffs = c(0.10,0.05), column.sep.width=c(\"12pt\"))\n\n\n# generate the residuals and test for serial correlation\nbgtest(ssd, order=4, type = c(\"Chisq\"))Breusch-Godfrey test for serial correlation of order up to 4"},{"path":"dynamic-models-1.html","id":"graphing-conditional-marginal-effects-without-using-cplot.","chapter":"6 Dynamic Models","heading":"6.4.1.1 Graphing conditional marginal effects (without using cplot).","text":"figure suggests improving economic sentiment can impact increase presidential approval approval average high, effect (0.0) approval approval low (40). describes experience President Trump persistence low approval ratings face good economic performance.","code":"\n# Adapted from\n## Two-variable interaction plots in R\n## Anton Strezhnev \n## 06/17/2013\n\n# Extract Variance Covariance matrix\n  covMat = vcov(ssd)\n# Get coefficients of variables\n  beta_1 = ssd$coefficients[\"diff(mics)\"]\n  beta_3 = ssd$coefficients[\"diff(mics):stats::lag(approval, -1)\"]\n# Create list of moderator values at which marginal effect is evaluated\n  x_2 <- seq(from=5, to=95, by=5)\n  # Compute marginal effects\n  delta_1 = beta_1 + beta_3*x_2\n# Compute variances\n  var_1 = covMat[2,2] + (x_2^2)*covMat[6,6] + 2*x_2*covMat[2,6]\n# Standard errors\n  se_1 = sqrt(var_1)\n# Upper and lower confidence bounds\n  z_score = qnorm(1 - ((1 - 0.95)/2))\n  upper_bound = delta_1 + z_score*se_1\n  lower_bound = delta_1 - z_score*se_1\nfigure_data<-as.data.frame(cbind(x_2,delta_1,upper_bound, lower_bound))\n\n# Figure 2\nggplot(figure_data, aes(x = x_2, y = delta_1)) +\n  geom_line(lwd = 1.5) +\n  geom_line(aes(y = upper_bound)) +\n  geom_line(aes(y = lower_bound)) +\n  ggtitle(\"Figure 2. Marginal effect of economic sentiment, by level of approval\") +\n   theme(plot.title.position = \"plot\", plot.title = element_text(face=\"bold\")) +\n  labs(x=\"Approval\", y=\"Marginal effect of first difference in MICS\")+\n  geom_hline(yintercept=0.00, lty=2)+\n  geom_vline(xintercept=mean(poq$approval, na.rm=TRUE), color=\"red\", lty=2)"},{"path":"dynamic-models-1.html","id":"vector-autoregression-or-var","chapter":"6 Dynamic Models","heading":"6.4.2 Vector AutoRegression or VAR","text":"time series analysts bridle restrictive nature family models – don’t let impact lagged variables take general form – perhaps size direction lagged effects varies way simple distributed lag. economics, classic article Sims (1980).assume X->Y – couldn’t case Y->X many applications? objections led modeling strategy labeled Vector Autogression VAR.framework, theory dictates variables included model, theory informs inclusion exclusion particular lag direction causal effects. modeling strategy parameter- intensive – lot parameters estimated modest amount data. fact, number estimated parameters large often results summarized figures – known impulse response functions.won’t cover models can recognize practice characteristic figures, rather tables coefficients. Just recognize dynamic models, estimated time series, restriction imposed theory.","code":""},{"path":"working-with-panel-data.html","id":"working-with-panel-data","chapter":"7 Working with panel data","heading":"7 Working with panel data","text":"","code":""},{"path":"working-with-panel-data.html","id":"statistical-models-for-panel-data-n-dominant-approaches","chapter":"7 Working with panel data","heading":"7.1 Statistical models for panel data: “N” dominant approaches","text":"Panel data describes datasets units observed one time period. Panel data may time dominant (units observed long period time) “N” dominant (many units observed short period time).Panel data may balanced (unit observed number times) unbalanced (units observed fewer times others).Panel data offer lot leverage empirical questions, introduce lot modeling choices.Economists often use panel data individual level - household consumers observed surveyed. Panel data econometrics entire subfield books devoted - one notable influential contribution Jeffrey Wooldridge’s Econometric Analysis Cross Section Panel Data (Wooldridge 2001). insights panel data econometrics also exported fields, including political science. Important findings voting behavior emerged 1972/4/6 ANES panel study. individual-level datasets typically many people (large N) observed short period time (small T). Work comparative politics extended panel data different forms (smaller N larger T).models focus week developed applied “N” dominant datasets. Next week talk dynamics endogeneity, complications become problematic T grows relative N.","code":""},{"path":"working-with-panel-data.html","id":"why-use-panel-data","chapter":"7 Working with panel data","heading":"Why use panel data?","text":"least four compelling, practical advantages using panel data:Increase N small sample nations regionsIncrease N small sample nations regionsAddress problems multicollinearity (variation Xs Y)Address problems multicollinearity (variation Xs Y)Model dynamic adjustment (leveraging many short-term reactions estimate properties long-term adjustment). next week, upshot time series: lagged values dependent variable go long way.Model dynamic adjustment (leveraging many short-term reactions estimate properties long-term adjustment). next week, upshot time series: lagged values dependent variable go long way.Test model unobserved heterogeneity - idea need understand features individuals may related Y, can’t measure observe directly. cross-sectional snapshot can never get handle - systematic variation across individuals just gets lumped error term. Panel data - multiple observations individuals lets us estimate manage heterogeneiety. important.Test model unobserved heterogeneity - idea need understand features individuals may related Y, can’t measure observe directly. cross-sectional snapshot can never get handle - systematic variation across individuals just gets lumped error term. Panel data - multiple observations individuals lets us estimate manage heterogeneiety. important.","code":""},{"path":"working-with-panel-data.html","id":"panel-data-problems","chapter":"7 Working with panel data","heading":"Panel data problems","text":"Two problems present panel data.problems already learned can emerge cross-sectional time-series applications present : heteroskedasticity serial correlation.observations errors persist time (slow adjustment).observations errors persist time (slow adjustment).Observations unit may share error error may vary across units.Observations unit may share error error may vary across units.problems may minor time periods, can major many time periods.","code":""},{"path":"working-with-panel-data.html","id":"why-panel-data-can-be-particularly-tricky","chapter":"7 Working with panel data","heading":"Why panel data can be particularly tricky","text":"figure describes relationship X Y hypothetical (contrived) time-dominant dataset. Think cluster observations describing 100 years experience one country. plot combines scatter plot three regression lines - one line entire dataset (pooled) one line 100 observations country.basic model links X Y simple linear model:\\[Y=\\beta_0+\\beta_1X\\]Take close look code - specified model parameters - X translated Y true \\(\\beta_1=-2\\) country different values intercept (\\(\\beta_0\\)): 0 15. X also different: average 1 first 200 observations average 2 second 200 observations. difference intercepts form unobserved heterogeneity. pool data together estimate slope, ignoring differences.describe data arbitrary number ways. example mind two countries observed 200 times - total 400 observations. data 200 countries observed two times. 20 countries observed 20 times. Pooled OLS agnostic data collected. just regression Y X entire data set. code chunk sets data three forms - varying intercepts, varying levels X slope coefficient. data set indexed ccoden timet.\nTable 1. Estimates three datasets\n\ncan see examples N gets large relative T, estimate slope coefficient closer true value, still biased.","code":"\n# example of unobserved heterogeneity \n# adapted from Garrett Glasgow's panel data course site\n# the set.seed function just makes sure that the random numbers are the same each time I run the commands.  This is important for replication.\nset.seed(10725)\nx1 <- rnorm(200,1,1)\nx2 <- rnorm(200,2,1)\ne1 <- rnorm(200,0,0.8)\ne2 <- rnorm(200,0,0.8)\n# y1 and y2 are the same function of x but different intercept\n# In the second regression, x is the same (-2) but the intercept is higher (15)\ny1 <- 0 - 2*x1 + e1\ny2 <- 15 - 2*x2 + e2\n\nxy<-data.frame(x=append(x1,x2), y=append(y1,y2))\nxy1<-data.frame(x=x1, y=y1)\nxy2<-data.frame(x=x2, y=y2)\nggplot()+\n  geom_point(data = xy, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"red\", formula = y ~ x, data=xy1, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"red\", formula = y ~ x, data=xy2, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"blue\", formula = y ~ x, data=xy, aes(x = x, y = y)) +\n  labs(subtitle=expression(~~hat(beta)[true]==-2~~~~hat(beta)[blue]==1.04),\n    caption=\"Figure 1. Comparing true and estimated slope coefficients\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))\n\n# This creates a dataset that is T>N, just like above\nccode1<-c(rep(1, 200), rep(2, 200))\ntime1<-c(rep(1:200,2))\ndata1<-data.frame(cbind(ccode1,time1))\ndata1$x <- rnorm(400,ccode1,1)\ndata1$e<-rnorm(400,0,0.8)\ndata1$beta0<- 15*(data1$ccode1-1)\ndata1$y<- data1$beta0 + (-2*data1$x) + data1$e\n\n# This creates a dataset that is N~=T\nccode2<-c(rep(1,20))\nfor (n in seq(1:19)) {\nccode2<-append(ccode2, c(rep(n+1,20)))\n}\ntime2<-c(rep(1:20,20))\ndata2<-as.data.frame(cbind(ccode2,time2))\ndata2$x <- rnorm(400,ccode2/10,1)\ndata2$e<-rnorm(400,0,0.8)\ndata2$beta0<- 15*(data2$ccode2/10)\ndata2$y<- data2$beta0 + (-2*data2$x) + data2$e\n\n# This creates a dataset that is N>T\nccode3<-c(rep(1,2))\nfor (n in seq(1:199)) {\nccode3<-append(ccode3, c(rep(n+1,2)))\n}\ntime3<-c(rep(1:2,200))\ndata3<-as.data.frame(cbind(ccode3,time3))\ndata3$x <- rnorm(400,ccode2/100,1)\ndata3$e<-rnorm(400,0,0.8)\ndata3$beta0<- 15*(data3$ccode3/100)\ndata3$y<- data3$beta0 + (-2*data3$x) + data3$e\n\nreg.data1 <- lm(y~x, data=data1)\nreg.data2 <- lm(y~x, data=data2)\nreg.data3 <- lm(y~x, data=data3)\n\nstargazer(reg.data1, reg.data2, reg.data3, style=\"apsr\", type=\"html\" , digits=2, model.numbers=FALSE, column.labels = c(\"T>N\",\"N=T\",\"N>T\"), dep.var.labels = c(\"\"),\ntitle=\"Table 1. Estimates from three datasets\", notes= \"p<.10* ; p<.05**\",  omit.stat=c(\"ser\",\"f\"), notes.append = FALSE, star.cutoffs = c(0.10,0.05))"},{"path":"working-with-panel-data.html","id":"panel-data-in-r","chapter":"7 Working with panel data","heading":"7.2 Panel data in R","text":"estimates examples draw dataset used 2013 Journal Politics article investigated link border security democratization (Owsiak 2013). pulling couple variables look link trade openness, economic performance, settled borders, democratization.dependent variable Polity 2 score democratization (-10 10), trade openness lagged natural log exports plus imports, economic performance index lagged change annual GDP per capita (best determine based data descriptions Maddison project). Settled borders key predictor. Review article theory expectations - presence settled borders permits decentralization demilitarization, abet establishment persistence democracy.details Polity index, see Center Systemic Peace (2021). data covers 1919-2006.run panel data commands, make sure installed loaded plm package R.treat data panel, must specify two variables , together, uniquely identifies observation: code individual code time. R, pdim functionThe output indicates 200 countries, observed 2 years many 89 years. total 10,434 observations , since different numbers years countries, data “unbalanced.”One practical note. data time dominant (T>>N), use appropriate approach use known feasible generalized least squares. covered next week.data “N” dominant (N>>T, even N~T), use approach outlined . example clear N>>T since 200 countries maximum 89 years.can imagine , due expense collecting repeated observations, panel data large N small T.basic model using takings form:\\[Y_{}=c_i+\\beta_1X_{}+u_i\\]\nNotice intercept subscript, may vary across observationsIf \\(c_i\\) constant across individuals (\\(\\beta_0=\\beta_{0_i}\\)), OLS OKIf \\(c_i\\) constant across individuals (\\(\\beta_0=\\beta_{0_i}\\)), OLS OKIf \\(c_i\\) varies, OLS appropriate (form omitted variable bias)\\(c_i\\) varies, OLS appropriate (form omitted variable bias)","code":"# quitely=TRUE suppresses a warning message about conflict with dplyr\nlibrary(plm, quietly=TRUE)\n# Need JOP data\njop<-read_stata(\"data/jop_2013.dta\")\n# Identify the dataset as panel data\npdim(jop, index=c(\"ccode\", \"year\"))\nUnbalanced Panel: n = 200, T = 2-89, N = 10434"},{"path":"working-with-panel-data.html","id":"alternative-solutions","chapter":"7 Working with panel data","heading":"7.3 Alternative solutions","text":"","code":""},{"path":"working-with-panel-data.html","id":"pooled-data-generic-and-with-robust-standard-errors","chapter":"7 Working with panel data","heading":"Pooled data (generic and with robust standard errors)","text":"simply pool data assume intercepts across individuals (time). equivalent ignoring unit heterogeneity (unmeasured individual level differences).know can hazardous based example .results use plm function request model=pooling”\nTable 2. Polity 2 scores function trade, economics, borders\n\nBased results conclude settled borders, trade openness positive growth GDP associated higher levels democratization.effect borders obviously large given talking -10 +10 scale dummy variable.request robust standard errors. code chunk reproduces known “panel-corrected standard errors,” prescribed detailed widely cited piece, Beck Katz (1995). Check article origins rationale approach. use label TSCS time-series cross-ectional describe panel data. think meant differentiate type panel data see Political Science - larger T smaller N - panel data typical Economics (larger N smaller T).options specify mean standard errors robust presence heteroskedasticity (specifically, errors varying country) serial correlation. case, standard errors higher, conservative - everything still significant standard errors quite low relative parameter estimates.Note takes care problems standard error anything unobserved heterogeneity bias.happens try take tackle unobserved heterogeneity?","code":"\npooled.ols <- lm(polity2 ~ lagtradeopen + laggdpchg + allsettle, data=jop, na.action=na.omit)\npooled.plm <- plm(polity2~ lagtradeopen + laggdpchg+  allsettle, data=jop, index=c(\"ccode\", \"year\"), na.action=na.omit, model=\"pooling\")\n\nstargazer(pooled.ols, pooled.plm, style=\"apsr\", type=\"html\", title=\"**Table 2. Polity 2 scores as a function of trade, economics, and borders**\", notes= \"p<.10* ; p<.05**\",  notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"Polity democracy score\"), covariate.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\nkable(tidy(coeftest(pooled.plm, vcov=vcovBK, type=\"HC1\", cluster=\"time\")), digits=3)"},{"path":"working-with-panel-data.html","id":"fixed-effects","chapter":"7 Working with panel data","heading":"Fixed effects","text":"fixed effects approch assumes differences across units can captured differences constant term. two options implement approach:","code":""},{"path":"working-with-panel-data.html","id":"least-squares-dummy-variable-approach-lsdv","chapter":"7 Working with panel data","heading":"Least squares dummy variable approach (LSDV)","text":"approach fully captures individual heterogeneityIt can problematic large n datasets, useful small n:large matrix (n-1 dummy variables) forecloses estimation statistical softwarea large matrix (n-1 dummy variables) forecloses estimation statistical softwareinefficiency (many parameters)inefficiency (many parameters)inconsistency (adding observations increases number parameters)inconsistency (adding observations increases number parameters)output include coefficients 199 country dummy variables.first table reports results settled border example\n**Table 3 Polity 2 scores function trade, economics, borders\nAccounting country fixed effects, economics growth lagged GDP change settled borders now significant.can experiment approach datasets know true model.Note fixed effects approach works contrived datasets. dependence size N relative T. (Note know don’t serial correlation case. simulate serial correlation next week see results).\nTable 3. LSDV three panel structures\n\nIndependent way structured panel data, fixed effect strategy picked true value expected: \\(\\beta_1 \\approx\\) -2.0.","code":"\nlsdv.ols <- lm(polity2~ lagtradeopen + allsettle + laggdpchg +as.factor(ccode), data=jop, na.action=na.omit)\n\nstargazer(lsdv.ols, style=\"apsr\", type=\"html\", omit=\"ccode\", title=\"**Table 3 Polity 2 scores as a function of trade, economics, and borders\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"Polity democracy score\"), column.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\n#pdim(data1, index=c(\"ccode1\", \"time1\"))\nlsdv1 <- lm(y~ x+as.factor(ccode1), data=data1, na.action=na.omit)\n\n#pdim(data2, index=c(\"ccode2\", \"time2\"))\nlsdv2 <- lm(y~ x+as.factor(ccode2), data=data2, na.action=na.omit)\n\n#pdim(data3, index=c(\"ccode3\", \"time3\"))\nlsdv3 <- lm(y~ x+as.factor(ccode3), data=data3, na.action=na.omit)\n\nstargazer(lsdv1, lsdv2, lsdv3,\n          style=\"apsr\", type=\"html\" , digits=2, model.numbers=FALSE,\n          column.labels = c(\"T>N\",\"N=T\",\"N>T\"),\n          omit=c(\"ccode1\", \"ccode2\", \"ccode3\"), \n          dep.var.labels = c(\"\"),\n          title=\"**Table 3. LSDV with three panel structures**\",\n          notes= \"p<.10* ; p<.05**\",  notes.append=FALSE,\n          omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05))"},{"path":"working-with-panel-data.html","id":"testing-the-significance-of-group-effects","chapter":"7 Working with panel data","heading":"7.3.0.1 Testing the significance of group effects","text":"F-test can used test joint significance n-1 dummy variables.can test difference two models democratization pooled LSDV.output suggests including dummy variables country statistically significant improvement model fit - substantial reduction residual sum squares.tells us unobserved heterogeneity, captured country-specific constant.test data give result.","code":"\nkable(anova(pooled.ols, lsdv.ols))\nkable(anova(reg.data1, lsdv1))"},{"path":"working-with-panel-data.html","id":"within-group-estimators-fixed-effects-without-dummy-variables","chapter":"7 Working with panel data","heading":"7.3.1 Within group estimators (fixed effects without dummy variables)","text":"also estimate fixed effect “within” model plm package. LSDV fixed effects give resultFixed effects transformation within transformation required thinking link average level X average level Y.\\[\\mu_{yi}=c_i+\\beta_1\\mu_{xi}+\\epsilon_{}\\]Subtract basic model, \\[Y_{}-\\mu_y= \\beta_1(X_{}-\\mu_{xi}) +(e_{}-\\epsilon_{})\\]Note \\(c_i\\) drops outThe approach generally two step estimation. First, mean calculated individual Y X. difference time period Y average Y modeled function difference time period X average X. THINK MEANS THEORETICALLYOne big problem: transformation forecloses use variables fixed time individual unit (constitutional features countries; race gender individuals) since \\(X_{} - \\mu_{xi}\\) zero observations.\n**Table 3 Polity 2 scores function trade, economics, borders\n\nTable 4. Fixed effects, known slope coefficient equal 2.0\nCompare coefficients models LSDV model - identical.","code":"\n\nfe.model <- plm(polity2 ~ lagtradeopen + laggdpchg + allsettle, data=jop, index=c(\"ccode\", \"year\"), na.action=na.omit, model=\"within\")\nstargazer(fe.model, style=\"apsr\", type=\"html\", omit=\"ccode\", title=\"**Table 3 Polity 2 scores as a function of trade, economics, and borders\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"Polity democracy score\"), column.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\n\n\nfe.model1 <- plm(y ~ x, data=data1, index=c(\"ccode1\", \"time1\"), na.action=na.omit, model=\"within\")\nstargazer(fe.model1, style=\"apsr\", type=\"html\" , omit=\"ccode\", digits=2, dep.var.labels = c(\"T>N\"),  title=\"Table 4.  Fixed effects, with known slope coefficient equal to 2.0\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05))"},{"path":"working-with-panel-data.html","id":"random-effects-estimator","chapter":"7 Working with panel data","heading":"7.3.2 Random effects estimator","text":"random effects models, individuals varying intercepts, intercepts drawn distribution - normal distribution centered around zero.works unmeasured features uncorrelated predictors (way excluding variable related \\(Y\\) unrelated Xs).(fixed effects estimators works presence correlation)random effects estimator relies information “within” estimator, , “” estimatorThe estimator uses country means:\\[\\mu_{y}=c_i+\\beta_1\\mu_x+\\epsilon\\]approach throws away information things related time just uses average.\n**Table 3 Polity 2 scores function trade, economics, borders\nproblem effects approach: estimator consistent mean x related level intercept term (.e. correlated error term). contrived data, know intercept related X - higher X associated intercept. result estimate \\(\\beta_1\\) way - high positive truth negative. Ignoring unmeasured heterogeneity hazardous.Imagine made policy recommendation based result.random effects estimator weighted average within estimator, bias introduced estimator also present re estimator. intuition compelling -want combine insights observation deviates ’s mean along observations vary .","code":"\nbe.model <- plm(polity2 ~ lagtradeopen + laggdpchg + allsettle, data=jop, index=c(\"ccode\", \"year\"), na.action=na.omit, model=\"between\")\n\nstargazer(be.model, style=\"apsr\", type=\"html\", omit=\"ccode\", title=\"**Table 3 Polity 2 scores as a function of trade, economics, and borders\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"Polity democracy score\"), column.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\nbe.model3 <- plm(y ~ x, data=data3, index=c(\"ccode3\", \"time3\"), na.action=na.omit, model=\"between\")\nstargazer(be.model3, style=\"apsr\", type=\"html\", omit=\"ccode\", title=\"**Table 3, Random effects\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"T>N\"))\n\n<table style=\"text-align:center\"><caption><strong>**Table 3, Random effects<\/strong><\/caption>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\"><\/td><td>T>N<\/td><\/tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\">x<\/td><td>-0.66<\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.85)<\/td><\/tr>\n<tr><td style=\"text-align:left\">Constant<\/td><td>14.89<sup>**<\/sup><\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.63)<\/td><\/tr>\n<tr><td style=\"text-align:left\">N<\/td><td>200<\/td><\/tr>\n<tr><td style=\"text-align:left\">R<sup>2<\/sup><\/td><td>0.003<\/td><\/tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2<\/sup><\/td><td>-0.002<\/td><\/tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td colspan=\"2\" style=\"text-align:left\">p<.10* ; p<.05**<\/td><\/tr>\n<\/table>re.model <- plm(polity2 ~ lagtradeopen + laggdpchg + allsettle, data=jop, index=c(\"ccode\", \"year\"), na.action=na.omit, model=\"random\")\nstargazer(re.model, style=\"apsr\", type=\"html\", omit=\"ccode\", title=\"**Table 3 Polity 2 scores as a function of trade, economics, and borders\", notes=\"p<.10* ; p<.05**\", notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=2, dep.var.labels = c(\"Polity democracy score\"), column.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\n\n<table style=\"text-align:center\"><caption><strong>**Table 3 Polity 2 scores as a function of trade, economics, and borders<\/strong><\/caption>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\"><\/td><td>Polity democracy score<\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>Trade openness<\/td><\/tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td style=\"text-align:left\">lagtradeopen<\/td><td>1.10<sup>**<\/sup><\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.03)<\/td><\/tr>\n<tr><td style=\"text-align:left\">laggdpchg<\/td><td>-0.63<\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.85)<\/td><\/tr>\n<tr><td style=\"text-align:left\">allsettle<\/td><td>0.40<sup>*<\/sup><\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.21)<\/td><\/tr>\n<tr><td style=\"text-align:left\">Constant<\/td><td>-9.16<sup>**<\/sup><\/td><\/tr>\n<tr><td style=\"text-align:left\"><\/td><td>(0.51)<\/td><\/tr>\n<tr><td style=\"text-align:left\">N<\/td><td>7,238<\/td><\/tr>\n<tr><td style=\"text-align:left\">R<sup>2<\/sup><\/td><td>0.16<\/td><\/tr>\n<tr><td style=\"text-align:left\">Adjusted R<sup>2<\/sup><\/td><td>0.16<\/td><\/tr>\n<tr><td colspan=\"2\" style=\"border-bottom: 1px solid black\"><\/td><\/tr><tr><td colspan=\"2\" style=\"text-align:left\">p<.10* ; p<.05**<\/td><\/tr>\n<\/table>"},{"path":"working-with-panel-data.html","id":"why-random-effects-approaches-may-fail","chapter":"7 Working with panel data","heading":"7.4 Why random effects approaches may fail","text":"random effects estimator biased X variables correlated magnitude error term (case contrived data, ).example: use panel data estimate relationship education income, individual unobserved skill level. skill level associated x intercept. fixed effects estimator handle well (higher intercepts higher skill respondents). random effects estimator upward bias. must test bias. Hausman test indicates whether random effects fixed effects estimator appropriate.Hausman test indicates coefficients random effects fixed effects models different. null hypothesis (H0), coefficients unbiased estimates \\(\\beta_i\\) consistent. close, inspect coefficients can see case.reject null hypothesis, means can use random effects estimator.can see test indicated coefficients different. Trade openness positive significant models. GDP growth zero (significant) models. Settled borders positive threshold statistical significance models.","code":"\nkable(tidy(phtest(fe.model, re.model)))"},{"path":"working-with-panel-data.html","id":"returning-to-our-example","chapter":"7 Working with panel data","heading":"Returning to our example","text":"test random effects vs. fixed effects model N>>T dataset. 100 countries observed twice, Hausman test suggests fixed effects.","code":"\nfe.model3 <- plm(y ~ x, data=data3, index=c(\"ccode3\", \"time3\"), na.action=na.omit, model=\"within\")\nre.model3 <- plm(y ~ x, data=data3, index=c(\"ccode3\", \"time3\"), na.action=na.omit, model=\"random\")\nkable(tidy(phtest(fe.model3, re.model3)))"},{"path":"working-with-panel-data.html","id":"practical-implications","chapter":"7 Working with panel data","heading":"7.5 Practical implications","text":"T large N small, appropriate strategy use feasible generalized least squares dynamic panel model leverages can learn long time series.N large T small (often turn estimators), estimates can diverge. case example data set, whether N=T N>>T.F-test \\(u_{}=0\\) significant, pooled regression appropriate since clearly unmeasured heterogeneity across units.Hausmann test indicates coefficients random fixed effects substantially different, model choice fixed effects model. test indicates unit heterogeneity correlated X.N large T small Hausmann test significant, random effects estimator efficient estimator. (can use time invariant covariates).","code":""},{"path":"working-with-panel-data.html","id":"notes-and-next-week","chapter":"7 Working with panel data","heading":"7.6 Notes and next week","text":"Spring 2007 issue Political Analysis several articles analysis TSCS data - conclusions one set authors sobering insightful. Wilson Butler (2007):Although continued statistical analysis development better methods proceed, researchers must prepared answer regression analysis simply provide reliable conclusions instances-humbling fact relevant regression analysis generally, might add, just TSCS context. fact also points unavoidable conclusion research comparative politics international relations must remain qualitatively rich. rift qualitative quantitative analysts , especially case small panel data sets, counterproductive. Political science seems particularly well poised, think, pursue methodological agenda marrying quantitative qualitative methods (enlightened stronger theories), since neither mode analysis sufficient many cases.Next week. Modeling dynamics long T panel data.","code":""},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"dynamics-panel-models---a-brief-introduction","chapter":"8 Dynamics panel models - a brief introduction","heading":"8 Dynamics panel models - a brief introduction","text":"","code":""},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"plan-for-class","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.1 Plan for class","text":"cover four things today:Review - statistical significance - means importantReview - using model coefficients describe size effects\n_ happens panel data ar(1) error term?Replicating “Democratization International Border Settlements”Paper #5Looking ahead: Harvard Dataverse (Class example search: Journal Politics R)","code":""},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"review-statistical-significance.","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.2 Review: statistical significance.","text":"Statistical significance appeals logic repeated sampling. mean?rely specific threshold differentiate statistically significant statistically significant. p<.05 consider effect statistically significant - unlikely due chance product sampling error. use p<.05 threshold?p>.05 accept null hypothesis. mean practice?types mistakes risk making sticking threshold? brielfy review errors might confront.Figure 1. Type Type II error?","code":""},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"review-using-model-coefficients","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.3 Review: using model coefficients","text":"assignments using output statistical models evaluate effects variety conditions presidential approval, determine predicts voter turnout, makes countries less democratic.used logistic regression, use cplot function calculate probability turnout various configurations age, education, income, race gender. see - using several figures - moving low high levels education increased probability turnout effect compared income age race. used cplot func since model nonlinear effect X conditional values Xs.need take approach describing effects X models estimate OLS. Since using `cplot’, means need know something actual range X order explain understand much X matters. example last week, critical variable - settled borders - dummy variable, taking values zero one. coefficient variable - number table - captured positive effect settled borders. , assignment presidential approval, dummy variable divided government captured downward pressure presidential approval divided government. using coefficients types variables requires knowing specific things variables - range, maybe 25th percentile 75th percentile, pair low high values represent something meaningful. can make useful comparisons across variables combining information reported coefficients.instance, presidential approval dataset, couple used difference unemployment independent variable. Unemployment stationary, use first difference typical change unemployment one quarter next? Using summary function, learn :want understand effect unemployment approval, might make sense compare approval differ periods unemployment rapidly declining (-0.97) vs rapidly growing (1.67). compare less dramatic difference - 25th percentile (-0.200) vs 75 percentile (0.134).Say observe coefficient Partial Adjustment model -1.18 associated long-run effect (-1.18/(1-0.84))=7.375.say shifted period growing employment (-0.200) period rising unemployment (0.134), implies change x = +0.334. short-run, shift translate decrease 0.334 * 1.18 0.40 percent. long-run one time change eat away 0.334 * 7.375 2.5 points. seems like relatively small effect (helps us understand effect statistically significant cases).Carefully note logic :Take two representative theoretically relevant values XSubtract \\(X_1-X_2\\)Multiply coefficientThis effect - always compare effects--effects slope--slope","code":"poq<- read_dta(\"data/poq_guys.dta\")\nsummary(diff(poq$unempl))\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's \n-0.966000 -0.200000 -0.034000  0.009542  0.134000  1.667000        17 "},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"what-happens-when-we-have-panel-data-with-serially-correlated-errors","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.4 What happens when we have panel data with serially correlated errors?","text":"determine implications serial correlation, return sample data last week, 2 countries observed 100 time periods. able use fixed effects accurately recover true slope coefficient. codechunk replicates example, uses ar(1) error term.Balanced Panel: n = 2, T = 200, N = 400data: y ~ x\nchisq = 293.31, df = 200, p-value = 1.876e-05\nalternative hypothesis: serial correlation idiosyncratic errors\nTable 1. Panel data serially correlated errors\n","code":"\n# Create 400 draws of an error term that is serially correlated rho=0.7\n\nrho <- 0.7\nset.seed(3567567)\nN<-400\nv <- ts(rnorm(N,0,1))\n# AR(1)\ne <- ts(rep(0,N))\nfor (t in 2:N){\n  e[t]<- rho*e[t-1]+v[t]\n}\n\n#Recreate the example from last week with the new error term\nset.seed(10725)\nx1 <- rnorm(200,1,1)\nx2 <- rnorm(200,2,1)\ne1 <- e[1:200]\ne2 <- e[201:400]\n# y1 and y2 are the same function of x but different intercept\n# In the second regression, x is the same (-2) but the intercept is higher (15)\ny1 <- 0 - 2*x1 + e1\ny2 <- 15 - 2*x2 + e2\n\nxy<-data.frame(x=append(x1,x2), y=append(y1,y2))\nxy1<-data.frame(x=x1, y=y1)\nxy2<-data.frame(x=x2, y=y2)\nggplot()+\n  geom_point(data = xy, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"red\", formula = y ~ x, data=xy1, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"red\", formula = y ~ x, data=xy2, aes(x = x, y = y)) +\n  geom_smooth(method=lm, se=FALSE, color=\"blue\", formula = y ~ x, data=xy, aes(x = x, y = y)) +\n  labs(subtitle=expression(~~hat(beta)[true]==-2~~~~hat(beta)[blue]==1.02),\n    caption=\"Figure 2. Comparing true and estimated slope coefficients\") + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(hjust=0, face=\"bold\", size=rel(1.15)))# quietly=TRUE suppresses a warning message about conflict with dplyr\nlibrary(plm, quietly=TRUE)\n\nAttaching package: 'plm'\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n# This creates a dataset that is T>N\nccode1<-c(rep(1, 200), rep(2, 200))\ntime1<-c(rep(1:200,2))\ndata1<-data.frame(cbind(ccode1,time1,xy))\n# Like last week, use plm packge to specify panel structure and use tests\npdim(data1, index=c(\"ccode1\", \"time1\"))\nreg.pooled<- plm(y~x, model=\"pooling\", data=data1)\nreg.fixed<- plm(y~x, model=\"within\", data=data1)\nreg.pa<- plm(y~x+lag(y), model=\"within\", data=data1)\npbgtest(reg.fixed)Breusch-Godfrey/Wooldridge test for serial correlation in panel models\nstargazer(reg.pooled, reg.fixed, reg.pa, style=\"apsr\", type=\"html\" , digits=2, model.numbers=FALSE, column.labels = c(\"Pooled\",\"Fixed effects\"), dep.var.labels = c(\"\"),\ntitle=\"Table 1. Panel data with serially correlated errors\", notes= \"p<.10* ; p<.05**\",  omit.stat=c(\"ser\",\"f\"), notes.append = FALSE, star.cutoffs = c(0.10,0.05))"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"replicating-democratization-and-international-border-settlements","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.5 Replicating “Democratization and International Border Settlements”","text":"assignment draws dataset used 2013 Journal Politics article investigated link border security democratization. code chunk reads data replicates model reported Table 2, column 2 article. can compare numbers output numbers paper.dependent variable Polity 2 score democratization (-10 10). Independent variables include:\n- trade openness (lagged natural log exports plus imports)\n- settled border\n- two measures military expenditures\n- two measures economic performance\n- lag dependent variable (Partial Adjustment model)See Owsiak (2013) detailsThe dataset includes 200 countries, observed 2 years many 89 years. total 10,434 observations , since different numbers years countries, data “unbalanced.”Last week learned test unit effects - unmeasured heterogeneity capture permitting observation unique intercept.week focus dynamics, merge existing understanding handle time series learned unit effects.strategies dealing dynamics panel models involves starting point single unit observed long period time - test series stationary evaluate residuals OLS model serial correlation.","code":"# Need JOP data\njop<-read_stata(\"data/jop_2013.dta\")\n# Identify the dataset as panel data\npdim(jop, index=c(\"ccode\", \"year\"))\nUnbalanced Panel: n = 200, T = 2-89, N = 10434"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"testing-for-stationarity","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.5.1 Testing for stationarity","text":"need repeat tests assignment since clear series stationary.code cunk first filters cases missing values variable - known “listwise deletion” Beck Katz discusses alternatives approach - missing data imputation. use type algorithm random draw fill missing values particular variable don’t end deleting entire observation one variable missing value. can also help balance panel data (filling gaps time coverage unit units every time)case can reject null hypothesis. null series unit root alternative hypothesis series stationary. series used paper stationary. (Note: p-values reported 0.01. rounded , many much lower.)","code":"# This isolates lengths of the series with no missing values.\njop_test1<-jop %>% drop_na()\n# This step recreates the data set by adds the index information, clarifying that each observation is a unique combination of country code and year\npanel.set <- pdata.frame(jop_test1, index = c(\"ccode\", \"year\"))\n#  This is the augmented dickey fuller test from three weeks ago\n#  requires the tseries package\nlibrary(tseries, quietly=TRUE)\nadf.test(panel.set$polity2, k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$polity2\nDickey-Fuller = -13.329, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$allsettle , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$allsettle\nDickey-Fuller = -11.411, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$laggdpam , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$laggdpam\nDickey-Fuller = -11.568, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$lagtradeopen , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$lagtradeopen\nDickey-Fuller = -13.872, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$lagmilex , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$lagmilex\nDickey-Fuller = -15, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$lagmilper , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$lagmilper\nDickey-Fuller = -14.377, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$lagsumdown , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$lagsumdown\nDickey-Fuller = -10.962, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$laggdpchg , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$laggdpchg\nDickey-Fuller = -47.521, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary\nadf.test(panel.set$lagupop , k=1)\n\n    Augmented Dickey-Fuller Test\n\ndata:  panel.set$lagupop\nDickey-Fuller = -34.966, Lag order = 1, p-value = 0.01\nalternative hypothesis: stationary"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"testing-for-serial-correlation","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.5.2 Testing for serial correlation","text":"use standard BG test serial correlation. code chunk estimates model without lagged dependent variable. don’t report results, test serial correlation. two reported results. first set ignores fact panel data, relying simple lm function related tests. second set results use pbgtest plm package. Given large number time points relatively smaller number countries, approaches yield result.","code":"pooled.ols <- lm(polity2~ allsettle+laggdpam+laggdpchg+lagtradeopen+lagmilper+lagmilex+lagupop+lagsumdown, data=jop, na.action=na.omit)\nbgtest(pooled.ols, order=1, type = c(\"Chisq\"))\n\n    Breusch-Godfrey test for serial correlation of order up to 1\n\ndata:  pooled.ols\nLM test = 6430.6, df = 1, p-value < 2.2e-16\n\npooled.plm <- plm(polity2~ allsettle+laggdpam+laggdpchg+lagtradeopen+lagmilper+lagmilex+lagupop+lagsumdown, data=jop, na.action=na.omit, model=\"pooling\")\npbgtest(pooled.plm)\n\n    Breusch-Godfrey/Wooldridge test for serial correlation in panel models\n\ndata:  polity2 ~ allsettle + laggdpam + laggdpchg + lagtradeopen + lagmilper +     lagmilex + lagupop + lagsumdown\nchisq = 6433.4, df = 13, p-value < 2.2e-16\nalternative hypothesis: serial correlation in idiosyncratic errors"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"a-partial-adjustment-model","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.5.3 A partial adjustment model","text":"paper assignment, use partial adjustment model - lagged value dependent variable. implies gradual incorporation change X Y - partial adjustment period 1, period 2….long-run.code chunk replicates results reported paper. second column numbers matches Table 2, Model 2 JOP piece.\nTable 1. Polity 2 scores function trade, economics, borders\n\nresults clearly support expectations outlined paper. Nations settled borders democratic. short-run effect 0.179 units, , since partial adjustment model, know long-run effect (0.179)/(1-0.962) 4.71 units, large (almost large believable) -10 10 scale. Owziak reports text effects ten periods - calculate number? coefficient lagged military expenditures? mean coefficient negative significant, value zero? talk good practice.warning signs specification approach might misleading. trade openness military expenditures consistent expectation, GDP change - higher levels GDP associated lower levels democratization? coefficient consistent expectations previous downward movement polity scale? think, yes, might see upward movement low levels, explained clarified article. can also see democracy score persistent, coefficient nearing 1.0 , long adjustment processes.","code":"\n\n# This models pools the data, ignoring unit heterogeneity.  I estimate and report the coefficients along with 2 sets of standard errors:  conventional, clustered (as reported in the paper),\n\n# This is standard OLS\n\npooled.pa <- lm(polity2~ allsettle+laggdpam+laggdpchg+lagtradeopen+lagmilper+lagmilex+lagupop+lagsumdown+ lagpolity, data=jop, na.action=na.omit)\n\n#cov2 is the variance covariance matrix calculated with an adjustment that compensates for the presence of heteroskedasticity,  We have not used this before, so check out the syntax carefully\n\n\ncluster_se2        <- sqrt(diag(vcovCL(pooled.pa, cluster = ~ ccode, type = \"HC2\")))\n\n\nstargazer(pooled.pa, pooled.pa, style=\"apsr\", type=\"html\", se = list(NULL, cluster_se2), column.labels=c(\"Standard\", \"Clustered\"), title=\"**Table 1. Polity 2 scores as a function of trade, economics, and borders**\", notes= \"p<.10* ; p<.05**\",  notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=3, dep.var.labels = c(\"Polity democracy score\"), covariate.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"what-about-unit-effects","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.5.4 What about unit effects?","text":"far appear good shape - series stationary, can add lagged dependent variable work serial correlation. unit effects form discuss last week?strategy estimate model without lagged dependent variable compare results model without lagged dependent variable adding dummy variables country (least squares dummy variable approach handling fixed effects).\nTable 1. Polity 2 scores function trade, economics, borders, fixed effects\nAnalysis Variance TableModel 1: polity2 ~ allsettle + laggdpam + laggdpchg + lagtradeopen + lagmilper +\nlagmilex + lagupop + lagsumdown\nModel 2: polity2 ~ allsettle + laggdpam + laggdpchg + lagtradeopen + lagmilper +\nlagmilex + lagupop + lagsumdown + factor(ccode)\nRes.Df RSS Df Sum Sq F Pr(>F)\n1 7229 303377\n2 7098 125977 131 177400 76.301 < 2.2e-16 ***\n—\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1This output indicates model performance substantially improved add country dummies. residual sum squares - error - much lower model 2, includes country factor.suggests perhaps results published JOP piece ignore unmeasured heterogeneity.","code":"\nlsdv <- lm(polity2~ allsettle+laggdpam+laggdpchg+lagtradeopen+lagmilper+lagmilex+lagupop+lagsumdown +factor(ccode), data=jop, na.action=na.omit)\n\n#I used heteroskedasticity-consistent standard errors rather than clustered, since I can't cluster on the countries if I have a dummy variable for each country\n\ncov3         <-    vcovHC(lsdv, type = \"HC2\")\ncluster_se3        <- sqrt(diag(cov3))\n\nstargazer(lsdv, lsdv, omit=\"ccode\", se = list(NULL, cluster_se3), column.labels=c(\"Standard\", \"Consistent\"), style=\"apsr\", type=\"html\", title=\"**Table 1. Polity 2 scores as a function of trade, economics, and borders, fixed effects**\", notes= \"p<.10* ; p<.05**\",  notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=3, dep.var.labels = c(\"Polity democracy score\"), covariate.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))\n\n\nanova(pooled.ols, lsdv)"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"using-a-lagged-dependent-variable-with-fixed-effects","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.6 Using a lagged dependent variable with fixed effects","text":"well-known bias introduced panel models contain unit effects lagged dependent variable. Labeled, Nickell bias, due Nickell (1981), problem rely short time series (case applications panel studies), impossible differentiate unit effects passed next time period lagged dependent variable unit effects show intercept. result, conventional wisdom firm rule lagged dependent variables never combined fixed effects. Beck Katz (2011) demonstrate bad advice: since size bias related 1/T - small T panel (T=3), might bias approach 30 percent, large T panel (T=90, ) implies bias 1 percent - bias acceptable given tremendous gain realize - practically (serial correlation) theoretically (long-term dynamics) include lagged dependent variable.happens used lagged Y control unit effects? code chunk reports new results original output paper.\nTable 1. Polity 2 scores function trade, economics, borders, fixed effects\n","code":"\n\nfinal <- lm(polity2~ allsettle+laggdpam+laggdpchg+lagtradeopen+lagmilper+lagmilex+lagupop+lagsumdown+factor(ccode)+lagpolity, data=jop, na.action=na.omit)\n\n#I used heteroskedasticity-consistent standard errors rather than clustered, since I can't cluster on the countries if I have a dummy variable for each country\n\ncov4         <-    vcovHC(final, type = \"HC2\")\ncluster_se4        <- sqrt(diag(cov4))\n\n#stargazer(pooled.pa, final, style=\"qje\", type=\"text\", omit=\"ccode\", se = list(cluster_se2, cluster_se4), column.labels=c(\"As reported\", \"Fixed effects\"))\n\nstargazer(pooled.pa, final, omit=\"ccode\", se = list(cluster_se2, cluster_se4) ,column.labels=c(\"As reported\", \"Fixed effects\"),  style=\"apsr\", type=\"html\", title=\"**Table 1. Polity 2 scores as a function of trade, economics, and borders, fixed effects**\", notes= \"p<.10* ; p<.05**\",  notes.append=FALSE, omit.stat=c(\"ser\",\"f\"), star.cutoffs=c(0.10,0.05), digits=3, dep.var.labels = c(\"Polity democracy score\"), covariate.labels = c(\"Trade openness\", \"Economic growth\", \"Settled borders\", \"Constant\"))"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"one-other-detail","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.7 One other detail","text":"panel models describe last week hinge assumption can difficult sustain evaluate: strict exogeneity. means error past future uncorrelated current value X. variety mechanisms violate assumption.Feedback - values Y today lead changes X future. Consider running example - democratic nations experience higher levels economic growth, trade openness? specific example broader problem called simultaneity biasThere straight forward way test strict exogeneity using fixed effects. Just add future values X model. feedback, information significant.can see feedback case. Democratization time t consistent trade openness time t+1\n## feasible generalized least squares approaches?read literature political science, especially 1985-95, see statistical treatments panel data use generalized least square GLS. Beck Katz (1995) note inappropriate use GLS estimators T much larger N. can, course, estimate model data (T less 100 N 200). output consistent problem identified Beck Katz. standard errors coefficients much low.","code":"se.model <- plm(polity2 ~ lagtradeopen + laggdpchg +lead(lagtradeopen, k=2), data=jop, index=c(\"ccode\", \"year\"), na.action=na.omit, model=\"within\")\n\nsummary(se.model)\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = polity2 ~ lagtradeopen + laggdpchg + lead(lagtradeopen, \n    k = 2), data = jop, na.action = na.omit, model = \"within\", \n    index = c(\"ccode\", \"year\"))\n\nUnbalanced Panel: n = 154, T = 1-85, N = 7546\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-14.44631  -2.68323  -0.28629   2.44581  16.89374 \n\nCoefficients:\n                          Estimate Std. Error t-value  Pr(>|t|)    \nlagtradeopen               0.54497    0.13211  4.1250 3.748e-05 ***\nlaggdpchg                 -2.17641    0.79086 -2.7520 0.0059384 ** \nlead(lagtradeopen, k = 2)  0.48102    0.13086  3.6758 0.0002388 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    152510\nResidual Sum of Squares: 131380\nR-Squared:      0.13857\nAdj. R-Squared: 0.12038\nF-statistic: 396.198 on 3 and 7389 DF, p-value: < 2.22e-16"},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"things-i-did-not-have-time-to","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.7.1 Things I did not have time to","text":"","code":""},{"path":"dynamics-panel-models---a-brief-introduction.html","id":"panel-corrected-standard-errors-compared-to-cluster-using-the-plm-package","chapter":"8 Dynamics panel models - a brief introduction","heading":"8.7.1.1 Panel-corrected standard errors compared to cluster using the plm package","text":"","code":""},{"path":"counts.html","id":"counts","chapter":"9 Counts","heading":"9 Counts","text":"","code":""},{"path":"counts.html","id":"generalized-linear-models-in-r","chapter":"9 Counts","heading":"9.1 Generalized Linear Models in R","text":"R includes function permits estimate wide variety models using maximum likelihood - glm general linear model. idea specify dependent variable Y distributed function links observed data X underlying probabilities associated observed outcome Y. probability function (probability density function PDF) determined designating “family” way transform X get Y determined “link.”dependent variable distributed approximately normally – precisely, error term associated difference linear function X observed Y distributed normally, family(Gaussian) link(id) model – simple linear model used many times.Family(binomial) link(logit) logistic regression. assumption dependent variable distributed according binomial distribution function links observed p underlying X logit:\\[(\\beta_0+\\beta_1X)  = \\log (p/1-p)\\]Family(binomial) link(probit) known probit. dependent variable distributed according binomial distribution function links X p inverse cumulative normal distribution:\\[(\\beta_0+\\beta_1X) =\\Phi^{-1}= \\left(\\frac{1}{\\sqrt{2\\pi}} \\exp(-p^2/2) \\right)^{-1} = \\frac{\\sqrt{2\\pi}}{\\exp(-p^2/2)}.\\]\nLogit probit closely related. distributions compared figure . code chunk evaluates logit inverse cumulative normal distribution 0.00 1.00 increments 0.1. blue line logit red line inverse cumulative normal. can see marginal effects might diverge many observations predicted probabilities extremes.available families R include:\n- binomial(link = “probit”) <==probit\n- binomial(link = “logit”) <==logit\n- gaussian(link = “identity”) <==OLS\n- Gamma(link = “inverse”)\n- inverse.gaussian(link = “1/mu^2”)\n- poisson(link = “log”) <==poisson\n- quasi(link = “identity,” variance = “constant”)\n- quasibinomial(link = “logit”)\n- quasipoisson(link = “log”)","code":"\np<-seq(0,1,0.01)\ny1<-(log(p/(1-p)))\ny2<-qnorm(p)\ndata<-as.data.frame(cbind(p,y1,y2))\nggplot(data=data)+\n  geom_line(aes(x=p, y=y1), color=\"red\") +\n  geom_line(aes(x=p, y=y2), color=\"blue\") +\n  labs(x=\"Probability\", y=\"\")"},{"path":"counts.html","id":"count-models","chapter":"9 Counts","heading":"9.2 Count models","text":"important class models glm function permits us incorporate, beyond OLS, probit, logit, count models.Count models describe instances observed count events aggregated time. many years education beyond high school participant Youth Socialization Panel complete period 1972-1982? number vary 0 7. OLS make little sense apply data recover predicted values fall 0 exceed 7. need kind distributional assumption Y permits zeroes implies type upper bound.forms count data methods. instance, three four categories, use extensions logit (ordered multinomial) alternatives discrete outcomes, probit, related extensions (ordered multinomial).count unit time – months, years, days – options. observing number periods something took fail succeed, use survival data analysis (lets us talk duration dependence). Next week talk type approach.","code":""},{"path":"counts.html","id":"poisson-regression","chapter":"9 Counts","heading":"9.3 Poisson regression","text":"common strategy analyze simple counts known Poisson regression – data generating process Y assumed Poisson distribution (Poisson “distribution function” family Y). distribution turns describe number processes involve waiting line. , classically, number horsemen Prussian army killed kicks horses (details, read Ladislaus Bortkiewicz Härdle Vogt (2014)).shorthand Y \\(\\sim {\\rm poisson}(\\lambda)\\) used indicate \nrandom variable Y Poisson distribution scale parameter \\(\\lambda\\).Poisson random variable Y scale parameter \\(\\lambda\\) distribution\\[p(Y) = \\frac{\\lambda^Y e^{-\\lambda}}{Y!}\\]\nX=0,1,2,3…. \\(E(Y) = Var(Y) = \\lambda\\)code chunk generates random draws Poisson distribution 4 different settings \\(\\lambda\\): 0.25, 0.50, 1.0, 2.0","code":"\n\nx1<-rpois(100,0.25)\nx2<-rpois(100,0.50)\nx3<-rpois(100,1.0)\nx4<-rpois(100,2.0)\npar(mfrow = c(2, 2))\nbarplot(table(x1), main = expression(paste(lambda,\"=0.40\")), ylim=c(0,80))\nbarplot(table(x2), main = expression(paste(lambda,\"=0.80\")), ylim=c(0,80))\nbarplot(table(x3), main = expression(paste(lambda,\"=1.2\")), ylim=c(0,80))\nbarplot(table(x4), main = expression(paste(lambda,\"=1.6\")), ylim=c(0,80))"},{"path":"counts.html","id":"an-example-from-the-anes","chapter":"9 Counts","heading":"9.3.1 An example from the ANES","text":"Earlier term looked level political activity men women - count variable zero (activity) many people, four people politically active. code chunks read data plot distribution activity 1960 ANES sample.data clearly consistent distributions , Poisson regression appropriate. investigate tests better evaluate suitability Poisson distribution data, can check results first. test difference men women 1960, introduce controls age, education, income race. first set code chunks focuses difference men women.logistic regression, coefficients parameters line. Instead coefficients estimate log \\(\\lambda\\) men women: -0.5850 men (-0.5850-0.2684) =0.8503.code chunk plots distribution activity implied coefficients. can see model implies men much likely women engage least one political activity.two things notice output. output reminds us dispersion parameter taken 1.0 - assumption test. goodness--fit test statistic reported - AIC Akaike’s Information Criteria. test came discussed logistic regression. AIC simple function log likelihood function evaluated particular set parameter estimates.\\[AIC = −N * L(m_*)+2k\\]N number observations, k number model parameters, \\(L_(m_*)\\) log likelihood.can compare AIC two models determine model fits data better. code chunk adds age, education, income race see gender still matters introduce controls.output includes estimate deviance - analogous residual sum squares OLS. null deviance model fit constant appears model. residual deviance model fit model stimated. see improvement reduction error. can even test difference statistically significant.test indicates improvement deviance statistically significant.add information improve prediction? similar strategy one used create model turnout model use assignment.output reveals , 1960, income education good predictors political engagement gender matters . can also see output value AIC dropped - means complex model better fit.","code":"\n# read the data from the current directory\n# ANES cumulative file \ntemp<-read.csv(\"data/anes_timeseries_cdf_rawdata.txt\", header=TRUE, sep=\"|\")\nanes<-subset(temp, temp$VCF0004==1960)\n\n# recode missing values and create dummy variables\n# The mutate commands could be combined into one long command\n# But I prefer to do one at a time\n\n# note: all of the coding information is just cut and paste from the PDF codebook\n \n anes <- \n   anes %>%\n   mutate(education=VCF0140a,\n          education=na_if(education,8),\n          education=na_if(education,9))\n \n# #education is coded:\n# #1. 8 grades or less ('grade school')\n# #2. 9-12 grades ('high school'), no diploma/equivalency\n# #3. 12 grades, diploma or equivalency\n# #4. 12 grades, diploma or equivalency plus non-academic training\n# #5. Some college, no degree; junior/community college level degree (AA degree)\n# #6. BA level degrees\n# #7. Advanced degrees incl. LLB\n# \nanes <- anes %>%\n   mutate(turnout=\n          case_when(VCF0702==2 ~ 1,\n                    VCF0702==1 ~ 0))\n#turnout is coded:\n# #0 did not vote\n# #1 voted\n \n \n anes <- \n   anes %>%\n   mutate(income=VCF0114,\n          income=na_if(income,0))\n #income is coded:\n #1. 0 to 16 percentile\n #2. 17 to 33 percentile\n #3. 34 to 67 percentile\n #4. 68 to 95 percentile\n #5. 96 to 100 percentile\n \n anes <-\n   anes %>%\n   mutate(age=VCF0101,\n          age=na_if(age,0),\n          age=na_if(age,17))\n \n # age is age in years\n # 18 and over only\n # some very old (95+ are grouped together at 95 in ANES pre-1980, see the codebook)\n \n anes <-  anes %>%\n   mutate(female=\n   case_when(VCF0104==2 ~ 1,\n             VCF0104==1 ~ 0))\n # female=1 designate women\n # female=0 designates men\n # Other and refused are treated as missing\n \n # For this module, I specified female as a category or factor - this improves the look of the tables\n anes$female_c<-factor(anes$female, labels=c(\"Men\", \"Women\"))\n \n \n anes <-  anes %>%\n   mutate(minority=\n   case_when(VCF0105a>1 & VCF0105a<9~ 1,\n             VCF0105a==1 ~ 0))\n # minority=0 is white and not Hispanic\n # minority=1 is any other group, including all Hispanic-identifying\n\nanes <-  anes %>%\nmutate(activity=VCF0723a-1,\n          activity=na_if(activity,-1))\n# in the raw data, 0 is missing and activity ranges from\n# 1 (none) to 6 (high)\n# I substract one so that scale now ranges from 0 (none) to 5 (high)\nbarplot(table(anes$activity), ylab=\"N\", main=\"Distribution of campaign activity\")\nmodel1<-glm((activity)~female, data=anes, family = poisson(link = \"log\"))\nsummary(model1)\n\nCall:\nglm(formula = (activity) ~ female, family = poisson(link = \"log\"), \n    data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.0555  -1.0555  -0.9230   0.5331   2.5621  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.58503    0.05944  -9.842  < 2e-16 ***\nfemale      -0.26838    0.08625  -3.112  0.00186 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1145.0  on 1108  degrees of freedom\nResidual deviance: 1135.3  on 1107  degrees of freedom\n  (72 observations deleted due to missingness)\nAIC: 2040.9\n\nNumber of Fisher Scoring iterations: 5\nanova(model1,test=\"Chisq\")\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: (activity)\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev Pr(>Chi)   \nNULL                    1108     1145.0            \nfemale  1    9.696      1107     1135.3 0.001847 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nx<-rpois(500,exp(-0.5850))\nid1<-cbind(x,\"Men\")\nx<-rpois(500,exp(-0.8503))\nid2<-cbind(x,\"Women\")\ndata<-as.data.frame(rbind(id2,id1))\nggplot(data, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\") anova(model1,test=\"Chisq\")\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: (activity)\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev Pr(>Chi)   \nNULL                    1108     1145.0            \nfemale  1    9.696      1107     1135.3 0.001847 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nmodel2<-glm(activity~female+minority+age+income+education, data=anes, family=\"poisson\")\nsummary(model2)\n\nCall:\nglm(formula = activity ~ female + minority + age + income + education, \n    family = \"poisson\", data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6010  -0.9610  -0.7919   0.5420   3.0442  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.692636   0.266233  -6.358 2.05e-10 ***\nfemale      -0.233548   0.087210  -2.678 0.007407 ** \nminority    -0.110448   0.187332  -0.590 0.555470    \nage          0.004470   0.003342   1.338 0.181003    \nincome       0.182990   0.049595   3.690 0.000225 ***\neducation    0.101194   0.027378   3.696 0.000219 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1136.8  on 1099  degrees of freedom\nResidual deviance: 1071.0  on 1094  degrees of freedom\n  (81 observations deleted due to missingness)\nAIC: 1982.6\n\nNumber of Fisher Scoring iterations: 6"},{"path":"counts.html","id":"what-is-the-dispersion-parameter","chapter":"9 Counts","heading":"9.3.2 What is the dispersion parameter","text":"central problem leads rejection Poisson model overdispersion – variance dependent variable exceeds mean. presence overdispersion, estimates standard errors Poisson regression biased downward (leading Type errors). alternative known negative binomial regressionThe negative binomial probability distribution function two parameters:\\[p(x) = \\frac{\\lambda\\mu^x e^{-\\lambda\\mu}}{x!}\\]\n\\(\\mu=1\\), reduces PoissonThe logic negative binomial distribution:\n1. observe series independent trials outcomes success failure\n2. probability success failure constant one trial next.\n3. Trials conducted number (r) successes observedThe number produced function number failures occur observe rth success.distribution requires specification two parameters (average probability success, number successes observed).probability success 0.5, many failures occur observe 8 successes? probability success 0.5, observe 1 failure every one success – likely 8 failures 8 successes.plots shows probability observe x failures (1 8) observe 8 successes probability success 0.40 0.80.order determine two alternatives - negative binomial Poisson - appropriate, can first estimate model using approach called quasi-poisson - approach includes estimate dispersion parameter.case, dispersion parameter close 1.0, data likely fits poisson distribution, test see results hold negative binomial regression.","code":"\nx1 <- rnbinom(100, prob=0.50, size=1)\nx2<-  rnbinom(100, prob=0.50, size=8)\nx3<-  rnbinom(100, prob=0.80, size=1)\nx4<-  rnbinom(100, prob=0.80, size=8)\npar(mfrow = c(2, 2))\nbarplot(table(x1), main = expression(paste(p,\"=0.40, fails=1\")), ylim=c(0,80))\nbarplot(table(x2), main = expression(paste(p,\"=0.40, fails=8\")), ylim=c(0,80))\nbarplot(table(x3), main = expression(paste(p,\"=0.80, fails=1\")), ylim=c(0,80))\nbarplot(table(x4), main = expression(paste(p,\"=0.80, fails=8\")), ylim=c(0,80))model3<-glm(activity~female+minority+age+income+education, data=anes, family=\"quasipoisson\")\nsummary(model3)\n\nCall:\nglm(formula = activity ~ female + minority + age + income + education, \n    family = \"quasipoisson\", data = anes)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6010  -0.9610  -0.7919   0.5420   3.0442  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.692636   0.271546  -6.233 6.51e-10 ***\nfemale      -0.233548   0.088951  -2.626 0.008771 ** \nminority    -0.110448   0.191071  -0.578 0.563351    \nage          0.004470   0.003409   1.311 0.189965    \nincome       0.182990   0.050585   3.617 0.000311 ***\neducation    0.101194   0.027924   3.624 0.000304 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.040313)\n\n    Null deviance: 1136.8  on 1099  degrees of freedom\nResidual deviance: 1071.0  on 1094  degrees of freedom\n  (81 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 6"},{"path":"counts.html","id":"over-dispersion---negative-binomial","chapter":"9 Counts","heading":"9.3.3 Over-dispersion - Negative binomial","text":"Negative binomial regression requires glm.nb function MASS package.","code":"library(MASS)\n\nAttaching package: 'MASS'\nThe following object is masked from 'package:dplyr':\n\n    select\nmodel4 <-glm.nb(activity ~ female + minority + age + income + education, data=anes)\nsummary(model4)\n\nCall:\nglm.nb(formula = activity ~ female + minority + age + income + \n    education, data = anes, init.theta = 67.53212759, link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5947  -0.9591  -0.7910   0.5393   3.0266  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.693877   0.267304  -6.337 2.34e-10 ***\nfemale      -0.234179   0.087561  -2.674 0.007485 ** \nminority    -0.110799   0.187946  -0.590 0.555509    \nage          0.004487   0.003355   1.337 0.181107    \nincome       0.183079   0.049778   3.678 0.000235 ***\neducation    0.101357   0.027497   3.686 0.000228 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(67.5321) family taken to be 1)\n\n    Null deviance: 1128.4  on 1099  degrees of freedom\nResidual deviance: 1063.1  on 1094  degrees of freedom\n  (81 observations deleted due to missingness)\nAIC: 1984.6\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  68 \n          Std. Err.:  393 \n\n 2 x log-likelihood:  -1970.562 "},{"path":"counts.html","id":"under-dispersion---zero-inflated-regression","chapter":"9 Counts","heading":"9.3.4 Under-dispersion - Zero-inflated regression","text":"can case -dispersion - variance low relative mean require using known zero-inflated poisson regression, implemented \\(pscl\\) package zeroinfl function.","code":""},{"path":"counts.html","id":"using-the-predict-command-with-count-models","chapter":"9 Counts","heading":"9.4 Using the predict command with count models","text":"can use function cplot, used logistic regression, come predicted value \\(\\lambda\\) various groups - let us get handle variables matter . calculate predicted \\(\\lambda\\) ten groups , highest lowest value five predictors, variables held constant mean. simulate number activities expected based parameter compare pair groups figure table. probably much efficient way , works.One way make sense numbers might just see variable biggest differences percentage zero activity - rich vs. poor 54% rich zero vs. 73% poor. rich likely engage least one campaign activity. effect education comparable maybe little stronger. effect age, race, gender much smaller. know small effect race statistically significant. can imagine combined effect income education, wold find high income, high education respondents much likely engage.","code":"\nlibrary(margins)\ntest1<-cplot(model3, x=\"female\", type=\"response\", n=2, draw=FALSE)\ntest2<-cplot(model3, x=\"minority\", type=\"response\", n=2, draw=FALSE)\ntest3<-cplot(model3, x=\"age\", type=\"response\", n=2, draw=FALSE)\ntest4<-cplot(model3, x=\"education\", type=\"response\", n=2, draw=FALSE)\ntest5<-cplot(model3, x=\"income\", type=\"response\", n=2, draw=FALSE)\n\nx<-rpois(500,test1[1,2])\nid1<-cbind(x,\"Men\")\nx<-rpois(500,test1[2,2])\nid2<-cbind(x,\"Women\")\ndata1<-as.data.frame(rbind(id2,id1))\nggplot(data1, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\") prop.table(table(data1$x, data1$V2), margin=2)\n   \n      Men Women\n  0 0.572 0.686\n  1 0.350 0.244\n  2 0.070 0.064\n  3 0.006 0.006\n  4 0.002 0.000\n\n\nx<-rpois(500,test2[1,2])\nid1<-cbind(x,\"White\")\nx<-rpois(500,test2[2,2])\nid2<-cbind(x,\"People of color\")\ndata2<-as.data.frame(rbind(id2,id1))\nggplot(data2, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\")prop.table(table(data2$x, data2$V2), margin=2)\n   \n    People of color White\n  0           0.676 0.602\n  1           0.256 0.330\n  2           0.062 0.050\n  3           0.006 0.016\n  4           0.000 0.002\n\nx<-rpois(500,test3[1,2])\nid1<-cbind(x,\"Young\")\nx<-rpois(500,test3[2,2])\nid2<-cbind(x,\"Old\")\ndata3<-as.data.frame(rbind(id2,id1))\nggplot(data3, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\")prop.table(table(data3$x, data3$V2), margin=2)\n   \n      Old Young\n  0 0.596 0.658\n  1 0.310 0.280\n  2 0.072 0.056\n  3 0.022 0.006\n\n\nx<-rpois(500,test4[1,2])\nid1<-cbind(x,\"Low education\")\nx<-rpois(500,test4[2,2])\nid2<-cbind(x,\"High education\")\ndata4<-as.data.frame(rbind(id2,id1))\nggplot(data4, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\")prop.table(table(data4$x, data4$V2), margin=2)\n   \n    High education Low education\n  0          0.516         0.668\n  1          0.338         0.274\n  2          0.112         0.052\n  3          0.022         0.006\n  4          0.010         0.000\n  5          0.002         0.000\n\n\nx<-rpois(500,test5[1,2])\nid1<-cbind(x,\"Poor\")\nx<-rpois(500,test5[2,2])\nid2<-cbind(x,\"Rich\")\ndata5<-as.data.frame(rbind(id2,id1))\nggplot(data5, aes(x = x, fill = V2)) + geom_bar(position = \"dodge\")prop.table(table(data5$x, data5$V2), margin=2)\n   \n     Poor  Rich\n  0 0.742 0.572\n  1 0.224 0.264\n  2 0.030 0.134\n  3 0.004 0.028\n  4 0.000 0.002"},{"path":"counts.html","id":"exercise-modeling-political-activity-with-the-anes","chapter":"9 Counts","heading":"9.5 Exercise: Modeling political activity with the ANES","text":"Read ANES cumulative dataset pick election year - estimate campaign activity model poisson regression. Interpret output regression use strategy described , cplot, figure variables matter election year choose.","code":""},{"path":"counts.html","id":"next-week-duration-models","chapter":"9 Counts","heading":"9.6 Next week: duration models","text":"","code":""},{"path":"duration-models.html","id":"duration-models","chapter":"10 Duration models","heading":"10 Duration models","text":"Survival analysis duration models distinct count models. distinction takes form duration dependence - probability failure today may related probability failure past. know flips coin turns card unrelated – memory. know, hand, mechanical parts increasing probability failure age (people ). One recent text suggests use “whether ” test determine survival analysis appropriate technique use. research question contains word “” “whether,” might need learn technique. addicts relapse? nurses leave profession? governments fail? programs terminated?","code":""},{"path":"duration-models.html","id":"longitudinal-data","chapter":"10 Duration models","heading":"10.1 Longitudinal data","text":"Survival data analysis offers convenient empirical strategy identify features lead success failure – life death – subject. program patient innovation thrive fail?Prominent biostatistics, survival data describe time subject (patient program) comes observation, time subject failed (patient death program termination), attributes subject period observation. empirical objective model time failure subject observation.order exploit survival data analysis techniques, must observe subjects one (preferably many one) time periods – typical prescription three waves metric time (months observation, miles driven car). Survival data analysis one several types “longitudinal data analysis.” Much statistical foundations work come biostatistics, specifically models evaluate efficacy medical interventions (survival time really survival time).","code":""},{"path":"duration-models.html","id":"the-challenge-of-censoring","chapter":"10 Duration models","heading":"The challenge of censoring","text":"one important technical consideration survival data analysis: censoring. Consider clinical trial. Subjects recruited know certainty date subject came observation (given treatment first time, instance.) also know subjects move observation, due end trial experiment. trial ends, really don’t know fully recover . subjects experience termination event period observation ends right-censored. subject right-censored excluded statistical model - details implications .many political science applications, use observational approach picks arbitrary start date. case, don’t know, start date, long subject initial state. known left-censoring.ever read medical studies, might notice one metric median survival time (cancer therapy, instance. use median get around censoring problem - want figure average lifespan control treatment group, need wait everyone died. can learn median survival time group without treatment (number months years 50 percent group died) compare control group.","code":""},{"path":"duration-models.html","id":"the-hazard-function-and-survivor-function","chapter":"10 Duration models","heading":"10.2 The hazard function and survivor function","text":"Box-Steffensmeier Jones (1997) provide introduction modeling framework describe application duration analysis political science questions ranging durations war career paths members Congress. thorough technical introduction, see Hougaard (2000).core empirical theoretical concepts hazard function surivor function.","code":""},{"path":"duration-models.html","id":"what-is-the-hazard-function","chapter":"10 Duration models","heading":"What is the hazard function?","text":"hazard function summarizes probability subject observation experiences event time t, given survived t-1. can think two hazard functions represent opposite extremes - one hazard event increases every period subject survives. mechanical part subject wear likely survives first period, , time, probability failure increases. second extreme case survival early periods makes survival likely later periods - kind process weeds weak survivors hardy.Three hazard functions reproduced - red line shows typical example increasing hazards, green line consistent decreasing hazards - subject survives early time periods, hazard diminishes. blue line constant hazard. talk parameter values . adapted figures Moore (n.d.).","code":"\n\n# These examples from Moore (2016) in the Use R! series.   Figure 2.4\n\nweibHaz <- function(x, shape, scale) dweibull(x, shape=shape,scale=scale)/pweibull(x, shape=shape, scale=scale,lower.tail=F)\n\ncurve(weibHaz(x, shape=1.5, scale=1/0.03), from=0, to=80,ylab=\"Hazard\", xlab=\"Time\", col=\"red\")\ncurve(weibHaz(x, shape=1.0, scale=1/0.03), from=0, to=80,ylab=\"Hazard\", xlab=\"Time\", col=\"blue\", add=TRUE)\ncurve(weibHaz(x, shape=0.075, scale=1/0.03), from=0, to=80,ylab=\"Hazard\", xlab=\"Time\", col=\"green\", add=TRUE)\ntext(8.0,0.06, expression(alpha == 0.075))\ntext(50.0,0.035, expression(alpha == 1.0))\ntext(60.0,0.005, expression(alpha == 1.5))"},{"path":"duration-models.html","id":"what-is-the-survival-function","chapter":"10 Duration models","heading":"What is the survival function?","text":"survival function summarizes total proportion subjects survived event - 1.0 period 0 , eventually, 0. Every hazard function associated survival function. survival functions associated hazard functions . can see 1/2 subjects experience event time t=1 green hazard function takes 25 periods see hazard functions. latter, median survival time 20 periods.","code":"\n\n# These examples from Moore (2016) in the Use R! series.   Figure 2.4\n\nweibSurv <- function(t, shape, scale)\n   pweibull(t, shape=shape,scale=scale, lower.tail=F)\ncurve(weibSurv(x, shape=1.5, scale=1/0.03), from=0, to=80,ylim=c(0,1), ylab=\"Survival probability\", xlab=\"Time\", col=\"red\")\ncurve(weibSurv(x, shape=1.0, scale=1/0.03), from=0, to=80,ylim=c(0,1), ylab=\"Survival probability\", xlab=\"Time\", col=\"blue\", add=TRUE)\ncurve(weibSurv(x, shape=0.075, scale=1/0.03), from=0, to=80,ylim=c(0,1), ylab=\"Survival probability\", xlab=\"Time\", col=\"green\", add=TRUE)\nabline(h=0.5)"},{"path":"duration-models.html","id":"parametric-vs-nonparametric-approaches","chapter":"10 Duration models","heading":"10.3 Parametric vs nonparametric approaches","text":"Survival analysis requires several modeling choices, arguably important choices use parametric nonparametric statistical modeling approach appropriate form survivor function parametric approach.nonparametric approach (Cox proportional hazards model) permits estimation assumptions risk termination changes (requiring weak testable assumption proportionality hazard across programs time). nonparametric approach , however, permit graphical representation underlying hazard termination, useful visualization outcomes interest.Parametric models require assumptions distribution describes survivor function. variety functional forms can used just talk two: Weibull distribution generalized gamma distribution.","code":""},{"path":"duration-models.html","id":"the-weibull-distribution","chapter":"10 Duration models","heading":"The Weibull distribution","text":"important class commonly used models, survivor function assumed form makes associated risk termination period observation (hazard function) monotonic. Monotonicity implies risk either increases time decreases time, forecloses increasing risk outset later decreasing risk. Weibull distribution widely-used cases. distribution two parameters (\\(\\lambda\\)) known scale parameter(1/$)\\[ h(t)=\\alpha\\lambda^\\alpha t^{\\alpha-1}\\]\nfigures examples Weibull distribution \\(\\lambda\\) equal 0.075, 1.0 1.5. scale parameter case 0.03.","code":""},{"path":"duration-models.html","id":"generalized-gamma-distribution","chapter":"10 Duration models","heading":"Generalized gamma distribution","text":"Since (theoretically) probability survival fact initially increase decline many social science applications, imposing monotonic hazard function may appropriate.One broader class accelerated failure time models, generalized gamma distribution extremely complex, general form reduce distributions special case. useful since monotonically increasing certain time period, monotonically decreasing . example .","code":""},{"path":"duration-models.html","id":"an-exampleterminating-federal-programs","chapter":"10 Duration models","heading":"10.4 An example:terminating federal programs","text":"government programs immortal? Theoretical work public sector programs seems offer two different yet equally well-developed answers question.Since Congress create structure bureaucracy de novo, current programs may represent needs contemporary legislators (problem legislative coalitional drift). managers program may adapt innovate ways frustrate original architects program (bureau drift). Either type drift suggests , time, program agency may increasingly step members Congress vulnerable termination.Another set research emphasizes obstacles reform-minded politicians fiscal conservatives face attempting abolish public sector programs. Downs (1966), instance, makes explicit claim “older bureau less likely die.”Certainly perspectives descriptively accurate. Either programs face credible risk termination . two perspectives can distinguished answer single empirical question: hazards termination highest program creation (implying older, surviving programs difficult terminate) probability termination increase time (implying vulnerability older programs)? Estimation description hazard termination sample federal programs answer question.hazard function implied “program immortality” perspective summarized Figure 1, reproduced Corder (2004).hazard function implied “drift” perspective summarized Figure 2.application survival analysis compelling since two perspectives imply different prospects programs time, suggesting crisp empirical test.","code":""},{"path":"duration-models.html","id":"the-data","chapter":"10 Duration models","heading":"10.4.1 The data","text":"Indicators program activity used project federal credit programs (loan loan guaranteee program) included entries Catalog Federal Domestic Assistance (CFDA). CFDA provides exhaustive list active credit programs exclusive -budget agencies (Export-Import Bank) secondary market programs Government-Sponsored Enterprises (GSEs). Federal credit programs described CFDA span cabinet departments, components number independent agencies account primary activity one agency, Small Business Administration. sample includes credit programs existing 1974 programs created 1974. Termination identified date last entry CFDA. date applications loans guarantees accepted program.","code":""},{"path":"duration-models.html","id":"the-hazard-function-in-this-sample","chapter":"10 Duration models","heading":"10.4.2 The hazard function in this sample","text":"Results reported based generalized gamma distribution survivor function, conventional way introduce non-monotonic hazards linear survival model. One broader class accelerated failure time models, generalized gamma distribution best empirical fit survivor function credit program data based Akaike Information Criteria). distributional assumption absolutely effect substantive results reported table – results number different distributional families. distributional assumption important graphical representation hazard.upshot programs face increasing vulnerability 10 years old, point hazards diminish practically zero long term. result replicated number contexts scholars. results suggest emphasis legislative bureau drift misplaced. empirical reality old programs unlikely terminated.","code":""},{"path":"duration-models.html","id":"an-example-shelter-in-place-orders-sipos-in-the-united-states","chapter":"10 Duration models","heading":"10.5 An example: Shelter-In-Place Orders (SIPOs) in the United States","text":"","code":""},{"path":"duration-models.html","id":"what-explains-state-variation-in-timing","chapter":"10 Duration models","heading":"10.5.1 What explains state variation in timing?","text":"section reproduces figures models 2020 paper co-authored appeared Policy Design Practice. relied two durations - time announcement pandemic (5 January 2020) data state crosses 1:100,000 threshold COVID-19 cases per capita, time crossing threshold implementation first SIPO.first chunk, echoed, reads manages data cases state characteristics.","code":""},{"path":"duration-models.html","id":"box-plots","chapter":"10 Duration models","heading":"10.5.2 Box plots","text":"box plots summarize two different times: time reach 1:100,000 (time1) time 1:100,000 case implementation (tto3).long delays Democratic states first cases United States - Washington, Illinois California.box plot includes eight Republican states start date entered end observation period - manage right-censoring.","code":""},{"path":"duration-models.html","id":"kaplan-meier-curves","chapter":"10 Duration models","heading":"10.5.3 Kaplan Meier curves","text":"discussed parametric approaches estimating hazard survival functions. One common non-parametric approach just uses sample information abou proportion subjects entering observation experience event. produce figure shows percentage subjects experience event (survivors states implement SIPO “failure” case implementation action). proportion reported four groups states. called “Kaplan-Meier” curve","code":""},{"path":"duration-models.html","id":"parametric-models","chapter":"10 Duration models","heading":"10.5.4 Parametric models","text":"Weibull models two durations .Model 1 time announcement 1:100,000\nModel 3 time 1:100,000 SIPO 42 states implemented SIPOs. Model 5 includes eight states never implemented action data 30 May, end observation period.","code":""},{"path":"duration-models.html","id":"time-to-1100000","chapter":"10 Duration models","heading":"10.5.4.0.1 Time to 1:100,000","text":"Weibull model - predicting time takes states reach 1 case per 100,000 population, based political factors","code":"\n============================================================================\n                                                    timetocase              \n----------------------------------------------------------------------------\ncontrolDemocratic governor only                      -0.03***               \n                                                      (0.01)                \n                                                                            \ncontrolRepublican governor only                      -0.05***               \n                                                      (0.02)                \n                                                                            \ncontrolDemocratic governor and senate                -0.04***               \n                                                      (0.01)                \n                                                                            \nurban                                                -0.0003                \n                                                     (0.0004)               \n                                                                            \nelderly                                               0.002                 \n                                                     (0.002)                \n                                                                            \ntaxrevenuepercapita                                  -0.0001                \n                                                     (0.0003)               \n                                                                            \npopulation                                           -0.0003                \n                                                     (0.001)                \n                                                                            \nConstant                                             4.31***                \n                                                      (0.05)                \n                                                                            \nN                                                       50                  \nLog Likelihood                                        -93.18                \nchi2                                            32.86*** (df = 7)           \n============================================================================\nNotes:                                ***Significant at the 1 percent level.\n                                       **Significant at the 5 percent level.\n                                       *Significant at the 10 percent level."},{"path":"duration-models.html","id":"time-to-sipo-with-censored-data-excluded","chapter":"10 Duration models","heading":"10.5.4.1 Time to SIPO with censored data excluded","text":"","code":"\n============================================================================\n                                                      tto3_a                \n----------------------------------------------------------------------------\ncontrolDemocratic governor only                       -0.24                 \n                                                      (0.17)                \n                                                                            \ncontrolRepublican governor only                       -0.03                 \n                                                      (0.27)                \n                                                                            \ncontrolDemocratic governor and senate                 -0.07                 \n                                                      (0.21)                \n                                                                            \nurban                                                 -0.001                \n                                                     (0.004)                \n                                                                            \nelderly                                               0.003                 \n                                                      (0.03)                \n                                                                            \ntaxrevenuepercapita                                   -0.003                \n                                                     (0.003)                \n                                                                            \npopulation                                            0.0001                \n                                                      (0.01)                \n                                                                            \nConstant                                             2.96***                \n                                                      (0.71)                \n                                                                            \nN                                                       42                  \nLog Likelihood                                       -121.84                \nchi2                                              5.44 (df = 7)             \n============================================================================\nNotes:                                ***Significant at the 1 percent level.\n                                       **Significant at the 5 percent level.\n                                       *Significant at the 10 percent level."},{"path":"duration-models.html","id":"time-to-sipo-with-censored-observations-included.","chapter":"10 Duration models","heading":"10.5.4.2 Time to SIPO with censored observations included.","text":"calculated predicted values number days order implemented. variables held equal mean:Republican governor senate: 51 days\nRepublican governor Democratic senate: 10 days\nDemocratic governor Republican senate: 9 days\nDemocratic governor Democratic senate: 12 days\nRepublican governor, population 1 million\nRepublican governor, Democratic senate, population=1 million: 12\nRepublican governor, Democratic senate, population=10 million: 7.8The largest non-political effect much, much smaller effect partisan control.","code":"\n============================================================================\n                                                   tto3_uncen_a             \n----------------------------------------------------------------------------\ncontrolDemocratic governor only                      -1.44***               \n                                                      (0.26)                \n                                                                            \ncontrolRepublican governor only                      -1.72***               \n                                                      (0.40)                \n                                                                            \ncontrolDemocratic governor and senate                -1.58***               \n                                                      (0.27)                \n                                                                            \nurban                                                 -0.01                 \n                                                      (0.01)                \n                                                                            \nelderly                                               -0.10*                \n                                                      (0.05)                \n                                                                            \ntaxrevenuepercapita                                   0.01**                \n                                                      (0.01)                \n                                                                            \npopulation                                           -0.05***               \n                                                      (0.01)                \n                                                                            \nConstant                                             5.21***                \n                                                      (1.29)                \n                                                                            \nN                                                       50                  \nLog Likelihood                                       -154.28                \nchi2                                            47.81*** (df = 7)           \n============================================================================\nNotes:                                ***Significant at the 1 percent level.\n                                       **Significant at the 5 percent level.\n                                       *Significant at the 10 percent level."},{"path":"duration-models.html","id":"other-notes","chapter":"10 Duration models","heading":"10.6 Other notes","text":"","code":""},{"path":"duration-models.html","id":"paper-6-modeling-political-activity-with-the-anes","chapter":"10 Duration models","heading":"10.6.1 Paper 6: Modeling political activity with the ANES","text":"RMD shell paper 6 available. need pick election year estimate campaign activity model Poisson regression, quasi-poisson negative binomial regression. interpret output one regression use strategy , cplot, figure variables matter election year choose.","code":""},{"path":"duration-models.html","id":"note-on-qualitative-methods","chapter":"10 Duration models","heading":"10.6.2 Note on qualitative methods","text":"recent issue Perspectives Politics introduced really important set documents discussions: Qualitative Transparency Deliberations. short summary article:recent years, variety efforts made political science enable, encourage, require scholars open explicit bases empirical claims , turn, make claims readily evaluable others. qualitative scholars long taken interest making research open, reflexive, systematic, recent push overarching transparency norms requirements provoked serious concern within qualitative research communities raised fundamental questions meaning, value, costs, intellectual relevance transparency qualitative inquiry.Jacobs et al. (2021)short article presents findings much larger set reports several working groups , together, give good sense scope qualitative work Political Science questions qualitative research practice political scientists confront. entire collection available link:\nhttps://static.cambridge.org/content/id/urn:cambridge.org:id:article:S1537592720001164/resource/name/S1537592720001164sup002.pdfYou construct semester-length seminar just first report. take look, plan adopt qualitative approach work, make sense read report, learn work.","code":""},{"path":"references.html","id":"references","chapter":"11 References","heading":"11 References","text":"","code":""},{"path":"using-the-american-national-election-study.html","id":"using-the-american-national-election-study","chapter":"Using the American National Election Study","heading":"Using the American National Election Study","text":"","code":""},{"path":"the-harvard-dataverse.html","id":"the-harvard-dataverse","chapter":"The Harvard Dataverse","heading":"The Harvard Dataverse","text":"","code":""},{"path":"replicating-when-do-politicians-grandstand.html","id":"replicating-when-do-politicians-grandstand","chapter":"Replicating “When do politicians grandstand?”","heading":"Replicating “When do politicians grandstand?”","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-models-and-evidence-based-policymaking","chapter":"A Data, models and evidence-based policymaking","heading":"A Data, models and evidence-based policymaking","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"what-can-we---as-political-scientists---learn-from-advances-in-evidence-based-policymaking","chapter":"A Data, models and evidence-based policymaking","heading":"A.1 What can we - as political scientists - learn from advances in evidence-based policymaking?","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data","chapter":"A Data, models and evidence-based policymaking","heading":"A.1.1 Data","text":"new exciting opportunities use readily available electronic records, challenges matching records diverse sources, data security persistent challenge centralized collection information individuals invites extraordinary surveillance surveillance government.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"models","chapter":"A Data, models and evidence-based policymaking","heading":"A.1.2 Models","text":"One thing unites social science research strategies evidence-based policymaking use models.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-based-policymaking-in-practice","chapter":"A Data, models and evidence-based policymaking","heading":"A.1.3 Evidence-based policymaking in practice","text":"Research design choices, novel use data, resources evaluating evidence. research design approach use - theory applied problems motivating data collection - analogous evidence-based policymaking. research designs (data collection strategies) borrow work?","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-automation-of-data-collection","chapter":"A Data, models and evidence-based policymaking","heading":"A.2 Data: automation of data collection","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-collection-manually-versus-automated-90-testers-vs-3000000-actual-transactions-why-now","chapter":"A Data, models and evidence-based policymaking","heading":"A.2.1 Data collection – manually versus automated (90 testers vs 3,000,000 actual transactions) (Why now?)","text":"information stored electronically. Data “captured electronically first instance” Ayers example points costs challenges data collection – site visits, paper forms, data entry. Example: interest rate discrimination new car purchases – data set three million auto loan transactions – recoding name features captured lender, merged CA driver’s license data includes Social Security Number race. digital data now commodity. data frequently stored ways permit easy access…\nmany organizations embracing enterprise data solutions","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"what-is-administrative-data","chapter":"A Data, models and evidence-based policymaking","heading":"A.2.2 What is administrative data?","text":"different survey data? Administrative data – routinely collected non-statistical purposes (efficient use, broad coverage, timely), readily accessible digital records key (can exploit ). Example (?): Bank regulation: variety data available Financial Holding Companies: https://www.ffiec.gov/npw/Institution/Profile/2380443?dt=20150101\nSources administrative data (individual-level records vs. aggregated reports)","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"local-governments-have-been-innovators","chapter":"A Data, models and evidence-based policymaking","heading":"A.2.3 Local governments have been innovators","text":"s adoption performance metrics – pervasive data collection. Can scaled national level? Clinton administration initiatives (GPRA) Bush administration efforts culminated PART. didn’t things improve faster? missing? need high quality evidence. Today; “don’t keep score, don’t know inning , everybody gets trophy.” (talk distinction performance metrics evidence success )","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-storage","chapter":"A Data, models and evidence-based policymaking","heading":"A.3 Data: storage","text":"data available, still 3 important constraints:\n- statistical knowledge: “core regression randomization techniques” well-established\n- computational power: Computing power problem decades\n- storage: storage limit (compare $50 1TB drive today vs $400 book c. 2008)","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-matching-and-integration","chapter":"A Data, models and evidence-based policymaking","heading":"A.4 Data – matching and integration","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"merging","chapter":"A Data, models and evidence-based policymaking","heading":"A.4.1 Merging","text":"– function access, freeze/capture/preserve match – still challenge – improvements Ayers labels database technology ongoing","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"example-florida-felon-matching","chapter":"A Data, models and evidence-based policymaking","heading":"A.4.2 Example: Florida felon matching","text":"– details, see United States Civil Rights Commission report Voting Irregularities 2000 Presidential Election. Chapter 5. Reality List Maintenance. (now? 137/8).","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"federal-data-infrastructure","chapter":"A Data, models and evidence-based policymaking","heading":"A.4.3 Federal data infrastructure","text":"optimal arrangement administrative, survey, related statistical data series (reduce “barriers use already collected data”)\n“lots data kept isolated “data silos” ((ayres2007:135?)).\nmean?\nmajor Federal Statistical Agencies (FSAs) – used data sources? See Fedstats 13 primary federal statistical agencies\nCommission – gains evidence-building near-term linked challenge secure, private, confidential data access","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"solution-the-commission","chapter":"A Data, models and evidence-based policymaking","heading":"A.4.4 Solution: The Commission","text":"proposed “National Secure Data Service” integrate data purely statistical purposes (record linkages, privacy, transparency) specific ideas - data earnings likely vital piece personal information share (IRS, SSA)","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"confidentiality-and-privacy-concerns-surveillance","chapter":"A Data, models and evidence-based policymaking","heading":"A.5 Confidentiality and privacy concerns (surveillance)","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"data-privacy","chapter":"A Data, models and evidence-based policymaking","heading":"A.5.1 Data – privacy","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"privacy-confidentiality-issue-at-end-of-why-now-ubiquitous-surveillance","chapter":"A Data, models and evidence-based policymaking","heading":"A.5.2 Privacy, confidentiality issue at end of why now? – ubiquitous surveillance","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"models-1","chapter":"A Data, models and evidence-based policymaking","heading":"A.6 Models","text":"Research design – data-driven decision-making alternative driven responding anecdotes, complaints, good ideas.research design process – background, problem, theory, research question, hypothesis, operationalization, methods, evaluate data.One insight research tends problem-driven – searching solution. (Streets flooding often, crime , complaints park maintenance ).Another insight rely models – simple representation outcomes causes (appeals informally ideas covariance correlation).combination problem statement, theory, research question lead specific empirical claim can test – wider roads increased storm-water runoff.\ntest specific empirical questions – need data – measure individual components, can exploit existing data answer question?","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"why-are-people-bad-at-making-decisions","chapter":"A Data, models and evidence-based policymaking","heading":"A.6.1 Why are people bad at making decisions?","text":"Moneyball?Unbiased, unemotional, mechanical (neural network)Models - dispassionate determining marketing strategy works, policy works, partnership lasts? much movie money earn.human beings bad making predictions?much weight unusual eventstoo much weight unusual eventsselective acceptance attention disconfirming evidence.selective acceptance attention disconfirming evidence.overconfident predictive accuracyoverconfident predictive accuracy…get worse problems get complicated…get worse problems get complicated(Ayers, stark contrast equations – emotional attachment, specific information uncertainty)blend human machine? Human discretion end process wipes gains mechanical prediction Can’t judgement data coexist?Problem: judgment aided expert somewhere unaided expert mechanical prediction. conclusion machine/software control decision.Example: parole decisions. Marked decline expert decisions. RRASOR scores can trigger involuntary indefinite civil commitment (movie analogy Minority Report). Virginia statute mandates scores used – narrowing discretion criminal justice experts. even limited discretion can lead mistakes. thresholds proposed text – 10 percent automatic choices reversed, problem decisionmakers. , half overrides ultimately wrong, overrides frequent even 5 percent. Example: decline human component parole decisions. VA statue includes RRASOR tripwire – mandating involuntary confinement sexually violent criminal likely repeat offend","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"models-clinical-vs-statistical-predicions","chapter":"A Data, models and evidence-based policymaking","heading":"A.7 Models: clinical vs statistical predicions","text":"argument: Paul Meehl clinical versus statistical prediction. Meehl distinguishes informal non-mechanical judgments formal mechanical statistical – also labeled actuarial- predictions. latter require professional judgements.basic question whether exposure treatment produce benefit generate cost. example…. “four-variable regression equation . . . actuarial table tells criminal court judge particular delinquent probably commit another felony next 3 years case conference social worker says probably . . . plain fact [judge] act accordance incompatible predictions.”Meehl argues overwhelming majority cases, mechanical prediction superior. others reach similar conclusions. “long history quantifiable data form past experiences” …regression win.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"example-supreme-court-challenge","chapter":"A Data, models and evidence-based policymaking","heading":"A.7.0.1 Example: Supreme Court challenge","text":"egal scholars vs political scientists. Models retrospective – can predict future behavior? political science team used simple model - six factors.political science model outperformed legal scholars expert. expert judgement (human intervention) least two ways – determining elements model “coding” cases – capturing measures may somewhat subjective. (Ayers makes much fact discretion “channeled” narrower functions.)financial crisis? suggest quantitative understanding risks?","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"role-of-experts","chapter":"A Data, models and evidence-based policymaking","heading":"A.8 Role of experts","text":"Data modelsNeural network researcher role: raw dataHuman role – decide test collect data (data collect?)\nThreats professional autonomyHypothesize. X Z cause Y. statistical techniques can confirm existence “parameterize” size effect. experimental context (rather regression), researchers determine test – interventions treatments?","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"machine-learning-and-neural-networks","chapter":"A Data, models and evidence-based policymaking","heading":"A.8.1 Machine learning and neural networks","text":"tools Moneyball?Regression/neural networks – learning pastExperimental design- especially nimble low cost adaptationWhat neural network? (special type machine learning algorithm). Neural networks (large n, complex, multi-equation, nonlinear) ) vs regression (simple, single equation, lower n, linear)neural network looks like: inputs (observed), layers, outputs (observed train, predicted implemented).Neural network drawback: identifying impact single input predicted outcomeNeural network risk: -fitting? (hazard stepwise regression)Example: Epagogix “dispassionate weighting works” identify movie scripts generate large box-office revenues.Data-driven decision making requires results repeated decisions – many records success/failure – millions – train (large), validate (tune), test. now?","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-based-policymaking","chapter":"A Data, models and evidence-based policymaking","heading":"A.9 Evidence-based policymaking","text":"","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-building-history.","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.1 Evidence-building: history.","text":"mean use data “statistical purpose?” “Statistical activity statistical purpose summarizes information group rather single individual organization.” (Promise 11)History/evolution evidence-building – 1939 Roosevelt order coordinate “Federal statistical services,” , Add: Employment Act 1946, GPRA PART scores, 1960s Defense Great Society innovations data analysis (military / education, human services, health), 2000s CIPSEA – legal authorities###Community actors/experts – government, researchers, evaluators – (Promise 12) See COPAFS (Council Professional Associations Federal Statistics)","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-based-practice-in-other-contexts","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.2 Evidence-based practice in other contexts","text":"Evidence-based policymaking draws experience evidence-based practice medicine, education, management.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"medicine","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.3 Medicine","text":"FDA requires random clinical trials particular form evaluated particular way – strict protocol demonstrate safety efficacy. Advocates evidence-based medicine argue similar practices determine much wider set health care choices. One major innovation medical education involves idea continuous education – integrating latest data evidence.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"education","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.3.1 Education","text":"curriculum, hiring policies, discipline policies, practices produce best outcomes? Education relies extensively large statistical studies – using standardized exams. Critics point two major challenges – defining outcomes constitute good education , importantly, recognizing absence random assignment, parent, teacher community factors may overwhelm policy practice changes.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"management","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.3.2 Management","text":"Much management literature grounded personal experience cliché.Evidence-based management embraced variety methods – less wedded experimental design.Challenges public management. public managers – personnel (hire/fire), budgets, monitoring, contract administration, customer service. public managers need know survey customers, evaluate evidence-based claims, conduct research.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"the-legacy-of-performance-management","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.4 The legacy of performance management","text":"performance management movement different Moneyball Government? (Moneyball emphasizes statistical models experiments, performance management narrowly data measurement)Exploiting performance measures, example: Accurate timely intelligence, rapid deployment, effective tactics, relentless follow-– four principles behind CompStat, “major innovation American policing” led citywide implementation performance indicators Baltimore CitiStat spilled Rudi Guilliani’s vision nimble federal government. core, CompStat leadership strategy.Collecting data. search data driven two needs – analytical motivational – can analyze? can motivate people embrace improvements? availability data varies widely – police produce reports – collect data – routinely. agencies, performance measure obscure difficult expensive record. even police department may analyze data daily even weekly. organizations keep least two forms data – internal administrative external reporting data. start.Moneyball comparison might appropriate – baseball, purpose clear: win games rules specific, enforced rarely subject change. Neither describes government. craft meaningful metric “correlated organization’s purpose relatively easy obtain?” (Robert Behn describes hazard maximizing performance metrics capture organization’s mission – related problems corruption distortion – “Campbell effect”)Dials, Tin Openers Alarm bells. Performance indicators can prescriptive, descriptive, proscriptive (requiring immediate action stop behavior). Robert Behn claims performance metrics tin openers “opening can worms” might suggest performance deficit exists.Inputs, outputs, outcomes. Outcomes data suffer two problems – rarely timely (decades fruition) difficult attribute (factors can generate changes metric). “Output data timely controllable substitute” (Note another difference performance management Moneyball approaches)Performance measurement first step performance management.Learning asking questions. performing? working? deficits? innovations possible? core performance management starting point evidence-based management.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-based-policymaking-the-role-of-experiments","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.5 Evidence-based policymaking: the role of experiments","text":"One proposal Money ball follow lead business create low cost high impact interventions (like Google marketing strategies) – test change procedure random sample program participants,One example: specific successes Nurse Family Partnership (3 site randomized trials women diverted NFP onsite program offered opportunities screening treatment referral – among findings, Denver site, “3.6% nurse-visited children scored borderline clinical range internalizing problems, depression anxiety, versus 8.2% control-group children. difference statistically significant 0.10 level, 0.05 level.).”authors, evidence-based governance fiscal, economic moral imperative – use limited resources close achievement gap low high SES children.","code":""},{"path":"data-models-and-evidence-based-policymaking.html","id":"evidence-based-policymaking-solutions","chapter":"A Data, models and evidence-based policymaking","heading":"A.9.6 Evidence-based policymaking: solutions","text":"moneyball index. (Congress apparently adopted top tier evidence guidelines – see toptierevidence.org GAO report number GAO-10-30 entitled ‘Program Evaluation: Variety Rigorous Methods Can Help Identify Effective Interventions’ released November 23, 2009. Specific language 2007 HHS authorization: “ensure states use funds support [home visiting program] models shown, well-designed randomized controlled trials, produce sizeable, sustained effects important child outcomes, abuse neglect.) authors caution think continuum /- don’t simply sort programs keep kill, use data improve performance.\nAnother idea build clearinghouse share results (see evidencebasedprograms.org) Search “Postsecondary” http://ies.ed.gov/ncee/wwc/)Commission proposed funding Chief Evaluation Officers departments programs. consistent Moneyball prescription divert 1% program funding evaluation.suggests new objective graduate PA PSCI methods course: primary objective course enable (1) determine might make sense undertake form statistical analysis, (2) determine consultant might required evaluate consulting proposals, (3) evaluate understand products statistical consultant might produce.","code":""},{"path":"quasi--and-natural-experiments.html","id":"quasi--and-natural-experiments","chapter":"B Quasi- and natural experiments","heading":"B Quasi- and natural experiments","text":"short introduction quasi- natural experiments drawn Stock Watson (2015), Chapter 13, ” Experiments Quasi-Experiments”. also rely extensively R companion text, Hanck et al. (2021). useful resource background R work examples applications.review basics experimental design distinguish work observational approaches typically adopt Political Science. experimental approach came briefly looked Moneyball Government evaluation federal programs. Experimental designs leverage randomization. split subjects two groups - one treatment one control. Since groups identical every respect, except exposure treatment, can confident difference outcomes due treatment alone. Robust experimental work relies randomization, also blinds - double-blind experiment, neither researcher administering treatment subject receiving treatment knows whether working treatment placebo. cuts several potential biases: individuals given treatment may work harder succeed, physicans know using novel approach may wish see intervention work.can use true randomization contexts, experimental applications Political Science, particularly public policy, researcher aware receiving treatment participants may also recognize group .Stock Watson (2015) argue can improve understanding design observational research pay close attention experimental design practice. Specifically:randomized controlled experiment acts “benchmark observational approach” (can recognize limits ability make causal inferences)“results [randomized controlled] experiments can influential,” important understand limitations (implies considerations internal external validity)“external circumstances sometimes produce appears randomization; , external events, treatment individual occurs “” random, possibly conditional control variables. “” randomness produces quasi-experiment natural experiment, many methods developed analyzing randomized experiments can applied (modifications) quasi-experiments.”","code":""},{"path":"quasi--and-natural-experiments.html","id":"describing-treatment-effects","chapter":"B Quasi- and natural experiments","heading":"B.0.1 Describing treatment effects","text":"experiments, interested observing measuring outcome order estimate treatment effect. simply interested difference outcome variable Yi people receive treatment. examples equations , T represent treatment. average treatment effect simply difference expected value outcome people receive treatment expected value outcome people receive treatment.\\[\\text{Average treatment effect} =  E(Y_i\\vert T_i=1) -  E(Y_i\\vert T_i=0)\\]average causal effect can estimated using differences estimator, nothing OLS estimator simple regression model.\\[Y_i = \\beta_0 + \\beta_1 T_i + u_i \\ \\ , \\ \\ =1,\\dots,n \\]random assignment ensures \\(E(u_i|T_i)=0\\)OLS estimator regression model:\\[Y_i = \\beta_0 + \\beta_1 X_i + \\beta_2 W_{1i} + \\dots + \\beta_{1+r} W_{ri} + u_i \\ \\ , \\ \\ =1,\\dots,n\\]\nadditional regressors, \\(W_1,…,W_r\\), called differences estimator additional regressors. assumed treatment Ti randomly assigned independent pretreatment characteristic Wi. assumption called conditional mean independence implies:\\[E(u_i|T_i,W_i)=E(u_i|W_i)=0\\]conditional expectation error \\(u_i\\) given treatment indicator Ti pre-treatment characteristic Wi depend Ti.differences estimator additional regressors efficient differences estimator additional regressors explain variation Yi.","code":""},{"path":"quasi--and-natural-experiments.html","id":"analyzing-experimental-data-the-star-project","chapter":"B Quasi- and natural experiments","heading":"B.1 Analyzing experimental data: the STAR project","text":"Stock Watson (2015) discuss widely-cited example experimental design. description study adapted Hanck et al. (2021).Project Student-Teacher Achievement Ratio (STAR) large randomized controlled experiment designed determine small class size improves academic performance. Data collected four years 80 elementary schools Tennessee. 6400 students randomly assigned one three interventions: small class (13 17 students per teacher), regular class (22 25 students per teacher), regular--aide class (22 25 students full-time teacher’s aide). Teachers also randomly assigned classes taught. interventions initiated students entered school kindergarten continued third grade.year, students’ learning progress assessed using sum points scored math reading parts standardized test (Stanford Achievement Test). STAR data set part package AER - package includes 100 datasets used variety econometrics textbooks.estimate impact class size - average treatment effect - use differences estimator.\\[Y_i = \\beta_0 + \\beta_1 SmallClass_i + \\beta_2 RegAide_i + u_i\\]results briefly summarized . “stark” prefix indicates looking kindergarten classes. impact small class positive significant, presence teacher’s aide seem matter.also add regressors know associated academic performance improve efficiency estimate. Recall noisy models - lots error - decrease precision - higher standard errors coefficients. can identify add relevant variables, might change point estimate, standard error lower (efficient). model adds three covariates: years teacher experience, whether student boy girl, whether student qualifies free lunch (proxy low socioeconomic status).\\[Y_i = \\beta_0 + \\beta_1 SmallClass_i + \\beta_2 RegAide_i + \\beta_3 gender +\\beta_4 poor +\\beta_5experience + u_i\\]estimates precise results - small class size matters presence aide .","code":"\n# load the package AER and the STAR dataset\nlibrary(AER, quietly=TRUE)\ndata(STAR)fmk <- lm(I(readk + mathk) ~ stark, data = STAR)\ncoeftest(fmk, vcov = vcovHC, type= \"HC1\")\n\nt test of coefficients:\n\n                   Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept)       918.04289    1.63339 562.0473 < 2.2e-16 ***\nstarksmall         13.89899    2.45409   5.6636 1.554e-08 ***\nstarkregular+aide   0.31394    2.27098   0.1382    0.8901    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1fmke <- lm(I(readk + mathk) ~ stark+gender+lunchk+experiencek, data = STAR)\ncoeftest(fmke, vcov = vcovHC, type= \"HC1\")\n\nt test of coefficients:\n\n                   Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept)       919.09404    2.49639 368.1693 < 2.2e-16 ***\nstarksmall         14.14339    2.35845   5.9969 2.134e-09 ***\nstarkregular+aide   0.63946    2.16104   0.2959    0.7673    \ngenderfemale       13.80443    1.85516   7.4411 1.146e-13 ***\nlunchkfree        -39.11061    1.85864 -21.0426 < 2.2e-16 ***\nexperiencek         1.18034    0.16641   7.0931 1.469e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"quasi--and-natural-experiments.html","id":"validity-in-experiments","chapter":"B Quasi- and natural experiments","heading":"B.2 Validity in experiments","text":"Ensuring inferences valid central concern experimental design. distinguish internal vaiidity external validity.statistical analysis internal validity statistical inference made causal effects valid population - Type Type II error.analysis said external validity inferences conclusion valid studies’ population can generalized populations settings.two conditions internal validity exist:estimator causal effect, coefficient(s) interest, unbiased consistent.estimator causal effect, coefficient(s) interest, unbiased consistent.statistical inference valid, , hypothesis tests permit us distinguish accurately significant insignificant effects.statistical inference valid, , hypothesis tests permit us distinguish accurately significant insignificant effects.External validity might jeopardized:differences population studied population interest.differences population studied population interest.differences settings considered populations, e.g., legal framework time investigation.differences settings considered populations, e.g., legal framework time investigation.consider mean specifically context experiments.list descriptions drawn Stock Watson (2015).","code":""},{"path":"quasi--and-natural-experiments.html","id":"internal-validity-in-experimental-settings","chapter":"B Quasi- and natural experiments","heading":"B.2.1 Internal validity in experimental settings","text":"","code":""},{"path":"quasi--and-natural-experiments.html","id":"failure-to-randomize","chapter":"B Quasi- and natural experiments","heading":"B.2.1.1 Failure to Randomize","text":"subjects randomly assigned treatment group, outcomes contaminated effect subjects’ individual characteristics preferences possible obtain unbiased estimate treatment effect. One can test nonrandom assignment using significance test (F-Test) coefficients regression model:\\[T_i = \\beta_0 + \\beta_1 W_{1i} + \\dots +\\beta_r W_{ri} + u_i \\ \\ , \\ \\ =1,\\dots,n.\\]\n\\(W_i\\) observable characteristics subject. assignment random, none covariates related selection treatment.","code":""},{"path":"quasi--and-natural-experiments.html","id":"failure-to-follow-the-treatment-protocol","chapter":"B Quasi- and natural experiments","heading":"B.2.1.2 Failure to Follow the Treatment Protocol","text":"subjects follow treatment protocol, .e., subjects treatment group manage avoid receiving treatment /subjects control group manage receive treatment (partial compliance), correlation \\(T_i\\) und \\(u_i\\) OLS estimator average treatment effect biased.","code":""},{"path":"quasi--and-natural-experiments.html","id":"attrition","chapter":"B Quasi- and natural experiments","heading":"B.2.1.3 Attrition","text":"Attrition may result non-randomly selected sample. subjects systematically drop study assigned control treatment group (systematic means reason dropout related treatment) correlation \\(T_i\\) und \\(u_i\\), biasing OLS estimate treatment effect.","code":""},{"path":"quasi--and-natural-experiments.html","id":"experimental-effects","chapter":"B Quasi- and natural experiments","heading":"B.2.1.4 Experimental Effects","text":"human subjects treatment group /control group know experiment, might adapt behavior way prevents unbiased estimation treatment effect. widely known Hawthorne effect.","code":""},{"path":"quasi--and-natural-experiments.html","id":"small-sample-sizes","chapter":"B Quasi- and natural experiments","heading":"B.2.1.5 Small Sample Sizes","text":"Small sample sizes lead imprecise estimation coefficients thus imply imprecise estimation causal effect. Furthermore, confidence intervals hypothesis test may result Type II errors sample size small.","code":""},{"path":"quasi--and-natural-experiments.html","id":"threats-to-external-validity-in-experimental-settings","chapter":"B Quasi- and natural experiments","heading":"B.2.2 Threats to external validity in experimental settings","text":"","code":""},{"path":"quasi--and-natural-experiments.html","id":"nonrepresentative-sample","chapter":"B Quasi- and natural experiments","heading":"B.2.2.1 Nonrepresentative Sample","text":"population studied population interest sufficiently similar, justification generalizing results. experimental situation different actual experience lessons experiment won’t translate?","code":""},{"path":"quasi--and-natural-experiments.html","id":"nonrepresentative-program-or-policy","chapter":"B Quasi- and natural experiments","heading":"B.2.2.2 Nonrepresentative Program or Policy","text":"program policy population studied differs considerably program () applied population(s) interest, results generalized. example, small-scale program low funding might different effects widely available scaled-program actually implemented. factors like duration extent monitoring considered .","code":""},{"path":"quasi--and-natural-experiments.html","id":"general-equilibrium-effects","chapter":"B Quasi- and natural experiments","heading":"B.2.2.3 General Equilibrium Effects","text":"external environmental conditions kept constant internally valid program implemented broadly, external validity may doubtful.","code":""},{"path":"quasi--and-natural-experiments.html","id":"so-what-is-a-quasi-experiment","chapter":"B Quasi- and natural experiments","heading":"B.3 So what is a quasi-experiment?","text":"quasi-experiment situation nature (someone else) assigns people treatment control groups way approximates random selection. Assignment “-” random.experiment, need observations individual subjects prior exposure treatment. least two observations per subject measure groups periods. clarify works considering two techniques analysis quasi-experimental data - difference--difference estimator regression discontinuity designs.","code":""},{"path":"quasi--and-natural-experiments.html","id":"difference-in-difference-estimators.","chapter":"B Quasi- and natural experiments","heading":"B.3.1 Difference-in difference estimators.","text":"example (entire section) almost directly Hanck et al. (2021).quasi-experiments source “” randomness treatment assignment can often entirely prevent systematic differences control treatment groups. problem encountered Card & Krueger (1994) use geography “” random treatment assignment study effect employment fast-food restaurants caused increase state minimum wage New Jersey 1992. idea use fact increase minimum wage applied employees New Jersey (treatment group) living neighboring Pennsylvania (control group).quite conceivable wage hike correlated determinants employment. However, still might state-specific differences thus differences control treatment group. render differences estimator biased inconsistent. Card & Krueger (1994) solved using estimator: collected data February 1992 (treatment) November 1992 (treatment) restaurants estimated effect wage hike analyzing differences differences employment New Jersey Pennsylvania increase.figure sketches logic. outcome Y acknowledge treatment control group may start different levels focus change difference time.estimator can also written regression notation: \\(\\hat\\beta^{}\\) OLS estimator \\(\\beta_1\\) \\[\\Delta Y_i = \\beta_0 + \\beta_1 T_i + u_i\\]","code":""},{"path":"quasi--and-natural-experiments.html","id":"an-example-with-artificial-data","chapter":"B Quasi- and natural experiments","heading":"B.3.2 An example with artificial data","text":"code chunk creates artificial dataset treatment control group. control group mean 6.0 prior treatment, treatment group mean 8.0. treatment, control group mean increases 7.0, treatment group means increases fo 13.0. treatment effect introduced (\\(\\hat\\beta^{}\\)) exactly 4.0.can estimate treatment effect couple different ways: manually, estimator OLS using dummy variables periods treatment.","code":"# Method 1: manually\n# compute the DID estimator for the treatment effect 'by hand'\nmean(y_post[TDummy == 1]) - mean(y_pre[TDummy == 1]) - \n(mean(y_post[TDummy == 0]) - mean(y_pre[TDummy == 0]))\n[1] 3.808441# Method 1: DID\n# compute the DID estimator using a linear model\nlm(I(y_post - y_pre) ~ TDummy)\n\nCall:\nlm(formula = I(y_post - y_pre) ~ TDummy)\n\nCoefficients:\n(Intercept)       TDummy  \n      1.987        3.808  # Method 1: OLS with dummies\n# prepare data for DID regression using the interaction term \nd <- data.frame(\"Y\" = c(y_pre,y_post),\n                \"Treatment\" = TDummy, \n                \"Period\" = c(rep(\"1\", n), rep(\"2\", n)))\n\n# estimate the model\nlm(Y ~ Treatment * Period, data = d)\n\nCall:\nlm(formula = Y ~ Treatment * Period, data = d)\n\nCoefficients:\n      (Intercept)          Treatment            Period2  Treatment:Period2  \n           6.1204             0.8969             1.9870             3.8084  "},{"path":"quasi--and-natural-experiments.html","id":"regression-discontinuity-designs","chapter":"B Quasi- and natural experiments","heading":"B.3.3 Regression discontinuity designs","text":"Regression discontinuity exploits empirical situations subjects selected treatment based threshold cutoff. canonical example compulsory summer school students performing poorly. idea observe group students end school year require summer school students grades certain cut-, say 2.0. end next school year, look grades group students. summer school improve future performance students required attend?Discontinuity designs exploit fact students close threshold quite similar. group students 2.1 GPA probably quite similar group students 1.9 GPA. arbitrary administrative determination 2.0 threshold artificially splits group, way approximates random assignment.discussed experimental design earlier term, brought idea exploiting wait-lists services. large group subjects seeking small number treatments, randomly assign subjects treatment (lottery) control set experiment measure impact treatment. raises host problems - ethics denying treatment subjects greatest need, possible exposure harms treatment work. someone use criteria figure gets service - maybe geography - lives close center facility treatment - maybe kind need - based income, instance. kind administrative determination form sets possibility using discontinuity design.walk technical options, create artificial data set. first instance, everyone 2.0 gets treatment one 2.0 (strict cut-). second instance, small proportion eligible students attend small proportion students 2.0 opt attend (cut-fuzzy).true treatment effect 0.50 units. figure shows predicted values group, break regression line impact treatment.use two approaches estimate size effect.","code":"\n# generate sample data\n# The mean GPA is the same in year and year 2\nmu <- c(2.75, 2.75)\n\n# first and last term is variance - middle terms are covariance \n# since the covariance is a larger proportion of the variance, performance is correlated.  I adjusted the variance to try to keep the numbers between 0 and 4\nsigma <- matrix(c(0.4, 0.3, 0.3, 0.4), ncol = 2)\n\n#I pulled 2000 observations from a multivariate normal distribution\n#set.seed() makes sure that the random draw can be replicated.\nset.seed(1266)\nd <- as.data.frame(mvrnorm(2000, mu, sigma))\ncolnames(d) <- c(\"W\", \"Y\")\n\n# introduce fuzziness\n# Probabilty of treatment is 95% if you are under 2.0 and 1% if you are over 2.0\nd$treatProb <- ifelse(d$W < 2.0, 0.95, 0.01)\n# Also have a strict treatment.\nd$strict<- ifelse(d$W< 2.0, 1.0, 0.0)\nd$fuzz <- sapply(X = d$treatProb, FUN = function(x) rbinom(1, 1, prob = x))\n\n# treatment effects = 0.50\nd$Y_strict <- d$Y+  d$strict*0.50\nd$Y_fuzzy  <- d$Y + d$fuzz*0.50\n\n# generate a colored plot of treatment and control group\nplot(d$W, d$Y_fuzzy,\n     col = c(\"steelblue\", \"darkred\")[factor(d$fuzz)], \n     pch= 20, \n     cex = 0.5,\n     xlim = c(0, 4),\n     ylim = c(-0.5, 8),\n     xlab = \"W [treatment is fuzzy]\",\n     ylab = \"Y\")\n\n# add a dashed vertical line at cutoff\nabline(v = 2, lty = 2)\n## now predict 50 values spread evenly on range (-20, 0]\nfit<-lm(Y_strict~W+strict, data=d)\n\nnewdat1 <- data.frame(W = seq(0, 2, length = 50))\nnewdat1$strict<-1\nnewdat1 <- within(newdat1, ypred1 <- predict(fit, newdat1))\nnewdat0 <- data.frame(W = seq(2, 5, length = 50))\nnewdat0$strict<-0\nnewdat0 <- within(newdat0, ypred0 <- predict(fit, newdat0))\n\nlines(ypred0 ~ W, data = newdat0, col = \"blue\", lwd = 2)\nlines(ypred1 ~ W, data = newdat1, col = \"red\", lwd = 2)"},{"path":"quasi--and-natural-experiments.html","id":"estimating-the-size-of-the-effect---strict-assignment","chapter":"B Quasi- and natural experiments","heading":"B.3.3.1 Estimating the size of the effect - strict assignment","text":"couple alternatives - can use entire sample, apply OLS, permit intercept vary treatment assignment. spirit figure . just look observations close cut-. accomplish use RDD package. estimates strict cutoff .output , LATE indicates Local Area Treatment Effect” - Bandwidth indicates many observations total cutoff used calculation. estimate 672 observations -0.41.close known value 0.50. sign negative since group cutoff get treatment, lower outcome.OLS gives us similar results, even closer true use data. artificial data, data-generating process observations, observations efficient accurate. circumstances may make sense just concentrate observations close threshold. Using approaches lets compare implications larger smaller samples.","code":"# Estimate the strict RDD using a subset of observations\nsummary(RDestimate(Y_strict~W, data=d, cutpoint=2))\n\nCall:\nRDestimate(formula = Y_strict ~ W, data = d, cutpoint = 2)\n\nType:\nsharp \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(>|z|) \nLATE       0.5122      672          -0.4162   0.08021     -5.189   2.120e-07\nHalf-BW    0.2561      338          -0.3706   0.11152     -3.323   8.905e-04\nDouble-BW  1.0243     1332          -0.4693   0.05887     -7.971   1.575e-15\n              \nLATE       ***\nHalf-BW    ***\nDouble-BW  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F      Num. DoF  Denom. DoF  p        \nLATE       20.12  3          668        3.409e-12\nHalf-BW    13.29  3          334        6.439e-08\nDouble-BW  67.43  3         1328        0.000e+00# Estimate the effect using all observations\nsummary(lm(Y_strict~W+strict, data=d))\n\nCall:\nlm(formula = Y_strict ~ W + strict, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28587 -0.28317  0.00286  0.29226  1.65117 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.72096    0.05443   13.25   <2e-16 ***\nW            0.73954    0.01847   40.03   <2e-16 ***\nstrict       0.47802    0.03607   13.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4192 on 1997 degrees of freedom\nMultiple R-squared:  0.476, Adjusted R-squared:  0.4754 \nF-statistic: 906.9 on 2 and 1997 DF,  p-value: < 2.2e-16"},{"path":"quasi--and-natural-experiments.html","id":"fuzzy-assignment.","chapter":"B Quasi- and natural experiments","heading":"B.3.3.2 Fuzzy assignment.","text":"can use strategy fuzzy assignment. fuzzy assignment, introduce treatment probability variable directly. can also OLS, two-stage approach preferred just FYI.case, know treatment effect 0.5. Using strict cut-, estimates effects range 0.42 0.48. Using fuzzy cut-, estimates 0.43 0.48. Note implication - fuzzy case, knew administrative cut-(2.0) didn’t know exactly sure students attended, nevertheless able estimate effect.","code":"\n# What if treatment is fuzzy?\n# The default in RDestimate is to treat greater than W as treatment so treatprob needs to be .05 and .99 in order for signs to be comparable to above\nd$treatProb2<-1-d$treatProb\nsummary(RDestimate(Y_fuzzy~W+treatProb2, data=d, cutpoint=2))\n\nCall:\nRDestimate(formula = Y_fuzzy ~ W + treatProb2, data = d, cutpoint = 2)\n\nType:\nfuzzy \n\nEstimates:\n           Bandwidth  Observations  Estimate  Std. Error  z value  Pr(>|z|) \nLATE       0.5176      683          -0.4311   0.08595     -5.016   5.276e-07\nHalf-BW    0.2588      338          -0.3826   0.11917     -3.211   1.325e-03\nDouble-BW  1.0352     1344          -0.4799   0.06328     -7.584   3.351e-14\n              \nLATE       ***\nHalf-BW    ** \nDouble-BW  ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nF-statistics:\n           F      Num. DoF  Denom. DoF  p        \nLATE       17.94  3          679        3.240e-11\nHalf-BW    11.80  3          334        2.300e-07\nDouble-BW  68.60  3         1340        2.864e-41\nsummary(lm(Y_fuzzy~W+treatProb, data=d))\n\nCall:\nlm(formula = Y_fuzzy ~ W + treatProb, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.29321 -0.28232  0.00369  0.28859  1.66531 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.71966    0.05501   13.08   <2e-16 ***\nW            0.74097    0.01859   39.87   <2e-16 ***\ntreatProb    0.48721    0.03861   12.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4217 on 1997 degrees of freedom\nMultiple R-squared:  0.4769,    Adjusted R-squared:  0.4764 \nF-statistic: 910.3 on 2 and 1997 DF,  p-value: < 2.2e-16"},{"path":"quasi--and-natural-experiments.html","id":"a-recent-application-in-gender-and-politics","chapter":"B Quasi- and natural experiments","heading":"B.3.4 A recent application in gender and politics","text":"Basques Campes (2017)recent paper, provide comprehensive analysis impact candidate gender quotas using unique quasi-experimental evidence offered introduction Spanish local elections.context closed lists proportional representation, quota prescribes least 40% candidates list female (male). provision applies entire list, also every block five candidates.quota first implemented 2007 municipalities 5,000 inhabitants, extended 2011 municipalities 3,000 inhabitants.take advantage existence population cut-offs implement regression discontinuity design compare municipalities slightly cut-offs three rounds elections (2007, 2011, 2015).\ncomparing municipalities practically identical terms population (optimal bandwidth around 1,000 inhabitants), empirical strategy robust existence relevant factors vary smoothly threshold. analysis provides information impact quotas important context: relatively small municipalities, likely rural areas, persistence gender stereotypes tends stronger women traditionally excluded decision-making larger extent. Interestingly, group municipalities many countries decided exclude quota (e.g. France, Italy, Spain).sum, study confirms quotas electoral lists increase number women elected.However, ten years quotas place Spain, significantly affected probability women reach leadership positions type policies implemented, least small municipalities object study. time, fear quotas decrease quality politicians confirmed: quota candidates similar educational attainment attract similar number votes compared candidates replace.Details: https://voxeu.org/article/electoral-gender-quotas-fail-empower-women","code":""},{"path":"quasi--and-natural-experiments.html","id":"why-quasi-experiments-might-be-superior","chapter":"B Quasi- and natural experiments","heading":"B.4 Why quasi-experiments might be superior","text":"Green (2010) makes strong case widespread use quasi-experiments. randomized controlled experiments generally superior types casual inference, argues think continuum ranging form poorly designed quasi-experiments (least preferred) strongly designed controlled experiments (preferred). middle range strongly designed quasi-experiments poorly-designed controlled experiments. category, case made using quasi-experiments.Think threats validity:quasi-experiment, one - researcher subject - knows time part experiment. higher confidence internal validity.quasi-experiment, constrained access subjects practical ethical constraints. observing post-hoc.making choices access treatmentyou exposing subjects potential harmSince design can applied broader range settings contexts, can higher confidence external validity.since natural administrative choices set quasi-experiment may rare, external validity may challenge.insights might explain quasi-experiments particularly regression discontinuity design becoming widely used popular: inexpensive, powerful, practical.","code":""},{"path":"matching.html","id":"matching","chapter":"C Matching","heading":"C Matching","text":"","code":""},{"path":"matching.html","id":"what-is-matching","chapter":"C Matching","heading":"C.1 What is matching?","text":"Matching approach estimating treatment effects causal effects trimming pruning data create treatment control groups similar identical.know observational data (distinct experimental data) involves non-random assignment control treatment groups, variable related whether treatment group related level outcome variable. case neglect control variable, estimate impact treatment effect biased. Matching involves making sure treatment control groups similar levels various observed control variables - ensure groups similar possible order isolate effect treatment variable key predictor interested .regression discontinuity designs followed similar logic, focusing subset observations near arbitrary threshold. Matching doesn’t require quasi- natural experiment design. can applied observational data.","code":""},{"path":"matching.html","id":"describing-treatment-effects-1","chapter":"C Matching","heading":"C.2 Describing treatment effects","text":"use language conventions describe data outcomes used last week. experiments, interested observing measuring outcome order estimate treatment effect. simply interested difference outcome variable Yi people receive treatment. examples equations , Ti represent treatment. average treatment effect simply difference expected value outcome people receive treatment expected value outcome people receive treatment.\\[\\text{Average Treatment Effect} =  E(Y_i\\vert T_i=1) -  E(Y_i\\vert T_i=0)\\]context, work matching carefully distinguishes pre-treatment differences (measured unmeasured) post-treatment differences. idea need take account control relevant pre-treatment differences calculate average treatment effect.case last week, consider classical randomized experiment ideal type. context, individual observations randomly sampled population, assignment treatment control groups random, sample size large. means avoid selection bias (sample deviating population) omitted variable bias (since nothing covary treatment assignment).Observational research can fail meet criteria - may selection bias (function selected cases observe measure), may omitted crucial variables Xi influence Ti Yi may relatively opportunities observe (small n).top three features , also imposing assumptions - none pre-treatment covariates function T, units independent (spatial temporal correlation) treatment intensity across units.experimental observational contexts, can attempt estimate average treatment effect including treatment indicator pre-treatment covariates one model.looks like differences estimator additional regressors, introduced chapter quasi-experiemets, recognizing treatment Ti randomly assigned independent pre-treatment characteristic Xi imposing linear functional form.\\[Y_i = f(T_i, X_i)\\]","code":""},{"path":"matching.html","id":"why-use-matching","chapter":"C Matching","heading":"C.3 Why use matching?","text":"Ho et al. (2007) make case matching can dramatically reduce label model dependence. idea try estimate effects T Y make many choices model specification (covariates?), functional form (OLS count model?), variables transformed (logs squares interaction terms). choice imply impossibly large number permutations, typically report publish handful alternatives.pernicious dynamic work : researchers may bias results picking specification returns estimates confirm expectations. King Nielsen (2019) explain: “tendency imperceptibly favor one’s hypotheses, swayed unanticipated directions even without strong priors, unavoidable. People easy access mental processes little self-evident information use avoid problem … make matters worse, subject matter experts overestimate ability control personal biases nonexperts, prominent experts overconfident … Moreover, training researchers make better qualitative decisions based empirical estimates exists little information choose among scientifically unlikely reduce bias even taught social-psychological results.” (cites omitted)example, Ho et al. (2007) construct dataset three variables - outcome (Yi, outcome), dummy treatment variable (Ti) single covariate Xi, education) related treatment. estimate treatment effect using two approaches, first OLS - linear model - quadratic, nonlinear model (adding squared \\(X\\) term). difference intercepts case estimate treatment effect. can see two functional forms - quadratic linear - applied entire set data - yield two different estimates effect treatment - treatment effect positive use linear model negative use quadratic fit.can probably guess source problem - treated individuals middle distribution X, control subjects extremes. observing effect treatment narrow range \\(X\\). labeled common support problem one simple fix exclude observations control group X lower minimum X observed treatment group X higher maximum X observed treatment group.","code":""},{"path":"matching.html","id":"how-matching-works","chapter":"C Matching","heading":"C.4 How matching works","text":"Matching means trimming pruning data balance treatment control group. can see treatment control much different distribution key covariate . use MatchIt package generate 1:1 match treated observation one control.case, many controls treated observations, 1:1 matching means excluding many control observations.Recall one prescriptions data collection improve causal inference maximize variation X, making trade-reducing bias reducing efficiency. observations means efficiency, fewer observations means balance control d treatment groups.output reports results matching, comparing values X treatment control groups data matched data. can see control group mean X 12.59, much lower treatment group, matching reduced difference 54%, elimnating 22 observations match treatment group. can summarize improvement simple figure.figure reproduces estimate treatment effect using matched data, excluding observations identified red.estimates, using either functional formal, suggest treatment effect. , case reduced variation X sample, improved estimate.","code":"# Identify a match object and match using nearest neighbor method without propensity scoring - this is a different match method from the original article\ntemp <- matchit(t ~ x, data=dta, method=\"nearest\", distance=\"mahalanobis\")\nmatched <- match.data(temp)\nsummary(temp)\n\nCall:\nmatchit(formula = t ~ x, data = dta, method = \"nearest\", distance = \"mahalanobis\")\n\nSummary of Balance for All Data:\n  Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nx       15.5392       12.5973          1.0206     0.0927    0.2077   0.4161\n\n\nSummary of Balance for Matched Data:\n  Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nx       15.5392       14.1854          0.4696     0.3878    0.1182   0.3636\n  Std. Pair Dist.\nx          0.8685\n\nPercent Balance Improvement:\n  Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\nx              54       60.2      43.1     12.6\n\nSample Sizes:\n          Control Treated\nAll            39      22\nMatched        22      22\nUnmatched      17       0\nDiscarded       0       0\nframe()\nplot(summary(temp), threshold=NA, xlim=c(0, 1.5))\n# Linear model - matched data\nlm.m1 <- lm(y ~ t+x, data=matched)\n\n\n# Quadratic model - matched data\nlm.m2 <- lm(y ~ t+x+I(x^2), data=matched)\n\nplot(matched$x[matched$t==1], matched$y[matched$t==1],\n     pch=\"T\", xlab=\"Education\", ylab=\"Outcome\", xlim=range(dta$x),\n     ylim=range(dta$y), cex=0.8, main=\"After Matching\")\npoints(matched$x[matched$t==0], matched$y[matched$t==0], pch=\"C\", cex=0.8)\npoints(dta$x[temp$weights==0 & dta$t==0],\n       dta$y[temp$weights==0 & dta$t==0],\n       pch=\"C\", col=\"red\", cex=0.8)\nlines(plot.pts2, lm.m1$coef[1] + lm.m1$coef[2] + lm.m1$coef[3]*plot.pts2, lty=1, lwd=1.5)\nlines(plot.pts2, lm.m1$coef[1] + lm.m1$coef[3]*plot.pts2, lty=1, col=\"darkgrey\", lwd=1.5)\nlines(plot.pts2, lm.m2$coef[1] + lm.m2$coef[2] +\n      lm.m2$coef[3]*plot.pts2 + lm.m2$coef[4]*plot.pts2^2,\n      lty=2, lwd=1.5)\nlines(plot.pts2, lm.m2$coef[1] + lm.m2$coef[3]*plot.pts2 +\n      lm.m2$coef[4]*plot.pts2^2, lty=2, col=\"darkgrey\", lwd=1.5)\nlegend(5, 4.75, lty=c(1, 1, 2, 2), col=c(1, 8, 1, 8), lwd=1.5,\n       legend=c(\"Linear Model, Treated Group\",\n         \"Linear Model, Control Group\",\n         \"Quadratic Model, Treated Group\",\n         \"Quadratic Model, Control Group\"), cex=0.5)"},{"path":"matching.html","id":"the-method-of-matching-matters","chapter":"C Matching","heading":"C.4.1 The method of matching matters","text":"example , simply used one covariate, (often ) situations treatment group control group different several ways - imagine case control group higher education higher income likely live urban areas .order match situations MatchIt employs logistic regression calculate propensity score - probability part treatment group based covariates. can match observations similar levels propensity score - implying similar levels covariates.Recent work, King Nielsen (2019), suggests propensity scoring can result cases ideal matches - like pairing two observations similar propensity score one person high income low education low income high education. calculate propensity score different subjects. default MatchIt use prospensity scoring approach, can use , preferred approaches package.three options:matchit(y~x1+x2, data=data)matchit(y~x1+x2, data=data, method=“nearest,” distance=“mahalanobis”)matchit(y~x1+x2, data=data, method=“genetic”)first method nearest neighbor based propensity scores, default widely used literature. Mahalanobis distance matching method measures covariate imbalance individually, rather aggregating propensity score. genetic matching method borrows . easy , test sensitivity results matching method - approach prune different observations.","code":""},{"path":"matching.html","id":"two-applications","chapter":"C Matching","heading":"C.5 Two applications","text":"","code":""},{"path":"matching.html","id":"union-membership-and-campaign-activity","chapter":"C Matching","heading":"C.5.1 Union membership and campaign activity","text":"continue simple example, consider model turnout used early term - predicted probability turnout based age, education, income, gender race. effect living union household? might expect member union household mobilized engaged likely vote. test logistic regression, controlling variety controls. echo code chunk reads ANES data, visible RMD file.code chunk selects 1992 presidential election year, creates subset just variables interested drops observation missing values variable. drop_na() function requires tidyverse package.table reports results four regressions - logit - different control variables added.effect union positive case, size estimated effect varies across model specifications. need convert logistic proportion probability see exactly size effects implied.code chunk creates matched data set, pairing relatively rare union household non-union household similar levels covariates. summary matchit object - time presented figure - compares treatment control groups - look mean income treatment control groups data - union households much higher. Compare difference difference treatment control groups matched sample. Matching dramatically reduces difference every characteristic except minority (see “Percent Balance Improvement”).used propensity scoring approach Mahalanobis\ndistance approach can compare different matching methods improve balance . case, think propsenity matching superior, report results.can compare estimates logistic regression using matched data. can see estimate effect union household now entirely robust inclusion exclusion covariates. nearly much model dependence. (bit sleight hand researchers picked - estimates now dependent covariates use match matching method, different set variables used propensity score result different set matched pairs.) recent piece suggests ways pick across alternative matching approaches (King, Lucas, Nielsen (2017))","code":"# filter to select 1992\ntemp1 <- anes %>% filter(VCF0004==1992)  %>% select(turnout, age, education, income, union, minority, female) %>% drop_na()\n\n\n# estimate a logistic regission  of turnout \n\nmodel1<-glm(turnout~union, data=temp1, family=binomial(link = \"logit\"))\n\nmodel2<-glm(turnout~union+education, data=temp1, family=binomial(link = \"logit\"))\n\nmodel3<-glm(turnout~union+education+age+income, data=temp1, family=binomial(link = \"logit\"))\n\nmodel4<-glm(turnout~union+education+age+income+minority+female, data=temp1, family=binomial(link = \"logit\"))\n\nstargazer(model1, model2, model3, model4, style=\"qje\", type=\"text\" , omit=\"factor\" , dep.var.labels = c(\"Turnout\", \"\\n\"), digits=3)\n\n===========================================================\n                                   Turnout                 \n                      (1)        (2)       (3)       (4)   \n-----------------------------------------------------------\nunion              0.772***   0.742***  0.526***  0.540*** \n                    (0.165)    (0.171)   (0.179)   (0.179) \n                                                           \neducation                     0.482***  0.532***  0.529*** \n                               (0.035)   (0.044)   (0.044) \n                                                           \nage                                     0.040***  0.039*** \n                                         (0.004)   (0.004) \n                                                           \nincome                                  0.380***  0.383*** \n                                         (0.057)   (0.059) \n                                                           \nminority                                           -0.120  \n                                                   (0.135) \n                                                           \nfemale                                              0.174  \n                                                   (0.119) \n                                                           \nConstant           1.034***   -0.732*** -3.638*** -3.678***\n                    (0.055)    (0.132)   (0.281)   (0.310) \n                                                           \nN                    2,025      2,025     2,025     2,025  \nLog Likelihood    -1,108.069  -998.532  -912.627  -911.201 \nAkaike Inf. Crit.  2,220.137  2,003.064 1,835.254 1,836.402\n===========================================================\nNotes:               ***Significant at the 1 percent level.\n                      **Significant at the 5 percent level.\n                      *Significant at the 10 percent level.\nmatched <- matchit(union ~education+age+income+union+minority+female, na.action=na.exclude, data=temp1)\ntemp2 <- match.data(matched)\nplot(summary(matched), xlim=c(-0.1, 0.7), threshold=FALSE)\n\n\nmatched_m <- matchit(union ~education+age+income+union+minority+female, na.action=na.exclude, data=temp1, distance=\"mahalanobis\")\ntemp3 <- match.data(matched_m)\nplot(summary(matched_m), xlim=c(-0.3, 0.9), threshold=FALSE)model1<-glm(turnout~union, data=temp2, family=binomial(link = \"logit\"))\n\nmodel2<-glm(turnout~union+education, data=temp2, family=binomial(link = \"logit\"))\n\nmodel3<-glm(turnout~union+education+age+income, data=temp2, family=binomial(link = \"logit\"))\n\nmodel4<-glm(turnout~union+education+age+income+minority+female, data=temp2, family=binomial(link = \"logit\"))\n\nstargazer(model1, model2, model3, model4, style=\"qje\", type=\"text\" , omit=\"factor\" , dep.var.labels = c(\"Turnout\", \"\\n\"), digits=3)\n\n===========================================================\n                                   Turnout                 \n                     (1)        (2)        (3)       (4)   \n-----------------------------------------------------------\nunion              0.456**    0.446**    0.462**   0.479** \n                   (0.206)    (0.212)    (0.221)   (0.223) \n                                                           \neducation                     0.455***  0.499***  0.489*** \n                              (0.075)    (0.086)   (0.086) \n                                                           \nage                                     0.047***  0.046*** \n                                         (0.008)   (0.008) \n                                                           \nincome                                  0.583***  0.597*** \n                                         (0.131)   (0.134) \n                                                           \nminority                                           -0.253  \n                                                   (0.250) \n                                                           \nfemale                                              0.334  \n                                                   (0.226) \n                                                           \nConstant           1.350***    -0.344   -4.376*** -4.450***\n                   (0.134)    (0.290)    (0.683)   (0.724) \n                                                           \nN                    680        680        680       680   \nLog Likelihood     -311.285   -289.659  -264.559  -262.960 \nAkaike Inf. Crit.  626.569    585.318    539.119   539.920 \n===========================================================\nNotes:               ***Significant at the 1 percent level.\n                      **Significant at the 5 percent level.\n                      *Significant at the 10 percent level."},{"path":"matching.html","id":"fda-drug-approval-times","chapter":"C Matching","heading":"C.5.2 FDA drug approval times","text":"One works replicated matching piece, Carpenter (2002), bureaucratic politics literature. Carpenter uses duration model form looked weeks ago investigate determinants time takes FDA approve new drug. dataset consists drugs waiting approval predictors include features disease drug intended treat, intensity interest group activity related disease, media coverage, well political environment (central expectation Republican influence speed drug approval times).Carpenter ultimately finds disease characteristics, media interest group activity explain delays action, rather Republican Democratic control key parts Congress. fact, finds (gives cursory attention ) Democratic majority Senate associated faster approval times. results summarized table , piece. Notice coefficient Democratic Senate Majority negative significant, shorter approval time.bold-faced effects consistent substantively important: incidence severity disease, well media coverage number groups (quadratic).Ho et al. (2007) demonstrate changes modeling choices generate substantial uncertainty Senate Democratic Majority effect use entire dataset. , employing matching, able reduce model dependence substantially. impact matching summarized figure .can replicate original estimate test effect alternative matching strategies see can get balanced set data approach using propensity scores article. figures suggest (judgment call) second method results balanced dataset.","code":"\n# Slimmed down just to show how matching works\n\n## data\ndata <- read.table(\"data/FDA-Carpenter.csv.txt\", header=T, sep=\",\")\n\n## democratic senate as the treatment variable\ndata$treat <- data$demsnmaj\n\n## rescaling\ndata$hospdisc <- data$hospdisc/100000\ndata$natreg <- data$natreg/100\ndata$stafcder <- data$stafcder/100\ndata$prevgenx <- data$prevgenx/100\ndata$hhosleng <- data$hhosleng/10\ndata$condavg3 <- data$condavg3/10\ndata$orderent <- data$orderent/10\ndata$vandavg3 <- data$vandavg3/10\ndata$wpnoavg3 <- data$wpnoavg3/100\n\n## Democratic senate\nspec <- treat ~ orderent  + prevgenx  + lethal +\n  deathrt1 + hosp01 + hospdisc + hhosleng + femdiz01 + mandiz01 +\n  peddiz01 + acutediz + orphdum + natreg  +  wpnoavg3 +\n  vandavg3 + condavg3 + stafcder + sqrt(hospdisc) \n\nm1.out <- matchit(spec, method = \"nearest\", data = data,\n                 discard=\"both\", exact=c(\"acutediz\", \"peddiz01\", \"hosp01\",\n                                   \"lethal\", \"femdiz01\", \"mandiz01\"))\n\nplot(summary(m1.out), xlim=c(0,0.7))\n\nm2.out <- matchit(spec, method = \"nearest\", distance=\"mahalanobis\", data = data, exact=c(\"acutediz\", \"peddiz01\", \"hosp01\",\n                                   \"lethal\", \"femdiz01\", \"mandiz01\"))\nplot(summary(m2.out), xlim=c(0,0.7))"},{"path":"matching.html","id":"estimating-the-lognormal-duration-model-with-3-samples","chapter":"C Matching","heading":"C.5.3 Estimating the lognormal duration model with 3 samples","text":"full data set two matched datasets, can estimate model using specification functional form close published work. estimates three duration models - full model, propensity score matched data, matched data - reported .extent back started, size estimated effect depends matching method. least can select matching method honest way - evaluating balance - check see significant .","code":"library(survival)\nfullmodel <- survreg(Surv(acttime, d) ~ treat + orderent  +prevgenx  + lethal + deathrt1 + hosp01 + hospdisc + hhosleng +femdiz01 + mandiz01 + peddiz01 + acutediz + orphdum + natreg + wpnoavg3 + condavg3 +vandavg3 + stafcder + I(natreg^2), dist=\"lognormal\", data=data)\n\n\nm1.data <- match.data(m1.out)\nmodel1 <- survreg(Surv(acttime, d) ~ treat + orderent  +prevgenx  + lethal + deathrt1 + hosp01 + hospdisc + hhosleng +femdiz01 + mandiz01 + peddiz01 + acutediz + orphdum + natreg + wpnoavg3 + condavg3 +vandavg3 + stafcder + I(natreg^2), dist=\"lognormal\", data=m1.data)\n\nm2.data <- match.data(m2.out)\nmodel2<- survreg(Surv(acttime, d) ~ treat + orderent  +prevgenx  + lethal + deathrt1 + hosp01 + hospdisc + hhosleng +femdiz01 + mandiz01 + peddiz01 + acutediz + orphdum + natreg + wpnoavg3 + condavg3 +vandavg3 + stafcder + I(natreg^2), dist=\"lognormal\", data=m2.data)\n\nstargazer(fullmodel, model1, model2, style=\"qje\", type=\"text\" , digits=2)\n\n=======================================================\n                               acttime                 \n                    (1)           (2)          (3)     \n-------------------------------------------------------\ntreat            -0.41***       -0.21*       -0.45***  \n                  (0.11)        (0.12)        (0.13)   \n                                                       \norderent          0.14**        0.18***       0.13*    \n                  (0.06)        (0.07)        (0.08)   \n                                                       \nprevgenx          0.17***        0.11         0.20**   \n                  (0.06)        (0.07)        (0.08)   \n                                                       \nlethal             0.07          -0.05         0.13    \n                  (0.19)        (0.23)        (0.26)   \n                                                       \ndeathrt1         -0.54***       -0.44**      -0.55**   \n                  (0.21)        (0.21)        (0.23)   \n                                                       \nhosp01             0.003         -0.10        -0.19    \n                  (0.20)        (0.23)        (0.26)   \n                                                       \nhospdisc          0.15***       0.15***      0.19***   \n                  (0.04)        (0.05)        (0.06)   \n                                                       \nhhosleng           -0.07         -0.01        -0.03    \n                  (0.15)        (0.16)        (0.18)   \n                                                       \nfemdiz01          -0.54*        -0.56*        -0.51    \n                  (0.30)        (0.31)        (0.33)   \n                                                       \nmandiz01           -0.13         0.04         -0.03    \n                  (0.33)        (0.35)        (0.38)   \n                                                       \npeddiz01           -0.21         -0.21        -0.35    \n                  (0.37)        (0.39)        (0.43)   \n                                                       \nacutediz           -0.15         -0.20        -0.16    \n                  (0.19)        (0.22)        (0.23)   \n                                                       \norphdum           -0.32*         -0.22        -0.37*   \n                  (0.18)        (0.18)        (0.21)   \n                                                       \nnatreg            2.08***       1.88**       2.42***   \n                  (0.68)        (0.84)        (0.84)   \n                                                       \nwpnoavg3         -0.50***      -0.83***      -0.74***  \n                  (0.12)        (0.15)        (0.16)   \n                                                       \ncondavg3           0.20         0.77***       0.41**   \n                  (0.16)        (0.25)        (0.19)   \n                                                       \nvandavg3           0.10          0.11          0.13    \n                  (0.13)        (0.15)        (0.15)   \n                                                       \nstafcder         -0.21***      -0.16***      -0.21***  \n                  (0.02)        (0.03)        (0.03)   \n                                                       \nI(natreg2)       -1.01***        -0.85       -1.27**   \n                  (0.38)        (0.64)        (0.53)   \n                                                       \nConstant          6.13***       5.44***      6.18***   \n                  (0.39)        (0.45)        (0.50)   \n                                                       \nN                   408           306          312     \nLog Likelihood   -1,224.86      -973.75      -955.59   \nchi2 (df = 19)   200.57***     134.73***    156.01***  \n=======================================================\nNotes:           ***Significant at the 1 percent level.\n                  **Significant at the 5 percent level.\n                  *Significant at the 10 percent level."},{"path":"matching.html","id":"looking-ahead-multi-level-modeling","chapter":"C Matching","heading":"C.6 Looking ahead: multi-level modeling","text":"Multi-level modeling exploits situations observations nested hierarchy - individuals observed neighborhoods, part cities, nested counties, nested states, instance.include variables various levels individual-level model. top-level data can help improve lower-level, individual-level, estimates.One focus class recent piece US congress: Park (2021)piece interesting useful couple reasons merits careful read:work nicely located literature Congress. working reading list American politics comprehensive exam, see several classic recent work list.work nicely located literature Congress. working reading list American politics comprehensive exam, see several classic recent work list.dependent variable - grand-standing - meticulously crafted measure uses congressional statements machine learning identify members Congress engaged message politics.dependent variable - grand-standing - meticulously crafted measure uses congressional statements machine learning identify members Congress engaged message politics.modeling strategy uses multi-level approaches, leveraging fact individual members committees, particular Congresses. (also application matching)modeling strategy uses multi-level approaches, leveraging fact individual members committees, particular Congresses. (also application matching)replication data R code available Harvard Dataverse.","code":""},{"path":"multilevel-and-hierarchical-models.html","id":"multilevel-and-hierarchical-models","chapter":"D Multilevel and hierarchical models","heading":"D Multilevel and hierarchical models","text":"short introduction hierarchical models - drawn almost entirely Chapter 12. “Multilevel Linear Models: Basics” Gelman Hill (2006). authors use R throughout text make available data R code used book. adapt data manipulation presentation output lean functions dplyr, ggplot2, stargazer.","code":""},{"path":"multilevel-and-hierarchical-models.html","id":"why-is-ols-a-special-case","chapter":"D Multilevel and hierarchical models","heading":"D.1 Why is OLS a special case?","text":"Classical regression can imply number assumptions: treatment effects homogeneous (slope coefficients) unmeasured heterogeneity (intercepts). covered couple ways permit assumptions relaxed. use interaction terms model conditional marginal effects. instance, claiming groups observations may different slopes. data repeated measures - panel data - units measured one time - can use fixed effects (least squares dummy variables). case, attempting handle unmeasured heterogeneity allowing intercepts vary.","code":""},{"path":"multilevel-and-hierarchical-models.html","id":"why-multilevel-models","chapter":"D Multilevel and hierarchical models","heading":"D.2 Why multilevel models?","text":"alternative way managing research situations expect see diverse heterogeneous responses hierarchical multilevel model - two names approach. idea recognize observations different sources (organizations, geographic units, groups people). chosen group observations single dataset. might many households number different counties. might many counties many states provinces many countries. might individuals particular sites. might respondents different iterations similar surveys.understand observations indexed xij indicates individual j indicates group. Panel data really specific case form data, unit observed j time period xit. Time series cross sectional data (TSCS) form.Gelman Hill also introduce non-nested models implies multiple perhaps overlapping groups. example, individual-level dataset, people may drawn geographic area people may related ways (share occupation). Recall approaches panel data required assumed data strongly balanced - individuals observed time period gaps. Multilevel modeling impose type restrictions - might units one area, many units another area.Unlike approach panel data earlier classes, simply rely knowledge groups observations may share different intercepts errors. can actually introduce information higher level aggregation model variation. instance, might information states less us improve predictions something measure level counties townships. approach described hierarchical - top-level (group level) data lower level (individual level) data. Multilevel models imply two models estimated simultaneously - regression coefficients modeled lowest level regression model intercept differences coefficients level groups. powerful generalization classical linear model.introduced panel data estimators, considered estimate regression simply pooling data together, ignoring unmeasured heterogeneity ignoring problems heteroscedasticity (designated complete pooling). alternative strategy might simply estimate model parameters separately group observations (pooling): creates problem low numbers observations low power. Gelman Hill describe multilevel model compromise complete pooling (ignoring categorical grouping variable like country) pooling (estimating different models category country).One area multilevel models commonly used study education outcomes. dataset might consist student performance outcomes student characteristics, also know school county city student observed. may interested size school percentage free/reduced lunches. may interested demographics city county. know student characteristics interact environmental features influence performance.collect data form, consider use multilevel approach.","code":""},{"path":"multilevel-and-hierarchical-models.html","id":"a-sample-dataset-and-problem","chapter":"D Multilevel and hierarchical models","heading":"D.3 A sample dataset and problem","text":"Gelman Hill use dataset radon measurements motivate discussion multilevel approach. dataset contains information radon levels thousands homes located hundreds U.S. counties.Radon naturally occurring gas -product decay uranium. Homes near uranium deposits may accumulate dangerous levels radon gas; exposure gas linked lung cancer. use four pieces information dataset: measured radon level, whether sample taken basement first floor, county home , measurement soil uranium county. idea use information aggregate (county) improve predictions lower levels aggregation (homes). data R code figures models adapted directly companion website book. examples drawn Chapter 12.R package lme4 permits us estimate basic hierarchical models. particular function use lmer, maximum likelihood estimator.estimate interpret series progressively complicated models using data 85 counties dataset state Minnesota. use information county household understand likely value radon test home particular county. Since radon levels higher basements, important use information model, especially data counties primarily taken basement measurements others taken first floor.","code":"\n## Set up the radon data\n# Read, subset and tranform the data\nsrrs2 <- read.table (\"http://www.stat.columbia.edu/~gelman/arm/examples/radon/srrs2.dat\", header=T, sep=\",\")\nmn <- srrs2 %>%\n  filter(state==\"MN\") %>%\n  mutate(log.activity=log(ifelse (activity==0, .1, activity)))\n\n# Get the county-level predictor\ncty <- read.table (\"http://www.stat.columbia.edu/~gelman/arm/examples/radon/cty.dat\", header=T, sep=\",\")\ncty2 <- cty %>%\n  filter(st==\"MN\") %>%\n  mutate(u=log(Uppm), cntyfips=ctfips)\n\n# There are two duplicate counties, which need to be deleted before joining - other wise model 5 has problems\ncty2 <- cty2 %>% filter(!row_number() %in% c(11, 39))\n\n\n# This creates the data frame with all variables available\ndata<-left_join(mn, cty2, by=c(\"cntyfips\"))\n\n# This step just strips out the uranium by county, used below\nu_county <- data %>%\n  group_by(county) %>%\n  summarise(u=mean(u))"},{"path":"multilevel-and-hierarchical-models.html","id":"data-and-figures","chapter":"D Multilevel and hierarchical models","heading":"D.3.1 Data and figures","text":"actual distribution radon activity reported national survey fairly close zero households. metric measure picocuries per liter air, pCi/L. According EPA’s Citizen Guide Radon, typical level radon outside air 0.4 pCi/L average home indoor level 1.7. measured radon levels 919 sample homes summarized figure . threshold remediation 4.0.Since radon effects cumulative, researchers use log measure measure risk. overall distribution dependent variable, log measured radon activity, displayed :","code":"\n# Plot the histogram\nggplot(mn, aes(x=activity))+\n  geom_histogram(binwidth = 2, colour = \"black\", fill = \"white\")+\n  labs(x=\"Measured levels of radon\", y=\"Frequency\")\n# Plot the log\nggplot(mn, aes(x=log.activity))+\n  geom_histogram(binwidth = 0.25, colour = \"black\", fill = \"white\")+\n  labs(x=\"Log level of radon\", y=\"Frequency\")"},{"path":"multilevel-and-hierarchical-models.html","id":"model-1-complete-pooling-one-predictor","chapter":"D Multilevel and hierarchical models","heading":"D.3.2 Model 1 (complete pooling, one predictor)","text":"model uses logged radon measurement dependent variable location test (first floor basement predictor). ignore information multiple households one county. just basic regression. counties share intercept value estimated slope coefficient.table tells us average predicted reading household county 1.33 basement measurement 0.713 first floor measurement. best guess radon measurement house sample, depending measure taken.","code":"\nlm.pooled <- lm (log.activity ~ floor, data=mn)\nstargazer (lm.pooled, style=\"apsr\", type=\"html\" , omit=\"factor\" , dep.var.labels = c(\"Log of radon measure\", \"\\n\"), digits=3, covariate.labels = c(\"First floor reading\"), omit.stat=c(\"LL\",\"ser\",\"f\"),column.sep.width=c(\"12pt\"))"},{"path":"multilevel-and-hierarchical-models.html","id":"model-2-no-pooling-one-household-predictor-county-level-fixed-effects","chapter":"D Multilevel and hierarchical models","heading":"D.3.3 Model 2 (no pooling, one household predictor, county-level fixed effects)","text":"estimate model standard OLS fixed effects county. approach used panel data, dummy variable household. ” -1 ” factor(county) indicates intercept, can dummy every county. Adding county-level dummy permits otherwise unmeasured county-level differences improve prediction. model include 85 dummy variables - one county - suppressed output (using omit option stargazer command).Notice jump R-squared.figure shows intercept standard errors associated intercept function number households sampled county. One thing jumps extremely high intercepts (positive readings) associated counties 1 2 households sample. estimate counties probably high overall assessment county-level variation high since small samples large standard errors counties.","code":"\nlm.unpooled <- lm (log.activity ~ floor + factor(county) - 1, data=mn)\nstargazer (lm.unpooled, style=\"apsr\", type=\"html\" , omit=\"factor\" , dep.var.labels = c(\"Log of radon measure\", \"\\n\"), digits=2, covariate.labels = c(\"First floor reading\"), omit.stat=c(\"LL\",\"ser\",\"f\"),column.sep.width=c(\"12pt\"))\n# tidy the data and retain the 85 dummy coefficients and standard errors\nresults<-tidy(lm.unpooled)\nresults<- results %>% filter(term!=\"floor\")\n\n## get a table of sample size in each county\nsample.size<-as.data.frame(table(mn$county))\n\n# Be cautious with this command - doesn't really join just mashes the dataframes\n# A careful approach would strip out county names and use join\nresults<-cbind(results, sample.size)\n\n# Reproduce the figure with horizontal jitter\nggplot(data=results, aes(x=Freq, y=estimate))+\n  geom_point(position = position_jitter(width = 0.10, seed = 123)) +\n  scale_x_continuous(trans = \"log10\") + ylim(0, 3.5)+\n  geom_linerange(aes(x = Freq, ymin = estimate+std.error, ymax = estimate-std.error),\n                   lwd = 0.5, position = position_jitter(width = 0.10, seed = 123)) +\n  labs(x=\"Sample size in county\", y=\"Estimated intercept\", caption=\"Figure 3. Estimates +/- standard errors for the county intercepts, no pooling\")"},{"path":"multilevel-and-hierarchical-models.html","id":"model-3-partial-pooling-one-household-predictor-multilevel-model","chapter":"D Multilevel and hierarchical models","heading":"D.3.4 Model 3 (partial pooling, one household predictor, multilevel model)","text":"multilevel model permits intercept vary shared mean. know shared mean (1.33) Model 1 individual fixed effects Model 2. model combination information. Note different information reported invoke (lmer, rather lm). goodness fit test statistics deviance reported , rather R-squared. one indicator estimating via maximum likelihood.`(1 | county)’ notation lmer function indicates unique intercepts county.\nTable 3. Household radon, partial pooling, multilevel model\nfigure shows estimated intercept county standard errors attached.idea leverage information state mean counties observations drawn closer state mean. Counties large numbers households affected. Gelman Hill note Model 1 Model 2 can problematic: might overstate county-level variation based small samples Model 2 might understate county-level variation ignore much information counties extreme variation mean Model 1. Model 3 compromise.","code":"\n# This corresponds to M1 at page 259 of Gelman and Hill\nmodel3 <- lmer (formula = log.activity ~ floor + (1 | county), data=mn)\nstargazer (model3, type=\"html\" , style=\"apsr\", omit=\"factor\" , dep.var.labels = c(\"Log of radon measure\", \"\\n\"), covariate.labels = c(\"First floor reading\"), digits=2, omit.stat=c(\"LL\",\"ser\",\"f\"), title=\"**Table 3. Household radon, partial pooling, multilevel model**\",\n          notes= \"p<.01*** ; p<.05**\",  notes.append = FALSE)\n# tidy won't work lmer\na.hat.model3 <- coef(model3)$county[,1]   \na.se.model3 <- se.coef(model3)$county\nresults<- as.data.frame(cbind(a.hat.model3, a.se.model3))\nresults<-std.err<-results$`(Intercept)`\n## get a table of sample size in each county\nsample.size<-as.data.frame(table(mn$county))\n\n# Be cautious with this command - doesn't really join just mashes the dataframes\n# A careful approach would strip out county names and use join\nresults<-cbind(results, sample.size)\n\n# Reproduce the figure with horizontal jitter\nggplot(data=results, aes(x=Freq, y=a.hat.model3))+\n  geom_point(position = position_jitter(width = 0.10, seed = 123)) +\n  scale_x_continuous(trans = \"log10\") + ylim(0,3.5) +\n  geom_linerange(aes(x = Freq, ymin = a.hat.model3+std.err, ymax = a.hat.model3-std.err),\n                   lwd = 0.5, position = position_jitter(width = 0.10, seed = 123)) +\n  labs(x=\"Sample size in county\", y=\"Estimated intercept\", caption=\"Figure 4. Estimates +/- standard errors for the county intercepts, multilevel model\")"},{"path":"multilevel-and-hierarchical-models.html","id":"model-4-pooled-data-one-household-predictor-one-county-level-predictor-multilevel-model.","chapter":"D Multilevel and hierarchical models","heading":"D.3.5 Model 4 (pooled data, one household predictor, one county-level predictor, multilevel model).","text":"real advantage multilevel modelling can directly introduce information higher levels aggregation levels equation. Remember used fixed effects panel data, introduce time invariant country features since dropped ? modelling framework use features grouping variable (county case) vary across households within county. variable introducing soil uranium. County-level uranium soil levels introduced predictor. assumption coefficients basement/floor soil uranium households counties. level uranium soil county improve estimate intercept, remaining unmeasured heterogeneity.Notice goodness fit test statistics lower - indicates better model performance.figure plots intercept counties function measured soil uranium. kepy y-axis scale Figure 4, can clearly see standard errors much smaller introduced group=level predictorAs county level uranium increases, estimate intercept increases remaining county-level error much lower.","code":"\n# This corresponds to M2 in Gelman and Hill page 266\nmodel4 <- lmer (formula = log.activity ~ floor + u + (1 | county), data=data)\nstargazer (model4, type=\"html\" ,style=\"apsr\",  omit=\"factor\" ,  dep.var.labels = c(\"Log of radon measure\", \"\\n\"), covariate.labels = c(\"First floor reading\", \"Soil uranium\"), digits=2, omit.stat=c(\"LL\",\"ser\",\"f\"),column.sep.width=c(\"12pt\"))\n# Plot of ests & se's vs. county uranium (Figure 12.6)\n# Pick up the standard errors and coefficients\n# Write as new columns in the results file created above\nresults$std.err.model4 <- se.coef(model4)$county\na.hat.model4 <- fixef(model4)[1] + fixef(model4)[3]*u_county$u + ranef(model4)$county\nresults$a.hat.model4<-a.hat.model4[,1]\nresults$b.hat.model4 <- fixef(model4)[2]\nresults$u<-u_county$u\n# Need vector of u to pass to results (n=85 only)\n\n# Produce the figure\n# Reproduce the figure with horizontal jitter\nggplot(data=results, aes(x=u, y=a.hat.model4))+\n  geom_point(position = position_jitter(width = 0.10, seed = 123)) +\n  geom_smooth(method=\"lm\", se= FALSE, formula = y~x) + ylim(0, 3.5) +\n labs(x=\"County-level uranium measure\", y=\"Estimated intercept\", caption=\"Figure 5. Estimates +/- standard errors for the county intercepts, multilevel model, one group predictor\") +\ngeom_linerange(aes(x = u, ymin = a.hat.model4+std.err.model4, ymax = a.hat.model4-std.err.model4),\n                   lwd = 0.5, position = position_jitter(width = 0.10, seed = 123))"},{"path":"multilevel-and-hierarchical-models.html","id":"model-5.-permit-intercepts-and-coefficients-to-vary.-no-restrictions.","chapter":"D Multilevel and hierarchical models","heading":"D.3.6 Model 5. Permit intercepts and coefficients to vary. No restrictions.","text":"least restrictive model. x:ufull interaction term (permitting slopes vary), (1 | county) indicates intercept can vary across counties independent soil measure. error messgae warning “boundary (singular) fit:” - suggests -fitting model, estimating many parameters little data.figures show slopes intercepts vary. Notice, , standard errors around intercepts smaller. notice slopes decline (negative) uranium soil increases. means difference first floor basement larger counties.","code":"\n# This corresponds to M4 in Gelman and Hill, p. 281\nmodel5 <-  lmer (formula = log.activity ~ floor + +u + floor:u + (1 + floor | county), data=data)\n\nstargazer (model5, type=\"html\" , style=\"apsr\", omit=\"factor\" , dep.var.labels = c(\"Log of radon measure\", \"\\n\"), digits=2, covariate.labels = c(\"First floor reading\", \"Soil uranium\", \"Interaction term\"), omit.stat=c(\"LL\",\"ser\",\"f\"),column.sep.width=c(\"12pt\"))\n\na.hat.model5 <- fixef(model5)[1] + fixef(model5)[3]*u_county$u + ranef(model5)$county[,1]\nb.hat.model5 <- fixef(model5)[2] + fixef(model5)[4]*u_county$u + ranef(model5)$county[,2]\na.se.model5 <- se.ranef(model5)$county[,1]\nb.se.model5 <- se.ranef(model5)$county[,2]\n\n\n# par (mfrow=c(1,1))\n# \n#  # plot on Figure 13.2(a)\n# lower <- a.hat.model5 - a.se.model5\n# upper <- a.hat.model5 + a.se.model5\n# par (mar=c(5,5,4,2)+.1)\n# plot (u, a.hat.model5, cex.lab=1.2, cex.axis=1.1, ylim=range(lower,upper), \n#       xlab=\"county-level uranium measure\", ylab=\"regression intercept\", \n#       pch=20, yaxt=\"n\")\n# axis (2, c(0,1,1.5,2))\n# curve (fixef(model5)[1] + fixef(model5)[3]*x, lwd=1, col=\"black\", add=TRUE)\n# segments (u, lower, u, upper, lwd=.5, col=\"gray10\")\n# mtext (\"Intercepts\", line=1)\n# \n#  # plot on Figure 13.2(b)\n# lower <- b.hat.model5 - b.se.model5\n# upper <- b.hat.model5 + b.se.model5\n# par (mar=c(5,5,4,2)+.1)\n# plot (u, b.hat.model5, cex.lab=1.2, cex.axis=1.1, ylim=range(lower,upper),\n#       xlab=\"county-level uranium measure\", ylab=\"regression slope\", pch=20)\n# curve (fixef(model5)[2] + fixef(model5)[4]*x, lwd=1, col=\"black\", add=TRUE)\n# segments (u, lower, u, upper, lwd=.5, col=\"gray10\")\n# mtext (\"Slopes\", line=1)"},{"path":"multilevel-and-hierarchical-models.html","id":"summary","chapter":"D Multilevel and hierarchical models","heading":"D.4 Summary","text":"Multilevel models extremely useful research situations data collected across varying geographic organizational contexts, expect common mechanisms linking X Y expect form links variable dependent context.","code":""},{"path":"multilevel-and-hierarchical-models.html","id":"next-week-2","chapter":"D Multilevel and hierarchical models","heading":"D.5 Next week","text":"Review replication example Appendix, reproducing key results Park (2021)use multilevel model bridge learn Bayesian methods.","code":""},{"path":"about-the-author.html","id":"about-the-author","chapter":"About the author","heading":"About the author","text":"","code":""},{"path":"about-the-author.html","id":"j.-kevin-corder","chapter":"About the author","heading":"J. Kevin Corder","text":"professor political science Western Michigan University Kalamazoo. research appeared American Political Science Review, Journal Politics, outlets political science public administration. Much work focuses economic policy, including two books Federal Reserve System. 2013, received Fulbright–Schuman European Affairs program grant study regulation banks Malta United Kingdom. shared National Science Foundation grant Carrie Chapman Catt prize Christina Wolbrecht, University Notre Dame. co-authored Counting Women’s Ballots: Female Voters Suffrage New Deal Century Votes Women: American Elections since Suffrage (Cambridge University Press).teach undergraduate graduate course quantitative methods working open source texts class.","code":""}]
