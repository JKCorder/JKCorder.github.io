<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>7 Statistical models with many predictors | Applied Statistics for Political Science</title>
<meta name="author" content="J.K. Corder">
<meta name="description" content="This chapter introduces the statistical model that is most commonly used in the social sciences and the building block for more complex models that have emerged to confront specific research...">
<meta name="generator" content="bookdown 0.37 with bs4_book()">
<meta property="og:title" content="7 Statistical models with many predictors | Applied Statistics for Political Science">
<meta property="og:type" content="book">
<meta property="og:description" content="This chapter introduces the statistical model that is most commonly used in the social sciences and the building block for more complex models that have emerged to confront specific research...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="7 Statistical models with many predictors | Applied Statistics for Political Science">
<meta name="twitter:description" content="This chapter introduces the statistical model that is most commonly used in the social sciences and the building block for more complex models that have emerged to confront specific research...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.5.1/transition.js"></script><script src="libs/bs3compat-0.5.1/tabs.js"></script><script src="libs/bs3compat-0.5.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
          margin-bottom: 0em;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="bs4_style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Applied Statistics for Political Science</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">About this book</a></li>
<li><a class="" href="data-science-political-science-and-public-policy.html"><span class="header-section-number">1</span> Data science, political science, and public policy</a></li>
<li><a class="" href="descriptive-statistics.html"><span class="header-section-number">2</span> Descriptive statistics</a></li>
<li><a class="" href="an-introduction-to-measures-of-association-correlation.html"><span class="header-section-number">3</span> An introduction to measures of association: correlation</a></li>
<li><a class="" href="measures-of-association-t-test.html"><span class="header-section-number">4</span> Measures of association: t-test</a></li>
<li><a class="" href="measures-of-association-chi-square.html"><span class="header-section-number">5</span> Measures of association: chi-square</a></li>
<li><a class="" href="regression.html"><span class="header-section-number">6</span> Regression</a></li>
<li><a class="active" href="statistical-models-with-many-predictors.html"><span class="header-section-number">7</span> Statistical models with many predictors</a></li>
<li><a class="" href="references.html">References</a></li>
<li><a class="" href="about-the-author.html">About the author</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/JKCorder/RMD-for-AppStats-chapters">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="statistical-models-with-many-predictors" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Statistical models with many predictors<a class="anchor" aria-label="anchor" href="#statistical-models-with-many-predictors"><i class="fas fa-link"></i></a>
</h1>
<p>This chapter introduces the statistical model that is most commonly used in the social sciences and the building block for more complex models that have emerged to confront specific research challenges.</p>
<p>The simple linear model with more than one predictor is known as the multivariate model or multivariate regression. Social scientists use this approach to study behavior, evaluate public policy, or forecast events. It is also the foundation for more complex modeling used in fields ranging from public health to environmental sciences. We are mainly interested in the multivariate model since it offers a flexible way to simultaneously evaluate the effects of many predictors in one set of statistical tests. We can isolate the effect of one factor in a model by “controlling” for a variety of other factors.</p>
<div id="what-is-controlled-comparison" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> What is controlled comparison?<a class="anchor" aria-label="anchor" href="#what-is-controlled-comparison"><i class="fas fa-link"></i></a>
</h2>
<p>Recall the distinction between experimental research and observational research introduced early in the course. Experimental research in social science shows up in both work on public policy (program clients are randomly assigned to control or intervention) and surveys or lab experiments (subjects are exposed to different information or situations or frames, referred to as an experimental manipulation). For research in this course, we rely on an observational approach, using a random sample of potential US voters. The challenge with observational research is that we might be interested in comparing members of two groups – maybe older and younger voters, but we can’t isolate the effect of age alone. People who are older are likely to have more health problems, reside in a rural area, to attend church and have a lower than average level of education. To isolate the effect of age or any <span class="math inline">\(X\)</span> we care about we must “control” for other differences between groups.</p>
<p>We know that the basic assumption of the simple linear model is that the primary predictor we are interested in (<span class="math inline">\(X\)</span>) has a link to the behavior or attitude we are investigating (<span class="math inline">\(Y\)</span>). Controlled comparison simply involves considering a third variable, <span class="math inline">\(Z\)</span>, that is related to both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(X\)</span>. <span class="math inline">\(Z\)</span> is associated with <span class="math inline">\(Y\)</span>. The question we are interested in: Does the link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> hold if you test the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> at varying levels of <span class="math inline">\(Z\)</span>?</p>
<p>We need to understand what the effect of <span class="math inline">\(X\)</span> is on <span class="math inline">\(Y\)</span> as we control for <span class="math inline">\(Z\)</span>. We can even broaden our investigation to include multiple control variables, not just a single <span class="math inline">\(Z\)</span>. This is the logic of controlled comparison – evaluate the link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for varying levels of <span class="math inline">\(Z\)</span>. For instance, if we were interested in how income (<span class="math inline">\(X\)</span>) influenced turnout (<span class="math inline">\(Y\)</span>), we might want to control for education (<span class="math inline">\(Z\)</span>). We know education is linked to turnout and to income, so our understanding of the link between income and turnout might be distorted by education.</p>
<p>Consider what might happen is you test the link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for varying levels of <span class="math inline">\(Z\)</span>.</p>
<p>You could see no change from your original investigation – <span class="math inline">\(X\)</span> affects <span class="math inline">\(Y\)</span> when you look at the relationship with <span class="math inline">\(X\)</span> for the entire sample and <span class="math inline">\(X\)</span> matters in the same way when you look at the link for subsets of the sample at various levels of <span class="math inline">\(Z\)</span>. This is labeled an <em>additive</em> effect.</p>
<p>But you might discover a link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> that disappears when you test the link at various levels of <span class="math inline">\(Z\)</span>. In that case, when a result that we found to be important shrinks to zero when we control for <span class="math inline">\(Z\)</span>. the original result is labeled as <em>spurious</em>.</p>
<p>Or it could be the case that you find no link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> but the link emerges as important or significant when you conduct a controlled comparison. This would be an example of <em>confounding</em> – you were misled, thinking there was no link between <span class="math inline">\(X\)</span> and Y, but discovered the link when you introduced controls.</p>
<p>A final case could be when find that the link <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is actually different across the levels of <span class="math inline">\(Z\)</span> – positive in some subsets of the sample and negative in others. This is known as an <em>interactive</em> effect.</p>
<p>In earlier chapters, we described three ways that we could use tables and figures to show the link between two variables: cross-tabs, bar charts, and line charts. We can use the same strategies to implement a controlled comparison, using the examples to reinforce the difference between additive, spurious and confounding effects.</p>
<div id="red-state-blue-state-cross-tabulation-with-a-control-variable" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> Red state, Blue state: cross tabulation with a control variable<a class="anchor" aria-label="anchor" href="#red-state-blue-state-cross-tabulation-with-a-control-variable"><i class="fas fa-link"></i></a>
</h3>
<p>Observers of American politics claim that the South is politically distinctive (100 % “Red State”). We associate Red states with what? Republican voting, support for the military, guns, Christian values, maybe even NASCAR and country music. We associate Blue states with Democratic voting, urban living, Volvos, latte and book clubs. While the stereotypes are fun, are Red states really that different? The 2004 election should be very good test of this question – George Bush – from Texas – was running for re-election against John Kerry, a northerner, a liberal, and career politician and diplomat. Kerry was portrayed as elitist, affected and out-of-touch. So what happened to Kerry in the South? Was the proportion of the electorate supporting Bush in 2004 really that different between Southern and other states? The South is all red, the rest of the country a mixture of red and blue – so did the South deliver votes for Bush? Table 2 reports each candidate’s percentage of the two-party vote <span class="citation">(<a href="references.html#ref-anes2004">U. of M. American National Election Studies [ANES] 2005</a>)</span>.</p>
<p><strong>Table 7.1 Support for George Bush, percentage of the two-party vote, by region, 2004</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th align="left">Region</th>
<th align="right">Percentage</th>
</tr></thead>
<tbody>
<tr>
<td align="left">Outside of the South</td>
<td align="right">50.88</td>
</tr>
<tr>
<td align="left">South</td>
<td align="right">49.79</td>
</tr>
</tbody>
</table></div>
<p>What happened? There is almost no difference between the South and the rest of the nation – just over one percentage point and not in the expected direction. (This result is, by the way, not statistically significant - there is no difference between these two groups of voters in the population.) Do political pundits make way too much of Red State / Blue State differences? What are we missing here?</p>
<p>Perhaps we should consider other factors that could be distorting the relationship - what demographic feature of the South distinguishes the South from the rest of the US? One is race – the South has a higher proportion of African-Americans than the rest of the US and a couple of big states with large Latinx population (Texas and Florida). In the 2004 ANES sample, 40 percent of Southerns were racial and ethnic minorities, compared to only 20 percent outside of the South. This is somewhat puzzling to outside observers of American politics – the region of the country that is arguably the most ethnically and racially diverse is expected to be more Republican?</p>
<p>In any case, this question requires a controlled comparison – we know that the South is more diverse, we know or think that a diverse electorate is going to be more Democratic – so we need to control for race in order to clarify the link between region and vote. Table 2 reports the controlled comparison – summarizing the link between region and vote for White respondents compared to respondents with any other ethnic or racial identity.</p>
<p><strong>Table 7.2 Bush and Kerry vote in the South and outside of the South, by race, ANES 2004</strong></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th align="left">Region</th>
<th align="left">Race</th>
<th align="right">Percentage</th>
</tr></thead>
<tbody>
<tr>
<td align="left">White, Non-Hispanic</td>
<td align="left">Outside of the South</td>
<td align="right">56.11</td>
</tr>
<tr>
<td align="left">White, Non-Hispanic</td>
<td align="left">South</td>
<td align="right">65.58</td>
</tr>
<tr>
<td align="left">Any other racial or identification</td>
<td align="left">Outside of the South</td>
<td align="right">28.70</td>
</tr>
<tr>
<td align="left">Any other racial or identification</td>
<td align="left">South</td>
<td align="right">19.75</td>
</tr>
</tbody>
</table></div>
<p>The regional differences are clearly much larger if we control for race. White Southerners are 66% Bush, 10 percentage points higher than Whites outside of the South. Southerners who are not White are the most Democratic of the four groups. Racial differences in the composition of the electorate confounded our attempt to understand regional differences. It is interesting that the Red State - Blue state narrative works only if you exclusively focus on White voters. (This is also an example of an interactive effect - Republican support is higher in the South if you look at White voters, Republican support is lower in the South if look at everyone else).</p>
</div>
<div id="turnout-income-and-education-bar-charts-with-a-control-variable" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">7.1.2</span> Turnout, income and education: bar charts with a control variable<a class="anchor" aria-label="anchor" href="#turnout-income-and-education-bar-charts-with-a-control-variable"><i class="fas fa-link"></i></a>
</h3>
<p>We know that wealthy respondents are more likely to turnout in elections. This could be the case since higher income families are more likely to have access to transportation, a job that permits time off, and may perceive more at stake in an election. But we also know that highly educated people are more likely to turn out to vote – perhaps for reasons unrelated to income – highly educated people have more information, so the costs of voting (acquiring information, making choices) are lower – and, again, perhaps perceive the stakes as higher (more incentive to vote). But education and income are intertwined – maybe the effect of income is just due to high income people having more education? We should test whether the link between income and turnout is spurious. Figure 1 summarizes self-reported turnout for people in the lower, middle, and upper 1/3 of the income distribution. The turnout numbers seem high for a couple of reasons – the ANES has a potential response bias that is a consequence of people who are interested in politics being more likely to participate in the survey and, on top of that, people know they are supposed to vote, so they may self-report a vote when they did not vote. In any case, the results are fairly clear – higher income is associated with higher self-reported voting.</p>
<p><strong>Figure 7.1 Household income and self-reported turnout, 2020 ANES</strong>
<img src="07-models_files/figure-html/figure71-1.png" width="672"></p>
<p>Figure 2 breaks out the link between income and turnout across 3 levels of education – a controlled comparison. Now we can understand the link between turnout and income for people with the same (or at least similar) levels of education).</p>
<p><strong>Figure 7.2 Household and income and self-reported turnout, controlling for education, 2020 ANES</strong></p>
<div class="inline-figure"><img src="07-models_files/figure-html/figure72-1.png" width="672"></div>
<p>The data in the figure would lead us to conclude that there is an effect of income - a positive link – for every level of education. We can also see that education matters – in each case – each income group – the effect of more education is also positive. Although education and income are related, they both exert an independent effect on turnout – and the combined effects are large – low income, low education respondents are almost half as likely to vote as high education, high income – a reality that grossly distorts what we learn about citizen preferences from elections – we learn about the what voters want – not what everyone wants and the two things are very different since low income and low education voters are under-represented.</p>
<p>This is a good example of additive effects – the introduction of controls highlights the fact that we need to pay attention to both education and income.</p>
</div>
</div>
<div id="so-how-does-a-statistical-model-fit-in-to-all-of-this" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> So how does a statistical model fit in to all of this?<a class="anchor" aria-label="anchor" href="#so-how-does-a-statistical-model-fit-in-to-all-of-this"><i class="fas fa-link"></i></a>
</h2>
<p>There are two reasons we can’t reliably use tables and figures to make these types of controlled comparisons. In some cases, <span class="math inline">\(Z\)</span> has many categories and that implies too many tables or bars on bar chart. Moreover, as soon as you begin to think about multiple or several control variables, the strategy completely breaks down. For instance, if you want to check out the link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for every possible combination of race (white/minority), gender (male/female), income (low/middle/high) and education (low/middle/high), you would need (2 <span class="math inline">\(X\)</span> 2 <span class="math inline">\(X\)</span> 3 <span class="math inline">\(X\)</span> 3) or 36 separate tables – and each table would only have a small number of people.</p>
<p>As an alternative to explicitly looking at the link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for every value of <span class="math inline">\(Z\)</span>. we can simply add the control variable to our simple linear model. That steps turns out to be exactly the same or equivalent to looking at the slope coefficient for every possible subgroup and averaging across all of those groups.</p>
<p>For the simple linear model, recall that we assume a particular “functional form” for the relationship between <span class="math inline">\(X\)</span> and Y:</p>
<p><span class="math display">\[Y=\beta_0+\beta_1X+\epsilon\]</span></p>
<ul>
<li><p><span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are known (data)</p></li>
<li><p><span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are estimated.</p></li>
<li><p><span class="math inline">\(e\)</span> is a random error</p></li>
</ul>
<p>We are principally interested in <span class="math inline">\(\beta_1\)</span>. We wish to know two things: what can we infer about the population from our observed <span class="math inline">\(\beta_1\)</span>? What does the observed <span class="math inline">\(\beta_1\)</span> tell us about the relationship between <span class="math inline">\(X\)</span> and Y?</p>
<p>The multivariate model is convenient since it is easy to add multiple explanatory variables to the model. Your intuitions about “what matters” and existing theoretical expectations should guide your choice of what to introduce in the model. The process of deciding what variables to introduce is labeled <em>model specification</em>.</p>
<p>The general form of the model is:
<span class="math display">\[Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\epsilon\]</span></p>
<p>We focus on three ways that the multivariate model differs from the two-variable model:</p>
<ul>
<li><p>the coefficients are interpreted in a slightly different way</p></li>
<li><p>we use the adjusted r-squared (rather than r-squared) to evaluate which models fit the data better.</p></li>
<li><p>we introduce standardized coefficients n order to understand which variables matter more</p></li>
</ul>
</div>
<div id="the-multivariate-model" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> The multivariate model<a class="anchor" aria-label="anchor" href="#the-multivariate-model"><i class="fas fa-link"></i></a>
</h2>
<div id="partial-coefficients-vs.-full-coefficients" class="section level3 unnumbered">
<h3>Partial coefficients vs. full coefficients<a class="anchor" aria-label="anchor" href="#partial-coefficients-vs.-full-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>For the bivariate model, the computation of <span class="math inline">\(\beta_1\)</span> is simple. The sum of squares for any linear regression is minimized when:</p>
<p><span class="math display">\[\beta_1 =\frac{\sum((X-\mu_x)(Y-\mu_y))} {\sum(X-\mu_x)^2} \]</span></p>
<p>The coefficients from the bivariate model are easy to interpret: <span class="math inline">\(\beta_1\)</span> describes how much <span class="math inline">\(Y\)</span> changes for a one-unit change in <span class="math inline">\(X\)</span>.</p>
<p>Interpretation in the multivariate model is somewhat more complicated. The coefficient describes how much <span class="math inline">\(Y\)</span> changes for a unit change in X<sub>1</sub> , holding all other Xs constant.</p>
<p>The effect of X<sub>1</sub> (<span class="math inline">\(\beta_1\)</span> ) in the multivariate model is a <em>partial</em> effect (distinct from the <em>full</em> effect observed in the bivariate model). To make it clear just how much things change when we switch from the bivariate to the multivariate model, I rely on the notation short cut below. Let,</p>
<p><span class="math display">\[x_1=X_1-\mu_{X_1}\]</span></p>
<p><span class="math display">\[x_2=X_2-\mu_{X_2}\]</span></p>
<p><span class="math display">\[y=Y-\mu_Y\]</span></p>
<p>Using this short-hand, in the two variable case, the slope coefficient is the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> divided by the variance of X:</p>
<p><span class="math display">\[\beta_1=\frac{\sum(x_1y)}{\sum(x_1)^2}\]</span></p>
<p>In the three variable case, things are a lot more complicated:</p>
<p><span class="math display">\[\beta_1=\frac{\sum(x_1y)\sum({x_2})^2-\sum(x_2y)\sum(x_1x_2)} {\sum(x_1)^2\sum(x_2)^2-\sum(x_1x_2)^2}\]</span></p>
<p>Recall from previous assignments that the covariance of X<sub>1</sub> and X<sub>2</sub> is <span class="math inline">\(\sum(x_1x_2)\)</span>.</p>
<p>If this is zero (meaning X<sub>2</sub> is unrelated to X<sub>1</sub>), then <span class="math inline">\(\beta_1\)</span> for the multivariate case reduces to the original bivariate <span class="math inline">\(\beta_1\)</span>. You can see how the the math works above if you plug in zero for <span class="math inline">\(\sum(x_1,x_2)\)</span>.</p>
<p>This is crucial to understand – the only way that introducing a control variable matters is if that control is actually related to the variable you are interested in.</p>
<p>But, note that even if the covariance between <span class="math inline">\(Y\)</span> and X<sub>2</sub> is zero, the coefficient will still change if X<sub>2</sub> is added to the model.</p>
<p>The larger the covariance of X<sub>1</sub> and X<sub>2</sub> the greater the difference in the coefficients from the two models.</p>
<p>Note one implication: you should always understand the correlation between each pair of <span class="math inline">\(X\)</span> variables in a model</p>
</div>
<div id="adjusted-r-squared" class="section level3 unnumbered">
<h3>Adjusted r-squared<a class="anchor" aria-label="anchor" href="#adjusted-r-squared"><i class="fas fa-link"></i></a>
</h3>
<p>R-squared has limitations for comparing the performance of two multivariate models. Every time you add an <span class="math inline">\(X\)</span> variable to a model, the value of R-squared increases. Given this, we would add ALL possible variables to the model to maximize R-squared. It is also good to have a simple model, rather a complex model. To “reward” simple models, we can use an alternative measure of the “goodness-of-fit“: adjusted R-squared.</p>
<p><span class="math display">\[R^2_{adj} = {1-\frac{(1-R^2)(n-1) } {n-k-1}}\]</span></p>
<p>Adjusted R-squared may be very close to R-squared if you have a very large sample and a small number of predictors. In the example below, the two numbers are the same since we have at most five predictors and 5,807 observations - so the adjustment is very, very small. But, with smaller samples it is conventional to use the adjusted R-squared. So, if two models of the same <span class="math inline">\(Y\)</span> have a different number of <span class="math inline">\(X\)</span> variables, you should use adjusted R-squared to compare the models.</p>
</div>
<div id="how-do-we-determine-what-really-matters-standardized-coefficients" class="section level3 unnumbered">
<h3>How do we determine what really matters? Standardized coefficients<a class="anchor" aria-label="anchor" href="#how-do-we-determine-what-really-matters-standardized-coefficients"><i class="fas fa-link"></i></a>
</h3>
<p>In the multivariate model, you can use two pieces of information in the output to determine which <span class="math inline">\(X\)</span> is most important for predicting <span class="math inline">\(Y\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>As in earlier chapters, you could determine the range of each <span class="math inline">\(X\)</span> (<span class="math inline">\(\Delta\)</span>X) and multiply that quantity times the slope coefficient for each <span class="math inline">\(X\)</span> – that would let you compare the size of the effect on <span class="math inline">\(Y\)</span> for two variables</p></li>
<li><p>You could use a standardized coefficient (which will be labeled <span class="math inline">\(\beta^*\)</span>). Some software packages report standardized coefficients along with the unstandardized coefficients we have used to this point. Or you could calculate the standardized coefficients and these numbers to the output. The calculation is not difficult:</p></li>
</ol>
<p><span class="math display">\[\beta_1^*=\beta_1 * \frac{\sigma_x}{\sigma_y}\]</span></p>
<p>In the example below, I will add these standardized coefficients to the regression output. The standardized coefficient with the largest absolute value has the largest effect. Specifically, yhis coefficient tells you how much <span class="math inline">\(Y\)</span> changes (how many standard deviations) for each one standard deviation increase in <span class="math inline">\(X\)</span>. You can see this by rearranging the terms in the formula above.</p>
<p>A quick scan of the standardized coefficients permits you to rank the size of effects from highest (farthest from zero) to weakest. This is something you can’t do with the unstandardized coefficients.</p>
</div>
</div>
<div id="an-example-using-the-multivariate-model-to-understand-who-likes-donald-trump" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> An example: Using the multivariate model to understand who likes Donald Trump<a class="anchor" aria-label="anchor" href="#an-example-using-the-multivariate-model-to-understand-who-likes-donald-trump"><i class="fas fa-link"></i></a>
</h2>
<p>The link between how people feel about Donald Trump and party identification helped us understand correlation and regression. Below, we consider whether or not the Trump feeling thermometer is primarily driven by party id or ideology or both. We will also consider a broader model with several variables in order to see how we use the standardized coefficients.</p>
<p>The single variable models predicting the Donald Trump feeling thermometer are reproduced as column (1) and column (2) in Table 3. The first column reports the link to party id – Strong Republicans (party id= 7) more than 80 pts higher on the feeling thermometer than Strong Democrats (party id = 1).</p>
<p>The second column reports the same numbers for the link between the ideology and the thermometer. The coefficient indicates the effect of ideology is even more powerful, with feelings increasing about 97 points - the full range of the thermometer - as we compare extreme liberals (ideology = 1) to extreme conservatives (ideology=7).</p>
<p><strong>Table 7.3 Two bivariate models predicting the Donald Trump feeling thermometer, using party and ideology</strong></p>
<p><br></p>
<div class="inline-table"><table style="text-align:center" class="table table-sm">
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td colspan="2">
Feeling Thermometer, Donald Trump
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
(1)
</td>
<td>
(2)
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Party identification
</td>
<td>
13.751<sup>***</sup>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Ideology
</td>
<td>
</td>
<td>
16.177<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
-12.644<sup>***</sup>
</td>
<td>
-26.666<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
<em>N</em>
</td>
<td>
2,868
</td>
<td>
2,440
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.638
</td>
<td>
0.479
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.638
</td>
<td>
0.479
</td>
</tr>
<tr>
<td colspan="3" style="border-bottom: 1px solid black">
</td>
</tr>
</table></div>
<p><br></p>
<p>Notice that the coefficient on party identification is lower than the coefficient for ideology, but the r-squared is higher for the model that uses party id. How can that be the case? Shouldn’t the large effect be associated with a higher r-squared? Yes, but, in this case, we know that there are many people who are at the ends of the party identification scale (1 or 7), while most people are in the middle of the scale for ideology (middle of the road), so the errors will be lower (better fit for 1 and 7) for party id. To settle any uncertainty about which variable matters more, we can test the link in a single multivariate model.</p>
<p>Moreover, how can we be sure if either of these models are accurate? We know that party id and ideology are correlated - conservatives are likely to be Republican – so we know that full effects (the slopes above) and partial effects (in a multivariate model) will be different. By introducing both predictors in one model we can disentangle the effects of ideology and party id. This permits us to clarify, for instance, if an extremely liberal Strong Democrat would feel differently about Hillary Clinton than a middle of the road Strong Democrat. Or if a conservative Strong Republican would feel differently than a conservative Independent. The coefficients from a multivariate model with both party id and ideology appears below as Table 4. The first column reports the unstandardized coefficients and standardized coefficients are reported at the bottom of the table.</p>
<p><strong>Table 7.4 A multivariate model predicting the Donald Trump feeling thermometer, using party and ideology in one model.</strong></p>
<p><br></p>
<div class="inline-table"><table style="text-align:center" class="table table-sm">
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
Feeling Thermometer, Donald Trump
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Party identification
</td>
<td>
11.395<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Ideology
</td>
<td>
4.842<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
-24.484<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
<em>N</em>
</td>
<td>
2,430
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.688
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.687
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
</table></div>
<p><br></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th align="left">Standardized coefficients</th>
<th align="left"></th>
</tr></thead>
<tbody>
<tr>
<td align="left">Party identification</td>
<td align="left">0.66</td>
</tr>
<tr>
<td align="left">Ideology</td>
<td align="left">0.21</td>
</tr>
</tbody>
</table></div>
<p>The table tells us three things. Using the significance levels, we know that both party id and ideology matter – both effects are positive and significant, as in Table 3. But we also learn the party id is the much more powerful effect. The standardized coefficient for party is 0.66, much farther from zero than the same number for ideology, 0.21. The unstandardized coefficients reinforce this conclusion – Strong Democrats are expected to be about 68 points lower than Strong Republicans, but extreme liberal and extreme conservatives are separated by only 29 points. The most important numbers to compare across the tables are the unstandardized coefficients. Ideology drops in strength, from 17.15 to 4.84. Party id stays about the same, 13.51 and 11.39.</p>
<p>So while this is an example of additive effects – both variables remain significant in the model – the multivariate models leads us to conclude that it is partisanship, not ideology, that is the most important predictor. The adjusted R<sup>2</sup> is informative here too - the model that best fits the data is in Table 4 - the two predictor model is better than either of the single predictor models in Table 3.</p>
<p>We could use a similar strategy to evaluate a range of variables. The table below reports the coefficients from a complex model – party id, ideology, race, gender, age, education and income.</p>
<p><strong>Table 7.5 A multivariate model predicting the Donald Trump feeling thermometer, testing several potential predictors.</strong></p>
<div class="inline-table"><table style="text-align:center" class="table table-sm">
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
Feeling Thermometer, Donald Trump
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
<tr>
<td style="text-align:left">
Party identification
</td>
<td>
11.300<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Ideology
</td>
<td>
4.421<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Education
</td>
<td>
-1.426<sup>***</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.000
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Income
</td>
<td>
-0.113
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.115
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Female
</td>
<td>
-0.967
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.180
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Minority
</td>
<td>
1.878<sup>*</sup>
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.061
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
Constant
</td>
<td>
-4.364
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
p = 0.138
</td>
</tr>
<tr>
<td style="text-align:left">
</td>
<td>
</td>
</tr>
<tr>
<td style="text-align:left">
<em>N</em>
</td>
<td>
2,123
</td>
</tr>
<tr>
<td style="text-align:left">
R<sup>2</sup>
</td>
<td>
0.707
</td>
</tr>
<tr>
<td style="text-align:left">
Adjusted R<sup>2</sup>
</td>
<td>
0.706
</td>
</tr>
<tr>
<td colspan="2" style="border-bottom: 1px solid black">
</td>
</tr>
</table></div>
<p><br></p>
<div class="inline-table"><table class="table table-sm">
<thead><tr>
<th align="left">Standardized coefficients</th>
<th align="left"></th>
</tr></thead>
<tbody>
<tr>
<td align="left">Party identification</td>
<td align="left">0.66</td>
</tr>
<tr>
<td align="left">Ideology</td>
<td align="left">0.19</td>
</tr>
<tr>
<td align="left">Education</td>
<td align="left">-0.08</td>
</tr>
<tr>
<td align="left">Income</td>
<td align="left">-0.02</td>
</tr>
<tr>
<td align="left">Female</td>
<td align="left">-0.02</td>
</tr>
<tr>
<td align="left">Minority</td>
<td align="left">0.02</td>
</tr>
</tbody>
</table></div>
<p>This type of table has a lot of information, but we can focus on a few numbers. First, which variables can we ignore as not statistically significant? You can see that the coefficient on income is not statistically significant - after controlling for other factors, income doesn’t matter. Race and gender also don’t matter.</p>
<p>To figure out which of the significant variables are the best predictors, we can use the standardized coefficients. The largest (in absolute value) is party identification, followed by ideology, then education.</p>
<p>So we could conclude that highly educated, liberal, Strong Democrats will dislike Donald Trump, while poorly educated, conservative Strong Republicans will like Donald Trump the most. Within these two categories, women and men will be the roughly the same - and there will no differences across racial categories. Notice the r-squared – over 70 percent of the variation in this attitude is explained by five variables. Review the table to be sure you understand how these conclusions follow from the numbers.</p>
<div id="the-key-thing-to-report-how-coefficients-change-when-you-introduce-controls" class="section level3 unnumbered">
<h3>The key thing to report: how coefficients change when you introduce controls<a class="anchor" aria-label="anchor" href="#the-key-thing-to-report-how-coefficients-change-when-you-introduce-controls"><i class="fas fa-link"></i></a>
</h3>
<p>What if the coefficients on our variables change between the simple bivariate model and a more complex multivariate model. This can mean one of four things:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\beta_1\)</span> does not change. No changes in sign or direction of <span class="math inline">\(\beta_1\)</span> if control <span class="math inline">\(Z\)</span> is added. <span class="math inline">\(Z\)</span> and <span class="math inline">\(X\)</span> both influence <span class="math inline">\(Y\)</span> (Additive)</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> becomes insignificant if control <span class="math inline">\(Z\)</span> is added. <span class="math inline">\(Z\)</span> affects <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> and there is no link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> when <span class="math inline">\(Z\)</span> is added to the model (Spurious).</p></li>
<li><p><span class="math inline">\(\beta_1\)</span> becomes significant if control <span class="math inline">\(Z\)</span> is added. <span class="math inline">\(Z\)</span> affects <span class="math inline">\(Y\)</span> and X, and there is a link between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> when <span class="math inline">\(Z\)</span> is added to the model (confounding).</p></li>
<li><p><span class="math inline">\(Z\)</span> determines how <span class="math inline">\(X\)</span> influences <span class="math inline">\(Y\)</span> (interactive). We would need to create interaction term (X*Z) to test this. We won’t be doing this for the assignment, but to test for an interactive effect, we would estimate the model and see if <span class="math inline">\(\beta_3\)</span> was significant or not.</p></li>
</ol>
<p><span class="math display">\[Y=\beta_0+\beta_1X+\beta_2Z+\beta_3(X*Z)\]</span></p>
<p>Remember: the focus is always on the coefficients for the <span class="math inline">\(X\)</span> variables – we really don’t care how the constant does or does not change when we compare the bivariate and multivariate models.</p>
</div>
</div>
<div id="summary-1" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Summary<a class="anchor" aria-label="anchor" href="#summary-1"><i class="fas fa-link"></i></a>
</h2>
<p>Statistical models with many predictors are a simple way to isolate the effects of key variables, a necessity when we deal with observational data. The multivariate model is widely used in the social sciences in both policy analysis and studies of political behavior. This approach to statistical modeling is the foundation for more advanced data analysis strategies that have emerged over the past 30 years in subfields as diverse as judicial politics, the study of Congress, international relations, political economy, and the study of bureaucracy. If you pick a political science journal, the odds are high that you will see several examples of statistical models used to test competing expectations about what matters – you know to look for the sign, significance, and size of effects to understand what these regressions tell us.</p>

<!-- `r if (knitr::is_html_output()) ' -->
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="regression.html"><span class="header-section-number">6</span> Regression</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#statistical-models-with-many-predictors"><span class="header-section-number">7</span> Statistical models with many predictors</a></li>
<li>
<a class="nav-link" href="#what-is-controlled-comparison"><span class="header-section-number">7.1</span> What is controlled comparison?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#red-state-blue-state-cross-tabulation-with-a-control-variable"><span class="header-section-number">7.1.1</span> Red state, Blue state: cross tabulation with a control variable</a></li>
<li><a class="nav-link" href="#turnout-income-and-education-bar-charts-with-a-control-variable"><span class="header-section-number">7.1.2</span> Turnout, income and education: bar charts with a control variable</a></li>
</ul>
</li>
<li><a class="nav-link" href="#so-how-does-a-statistical-model-fit-in-to-all-of-this"><span class="header-section-number">7.2</span> So how does a statistical model fit in to all of this?</a></li>
<li>
<a class="nav-link" href="#the-multivariate-model"><span class="header-section-number">7.3</span> The multivariate model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#partial-coefficients-vs.-full-coefficients">Partial coefficients vs. full coefficients</a></li>
<li><a class="nav-link" href="#adjusted-r-squared">Adjusted r-squared</a></li>
<li><a class="nav-link" href="#how-do-we-determine-what-really-matters-standardized-coefficients">How do we determine what really matters? Standardized coefficients</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#an-example-using-the-multivariate-model-to-understand-who-likes-donald-trump"><span class="header-section-number">7.4</span> An example: Using the multivariate model to understand who likes Donald Trump</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#the-key-thing-to-report-how-coefficients-change-when-you-introduce-controls">The key thing to report: how coefficients change when you introduce controls</a></li></ul>
</li>
<li><a class="nav-link" href="#summary-1"><span class="header-section-number">7.5</span> Summary</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/JKCorder/RMD-for-AppStats-chapters/blob/master/07-models.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/JKCorder/RMD-for-AppStats-chapters/edit/master/07-models.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Applied Statistics for Political Science</strong>" was written by J.K. Corder. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
